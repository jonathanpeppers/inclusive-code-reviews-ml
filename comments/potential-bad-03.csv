Body
"```suggestion
        /// <remarks>The <see cref=""System.IO.Compression.DeflateStream.DisposeAsync()"" /> method enables you to perform a resource-intensive dispose operation without blocking the main thread. This performance consideration is particularly important in a Windows 8.x Store app or desktop app where a time-consuming stream operation can block the UI thread and make your app appear as if it is not working. The async methods are used in conjunction with the <see langword=""async"" /> and <see langword=""await"" /> keywords in Visual Basic and C#.
        /// This method disposes the Deflate stream by writing any changes to the backing store and closing the stream to release resources.
        /// Calling <see cref=""System.IO.Compression.DeflateStream.DisposeAsync()"" /> allows the resources used by the <see cref=""System.IO.Compression.DeflateStream"" /> to be reallocated for other purposes. For more information, see [Cleaning Up Unmanaged Resources](/dotnet/standard/garbage-collection/unmanaged).</remarks>
```"
"Added a comment with rationale, also pasting here since it's interesting:
```
// The following dictionaries are used as caches for operations that recurse over the structure of SymbolicRegexNode.
// These operations are called potentially on every step of the matching process, and they may do linear work in the
// of the pattern in each call. Thus, caching is necessary to avoid a quadratic worst-case over multiple steps of
// matching when simplification rules fail to eliminate the portions being walked over.
```
Currently with the stress tests we have removing these caches wouldn't be a problem. However, we don't have a proof that our current simplification rules always avoid these worst-cases. There might be hope for removing some of these caches with future work on the theory side."
"I haven't measured, but it's my understanding that in the worst case (when headers+body) happen to fit in a single TCP packet, you'll need an extra call to Http.Sys to schedule another IOCP callback.

@github ?"
"Ughhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh

can we have a CommonAwaitExpressionInfo? (seriously...)?"
"The overload accepting a char[] destination was only called from ReCreateParts, after [this conservative worst-case capacity calculation](https://github.com/dotnet/runtime/blob/c991d9bc35b38016d8a48e745f842b648678fb9a/src/libraries/System.Private.Uri/src/System/Uri.cs#L2711-L2718). I believe it was thankfully unreachable code, as we never needed a resize."
"Did a first quick review of the PR.
It would be great if this also worked when Drag and Drop from the desktop. Since currently it only works when drag and dorp from inside the Explorer. And ideally those two experiences would be the same.

I have also found out that the mac finder behaves like the explorer currently does, before this PR. So leaving it like it is is not the worst solution.

Apart from that: we do not ask the dialog for deeply rooted children correct, we just check for differences on the first level? Which I think we should just do it on the first level."
"We seriously don't have an existing resource with ""Windows Terminal"" already?"
"Previously this would throw a NullReferenceException if `library` was `null`. Now this will append garbage into the string.

We shouldn't change behavior here."
"I intentionally avoided VSB here for two reasons. First, though the expectation is that lines are under 100 chars, that's certainly not a requirement. Second, since connections can drop and throw exceptions mid-read, we could generate garbage. My experience is that StringBuilder is a bit more forgiving than VSB at this, since if a failure occurs while VSB is on the stack, it negatively impacts the performance of other components which are using the array pool.

Basically, I don't foresee better perf in the typical use case, and I do foresee worse perf in the extreme use case, when compared to a standard StringBuilder. It didn't seem like a good tradeoff since trying to account for this would involve complicating the code."
"> Is this a regression? What is the impact of this change for someone using Large Pages + hard limit?

@github - In short - yes, it is - and this change will make large Pages + hard limit better.

The story started with [your discovery](https://github.com/dotnet/runtime/pull/35802#issuecomment-624337471) that the 3x commit introduced by POH is a blocker for you. 

The root cause of the 3x commit is because when given a single hard limit, we do not know if the application is going to use that for which object heap, and therefore we assumed the worst. 

For the normal scenario without large pages, that is okay because we just reserved the memory without committing it.

With large pages, implementation forces us to commit upfront, and therefore we committed 3x the hard limit, and that is not okay

Therefore I introduced a new way to specify the hard limit per object heap, that eliminated the guesswork the runtime is doing:

- https://github.com/dotnet/runtime/pull/36731
- https://github.com/dotnet/runtime/pull/37166

By specifying the hard limit for individual object heaps, the runtime will commit exactly as specified.

As an example, I wish to have 1G in SOH, 2G in LOH, and 500M in POH, you would specify
```
COMPLUS_GCHeapHardLimitSOH=1G (in hex)
COMPLUS_GCHeapHardLimitLOH=2G (in hex)
COMPLUS_GCHeapHardLimitPOH=500M (in hex)
```
The runtime will reserve (or commit in large page case) 3.5G upfront and distribute them as instructed.

Just to be clear, if the application happens to allocate more than 1G in SOH, then it will OOM, regardless of whether or not we still have memory in LOH or POH.

After that, I worked on testing it. After https://github.com/dotnet/runtime/pull/37725, the code works functionally, but it has a performance bug. When individual heap hard limits are provided, the heuristic to determine which generation to condemn is broken. In particular, it chose to perform a gen 2 background GC always.

This is caused by my ignorance. I thought the field `heap_hard_limit` is used only for checking whether or not we exceed the limit, turn out it is also used to determine available memory, and thus impact the choices the heuristic would make. 

By setting `heap_hard_limit` to approximately what it should be, this change fixed the heuristic, and it will choose the same generation to condemn as it sees fit.

So for a longer summary:

- The 3x commit issue is solved
- My implementation had a couple of bugs, but they are fixed by now."
"It makes the full queue benchmark slightly slower, and the one-at-a-time benchmark slightly faster. The effective change is that garbage collection happens as the queue empties, rather than as it fills back up. This shifts work earlier, and into times where the server has more capacity. Good change."
"The semantics of clone have been a garbage fire since .NET Framework 1.0 ðŸ˜„ "
"this is very costly.  each `+=` will generate a new string (and the previous one will be garbage).

1. see if we have a helper that produces one string out of `classifiedText.Runs` elsewhere.
2. if not, then use a PooledStringBuilder so you can do this without garbage."
I'll try it out. Hopefully it works because `PrimaryWorkspace` was the worst part of this.
"I can't tell what you're advocating for; your comments contradict each other.

The pattern you're citing from MSDN is exactly what's already happening in that current code, except that their version sets the `disposed` flag to true after executing the `if(disposing)` block instead of before. (Whether it's better to set that flag before or after is a separate discussion, but I believe that ""before"" is the safer option.)

> When you call Dispose(false), it sets _disposed to true but does NOT actually dispose (the if block doesn't get executed).

""the if block doesn't get executed"" is not the same thing as ""does not dispose"". The `if(disposing)` block is for cleaning up managed resources when our code calls `Dispose()`. We can't clean up managed resources during `Dispose(false)` because we're being finalized, and the state of the managed resources is indeterminate. (It wouldn't do much good anyway, since by this point they're likely being garbage collected.)

> When you call Dispose(true) later on, it assumes the object is already disposed, so it returns.

That's not a thing. `Dispose(false)` signals that the finalizer is calling `Dispose()`; the object is being destroyed/finalized. There is no subsequent call to `Dispose(true)`. 

The converse is also true; if `Dispose(true)` has been called, `Dispose(false)` never happens, because the root class's `Dispose()` implementation calls `GC.SuppressFinalize()`.

> MasterDetailPageRenderer.cs and NavigationPageRenderer.cs have the correct implementation.

Their implementations are subtly wrong. In both, if `Dispose()` is called by the finalizer (i.e., `Dispose(false)`), `_disposed` will never be set to true, and nothing will prevent a double-Dispose situation. In those two classes we happen to be fine because there are no unmanaged resources to handle (i.e., nothing outside of the `if(disposing)` block), so no disposal logic gets run multiple times. We _are_ incurring a slight performance penalty, though, and that's something I hope to fix soon.
"
"I believe this got to a point which is incredibly hard to understand. I decided to take 30 minutes to unwrap it, got lost in all the spaghetti and gave up.

I suggest to document the exact order and direction of all the expected messages during handshake, as well as what should happen between each message.

When documenting, I suggest to drop the client/server nomenclatures and simply talk about _shared process_ and _main_, especially because the roles are reversed when referring to the Electron IPC communication vs. the named pipe communication, via which both processes are connected:

1. When the shared process is spawned, during its handshake, it acts as a client via Electron IPC
2. After the handshake, while communicating to main, it acts as a server via the named pipe

Feel free to merge if it works. :+1: "
"Seriously, on every branch the feature tests fail? Is there a problem with the CI and running from forks or something?"
"Done.  I made the buffers one character longer also to accommodate there always being a null char even at max actual length.  I seriously doubt it would ever get there with this particular API's typical values, but you never know."
"CWT is basically pool. why create so many redundant weak reference objects? if there are 300 analyzers (basically install style cop analyzers), worst case will be 300 x number of projects. which pointing to same workspace. why just use 1 rather than creating all those weak reference?
"
"@github sorry this has taken so long, but weâ€™ve decided this is looking good for the 4.2 release, which means we could merge as soon as 4.1-rc is out. The type checking behavior should behave just like there are colons in the element names and element attributesâ€”no nested objects. This means the type checking should already work, but it would be good to see a test or two:

```ts
// @noImplicitAny: true
// @jsx: preserve

declare namespace JSX {
  interface IntrinsicElements {
    ""ns:element"": {
      ""ns:attribute"": string;
    }
  }
}

const e = <ns:element ns:attribute=""yep"" />;
```

As an aside:

> By making it nested, it is possible to reuse attributes... Without this, I have to write `""namespace1:attr1"": attributes['attr1']`, `""namespace2:attr1: attributes['attr1']` because ts can't calculate string literal in type level

I think this should be possible with #40336 now ðŸ˜„ 

We also decided that thereâ€™s not a great reason to limit this to `jsx: preserve`. `jsx: react` can be used for more than just React with JSX pragmas, so it seems overly prescriptive to disallow it. Youâ€™ll still get type-checking errors if you try to put attributes with colons on the intrinsic elements defined by @types/react, because those intrinsic elements lack any attributes with colons. You would not get type errors on your own component if you defined its props as `any`, or included prop names with colons, and I guess React wouldnâ€™t like that, but that seems outside the scope of what TypeScript is supposed to help you with.

We also need to make sure that we gracefully error on `<Capitalized:component />` and hopefully not emit total garbage with `--jsx=react`, since this doesnâ€™t make any sense.

Thanks for your patience! Iâ€™ll be keeping an eye out to make sure this is ready for 4.2 and to help with whatever you need."
"> I am simply curious why we need to keep the usage of DiagnosticSource with Activity?

While I wouldn't encourage most users to do it, there are still some scenarios that can't be accomplished any other way. We also haven't yet replicated every relevant piece of useful guidance onto the official docs for the people who aren't using DiagnosticSource with Activity. I am making a bet (and perhaps it is a bad bet) that the people who find this doc and choose to keep reading it after seeing the obsoletion warning at best will learn something useful and at worst they'll decide it was a waste of their time to read and forget what they read in it."
"Thanks - added comment:

```
    // Define a separate cache for each base href, so we're isolated from any other
    // Blazor application running on the same origin. We need this so that we're free
    // to purge from the cache anything we're not using and don't let it keep growing,
    // since we don't want to be worst offenders for space usage.
```

It's not enough to rely on the browser clearing stuff from its cache when storage space is low. End users short on space (especially on mobiles) do look at the lists of which sites are using the most space and will have a negative opinion of the ones at the top of the list, and are more likely to target those for manual removal from the cache."
"I removed it because its behavior is wrong on big-endian.
I ran the benchmark for 32bit and there is a ~4% regression in the worst-case.

If we believe this kind of optimization is necessary, I can add an alternative version of it - initial testing showed it can save ~15% from ctor time for simple Uris."
"I seem to recall it was skipped due to streaming issues.  But since the link no longer works, we won't be able to diagnose it.  The handler as well has been through a bunch of issues since the issue was filed, so it may not even be applicable.  If these are running stably for you, I think we should just unskip all these.  At worst we have to disable them again, but we'll have updated diagnostic information."
"> Since the code order is exactly the same, it shouldn't have any other side effects.

The managed code is in GC safe mode and the unmanaged code is in GC unsafe mode. The transition happens at different point in this PR which may have a side effect. Now the garbage collector can run between the two statements while it previously could not.

That said, in this particular case I don't see any issue with it."
"@github You say that, and I've said that, but that's not true. We parse `jsdoc` even in TS files for the LS (that's why you can still use `Go To Definition` on a typedef in a `.ts` file) - we just don't consider them as part of the types of any associated nodes in the checker. Seriously - `addJSDocComment` in `parser.ts` is unconditional. Parts of the binding process are also unconditional (not typedef binding, that's only done in JS; but parent pointer setup is always done)."
"> Why exclude them in release builds? If the state and conditions being valid is important we 
> should validate it at runtime in all cases, unless the performance hit is too significant. Not
> validating it could lead to larger bugs and (in the worst case) potential security problems

CoreCLR traditionally doesn't check asserts in the release build.

For Mono, `EP_ASSERT` is defined as `g_assert` which is always checked. We also have `g_assert_checked` - which is rarely used - which is only defined when configured with `--enable-checked-build=all`.  We can switch to that, or add an additional `EP_ASSERT_HEAVY` for particularly expensive checks."
"So my take on this is: 

I think the existing parser code is trying to be way to 'slim' and goes through incredibly complex hoops to try to figure out what's going on.  A *much* simpler approach, for me, would be to

1. mark the starting point of the potential modifiers.
2. eagerly consume *anything* that could be the modifiers of a type/member, and keep track of if you saw contextual modifiers.
3. If you didn't see partial/async (or any other contextual modifiers we add in the future), great! Just return the modifiers you parsed out.  Do not reset, just release your rewind point.
4. if you did have contextual modifiers, now actually examine them to ensure they're ok.  this is hte point where we can do complex checks between them and hte other modifiers, and/or the tokens that follow.
5. If you realize that one of these contextual modifiers was not actually a modifier, store that index.  Rewind.  Consume modifiers up to that part, then actually go do the rest of the parsing.

This just seems much simpler and saner for me.  it feels like right now we're trying to eat a token at a time, trying to keep track of everything and doing complex incremental moving forward and peeking.  I'd just rather get the modifiers, and hten see if partial/async are used properly based on if there are other modifiers that follow and/or you can properly see the start of a member/type afterwards.





"
note: as per my conversation with @github  i think the case that VB is handling is incredibly esoteric.  I would be ok with having VB just checking for that case and *not* offerring invert-if if it helps simplify the rest of the algorithms here.  I think it's a practically irrelevant case in practice and i would be fine with no invert-if for it.
"> typically from Program Files

Wait, seriously? I thought it was supposed to be using the one restored in the repo under `<repoRoot>\.dotnet`. Seems like a bug in our test infrastructure / something we should file an issue for if we are expecting / using the system's global install."
"calling either Wait or Result on a task in background thread will make that thread wasted until the given task returns.

depends on how heavy the task is, it could sometimes block thread quite long time. so, we try to make sure anything that runs in background never calls Wait/Result.

the worst thing that can happen (which happened before) is that multiple caller wait on heavy tasks on multiple threads causing thread poll to create +1000 threads (which gets worse as it goes since more threads mean slower progress in this situation).

once lesson learned, whenever we find mis-usage like this, we fix it. even though it can propagate quite big.
"
"Thanks for the suggestion! I looped through the `_isBackwards` array one extra time to count the number to allocate. It seems a bit weird to loop twice, but having a counter for the `nonBackwardsTZIDs` in the class would be allocating for an `int` for the entire lifetime of the class, and thats worse than just the lifetime of `GetTimeZoneIds` right?

Out of curiosity, what specifically are we avoiding in allocating the list? I am seeing that a list allocates space for a 2<sup>n</sup> array where its doubled each time it runs out of space. Are we avoiding that worst case 2<sup>n-1</sup>-1 space allocation or are we mainly avoiding the extra variables being allocated through the `List<T>` object?"
"Garbage-In/Garbage-Out is hard, since there's not a good bypass in the AsnWriter, and that CA is already on shaky grounds for interop.  Rather than do something like CertificateRequest's NormalizeSerialNumber (which helps let someone do RNG.Fill and not have to worry about it) I think rethrowing it with a corrected parameter name is probably the best bet.

Hm."
"Is ByRef and IntPtr overlap a problem? We decided not to check this for ByRef-like types.

ByRef pointing to unmanaged memory or other garbage should be fine because that's what happens for Span pointing to unmanaged memory.

Or is this futureproofing for when we allow byrefs on the GC heap and we'll need to make sure these writes go through a write barriers when that happens?"
"I found https://blogs.msdn.microsoft.com/yunjin/2005/07/05/special-threads-in-clr/

>Because there is only one thread to run all finalizers, if one finalizer is blocked, no other finalizers could run. So it is discouraged to take any lock in finalizer. Also see [Maoni Stephens's blog](http://blogs.msdn.com/maoni/archive/2004/11/04/252697.aspx) for details about finalizer thread.

It recommends not taking any lock, but only because it can stop other finalizers from running. It doesn't say whether the GC suspends normal threads while the finalizer is running.

[This doc](https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/fundamentals#concurrent-garbage-collection) seems to indicate the GC will not suspend other threads in .NET Core unless concurrent garbage collection is disabled. That said, as a server, we need to make sure we don't deadlock when concurrent garbage connection is disabled considering we tell customers ""[t]o improve performance when several processes are running, disable concurrent garbage collection.""

@github Please feel free to correct any inaccuracies in my understanding.
"
"Yes, the typical case for this is that something is wrong in the setup and either the process doesn't get created at all or the command executed but fails instantly. The worst outcome of this is a potential missing warning, if we wanted to be more accurate we can attach to the exit event handler and do it there, but I don't think its worth the additional complexity"
"Setting the length to zero clears the file, so you could end up with garbage at the end of the file if the new file is shorter than the old one."
"We are, but I have been mostly focusing on C#, as a reultVB side might not be well thought out and needs more work. Given this is an experimental feature, it's probably OK worst case we just disable it for VB for now."
I also don't like it but I wasn't aware it's considered legacy now. There is nothing in the docs that says so: https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/implementing-dispose
Is this seriously the only way to do `isMember` with a `string_view`?
"But what if we have 10,000 preprocessor keywords? :smile:

More seriously, not sure if easier to just make the sortText be ""_0"" + name; that way they stay together due to the common prefix but are otherwise sorted in whatever way they should be sorted."
"This is going to be ""expensive"" for production code. I think we can just insert `null` at the first correct position.

We already have what could be considered ""garbage"" data in the rest of the span due to `SkipLocalsInit` and so we rely on `\0` correctly marking the end."
This follows the disposing pattern - https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/implementing-dispose
"![image](https://user-images.githubusercontent.com/16246502/142055246-88902307-b8b9-4a9b-8b60-48bd2e472045.png)

More seriously though, should we just expose this / state in general?"
"There is no point in saving/restoring volatile registers as at return site they contain garbage.
lr,fp are saved as a part of stack alloc/dealloc, so no need to save lr again.
NZCV was not actually stored and there is no need."
"> I suppose you have run them already

I ran a worst-case test that would cause us to perform a timeout check at essentially every step, and I saw no measurable impact from calling Environment.TickCount64 at each check rather than maintaining a counter and calling Environment.TickCount 1/1000 such checks; TickCount{64} is cheap, and the other costs involved simply dominate.  This also removes the checks on paths that have less overhead and are hotter, e.g. as part of doing the initial linear match of a loop, so all of the timeout overhead goes away in those cases."
"> Expecting culture-sensitive result on en-US, but getting ordinal result?

Yes.  I find it highly questionable that people would write code that needs culture sensitive results in en-us.  And even if they did, i'm ok with that scenario regressing given the gain here.

> Expecting the same speedup as en-US on a different culture when data is in ASCII range?

I'm also ok with that.  At worst the code performs the same as today for non-en-us.  But for the common case of en-us and english code, we get a boost.  "
"> Approving on the diff, but not sure if there may be external considerations for this change. I'd hold of on merge till more approvals are in ðŸ˜„

This should be fine given how we do this for other assemblies. I need this to get access to `HashCodeCombiner`. Setting auto-merge to get the update dependencies to trigger. We can revert in the worst case ðŸ˜„ "
"I'm very glad we got that test failure, because this has identified a possible rendering bug. Admittedly it would be incredibly hard to repro, so much that it's possible nobody ever has done, but if you:

 * Set up a component that mutates the DOM in such a way that the mutation itself triggers an event (this is difficult to do, but our ""reordering focus retention"" E2E test case does it)
 * ... and then during the inner rendering loop, the component renders 2 or more times (so as to overwrite the original render tree in memory)

... then you could end up with an exception or undefined behavior.

I've implemented what I think is roughly the correct fix, but I want to add some proper E2E test cases dedicated to this edge case before merging this."
"No need for lock here.  At worst, we miss some write of 'true' to disposed.  But then the UI request that executes will immediately bail."
"This looks incredibly suspicious, as do the above two lines."
"@github @github  on this.  i'm just not getting it.  there' no reason i would expect or want to tie this option to a solution.  

> because that could have occurred while the user had a different solution open. 

If i go to option and turn on decompilaiton .i should get a prompt.  that prompt should be remembered forever.  Best case is that it's associated with my account and roamed.  Worst case it's remembered on the local machine.  But i can't understand any sort of design that ties this to somethinglike the solution or to navigation.  That makes no sense on any level, and does not give the user any sort of expected experience around decompilation that is sensible.

The fallout of that is the types of experiences i've been running into :-/"
use ArrayBuilder to make a temp buffer to copy the values into that we can then report (then free the temp buffer).  This way we're not just producing lots of garbage here.
"I found this code in particular so incredibly hard to understand that I changed it into simple C code.
In order to understand the existing code you have to understand that:
* `til::point` has a constructor for `COORD` (`options.coordCursor`)
* `til::rectangle` has a constructor for a point without size, which creates a rectangle at that origin of size 1x1
* `scale_up` multiplies the size of the rectangle, but not the origin
* `til::rectangle` implicitly turns into a `D2D1_RECT_F` using a conversion operator

The interaction between the second and third point are outright confusing for me and it took me more time to understand that code than I'd like to admit. I'd be fine with this code:
```cpp
D2D1_RECT_F rect = til::rectangle{ til::point{ options.coordCursor }, glyphSize };
```

But at that point I asked myself why you'd want to construct a til::rectangle anyways, just to turn it into a `D2D1_RECT_F` right after. If you'd like me to use that alternative code anyways, I'd be absolutely fine doing so of course! But personally I preferred the C-style code."
">  After #10615 (seriously guys, take a look and you can see how effective that PR is) and future PR lands

Dude its in my queue, it's just a big deal so I've been procrastinating. I feel seen. (I plan to look Monday... you're just catching me feeling too sleepy to ponder long term consequences...)
"
"quibble: these properties are not ""imaginary"". They are ""possibly existing"" properties.

Seriously, though, I'd say ""expando properties"". I also don't think that referencing the old implementation helps, so I'd delete that line or at least change it to ""This mirrors the behaviour of index signatures""."
"The only thing going for it is the `UseMany` scenario, and IMO it looks invalid. Notice how it *never* reassigns `queue2`, yet it repeatedly resets `queue` to be a modification of `queue2`. So we fill up a queue to be very large, then snap it as `queue2`. Then (for some reason) we enqueue one more element to `queue2` before dequeuing it. But the net effect is this loop does _not_ emulate adding and removing from a queue in sequence. All it does is exercise the very worst perf, which is ""first time dequeue"" in a loop, but made to look like a regular loop of enqueuing and dequeuing:

```cs
    [Benchmark]
    public IImmutableQueue<int>[] UseMany()
    {
        var queue2 = defaultQueue;
        var result = new IImmutableQueue<int>[N];
        for (int i = 0; i < result.Length; i++)
        {
            var queue = queue2.Enqueue(i);
            queue = queue.Dequeue();
            result[i] = queue;
        }
        return result;
    }
```

So no, that's not a scenario I've ever seen nor would expect to ever see and we should not optimize for it, particularly at such great expense to the common use cases. 

@github, I think a fair `UseMany` scenario (if we define it as a rather large queue that is repeatedly added to and removed from) would be something like this:

```cs
    [Benchmark]
    public void UseMany()
    {
        var queue = defaultQueue;
        for (int i = 0; i < N; i++)
        {
            queue = queue.Enqueue(i);
            queue = queue.Dequeue();
        }
    }
```

That I expect to perform *much* better, since the long queue length setup before this method runs would only be ""flipped"" occasionally instead of on every single dequeue operation."
@github that's what I'd recommend given it's incredibly easy to verify it's correct and you presumably want to merge today. But I've no objection to this (if perhaps someone else reviews)
"`workload install` is installing `6.0.2-mauipre.1.22102.15` version for the packs, which it gets from the mono.toolchain manifest for `6.0.300` .
```
  ** workload install **

  Running: /workspaces/runtime/artifacts/bin/dotnet-workload/dotnet workload install --skip-manifest-update --no-cache --configfile ""/tmp/elz3pk5g.cfm"" wasm-tools
  Using working directory: /tmp/
  Installing pack Microsoft.NET.Runtime.WebAssembly.Sdk version 6.0.2-mauipre.1.22102.15...
  Skip NuGet package signing validation. NuGet signing validation is not available on Linux or macOS https://aka.ms/workloadskippackagevalidation .
  Writing workload pack installation record for Microsoft.NET.Runtime.WebAssembly.Sdk version 6.0.2-mauipre.1.22102.15...
  Installing pack Microsoft.NETCore.App.Runtime.Mono.browser-wasm version 6.0.2-mauipre.1.22102.15...
....
                     Garbage collecting for SDK feature band(s) 6.0.300... (TaskId:444)
                     Successfully installed workload(s) wasm-tools. (TaskId:444)
```"
"If it is primarily for UTF-16 to UTF-8/etc, then 3x is the upper bound.

single UTF-16 character -> 1-3 UTF-8 bytes
surrogate pairs (i.e. 2 UTF-16 characters) -> 4 UTF-8 bytes (i.e. factor is still <= 3x)

If we expect arbitrary encodings, then ignore my comment, since I don't know what the worst case factor would be in that case."
"Yes, I had the same concern and experimented a bit with different syntaxes. Worst case scenario (unlikely)  we get a ""smalish"" bug here, that has an easy workaround and is easy to fix."
"Can these allocations for the intermediate string be avoided?
In the worst case it's 6 allocations. 
One for the substring, then one from string.Replace and another one for UnescapeDataString. And this for name and value, so 6 in total.

Ideally Uri.UnescapeDataString should have an overload for {RO}Span<char> too, so some intermeidate allocations can be avoided.

So should the queryString be copied to a Span<char>-buffer on which is operated?
The substring will instead be a slice of that span, avoiding one allocation.
string.Replace can be done with a helper on that span-slice (unforunately there's no Span<T>.Replace-API / extension).
Instead of 3 allocations there's only 1 left, and with Uri taking a span then this is can go away too.

<details>
   <summary>Helpers code</summary>

To replace `+` with ` `, and assuming length won't be short enough, so no vectorization needed:
```c#
internal static class MySpanExtensions
{
    public static void ReplaceInPlace(this Span<char> span, char oldChar, char newChar)
    {
        foreach (ref char c in span)
        {
            if (c == oldChar)
            {
                c = newChar;
            }
        }
    }
}
```

Or with vectorization (x86 only): Use methode `ReplacePlusWithSpaceInPlace` for best codegen.
```c#
internal static class MySpanExtensions
{
    public static void ReplacePlusWithSpaceInPlace(this Span<char> span)
        => ReplaceInPlace(span, '+', ' ');

    [MethodImpl(MethodImplOptions.AggressiveInlining)]
    public static unsafe void ReplaceInPlace(this Span<char> span, char oldChar, char newChar)
    {
        nint i = 0;
        nint n = (nint)(uint)span.Length;

        fixed (char* ptr = span)
        {
            ushort* pVec = (ushort*)ptr;

            if (Sse41.IsSupported && n >= Vector128<ushort>.Count)
            {
                Vector128<ushort> vecOldChar = Vector128.Create((ushort)oldChar);
                Vector128<ushort> vecNewChar = Vector128.Create((ushort)newChar);

                do
                {
                    Vector128<ushort> vec = Sse2.LoadVector128(pVec + i);
                    Vector128<ushort> mask = Sse2.CompareEqual(vec, vecOldChar);
                    Vector128<ushort> res = Sse41.BlendVariable(vec, vecNewChar, mask);
                    Sse2.Store(pVec + i, res);

                    i += Vector128<ushort>.Count;
                } while (i <= n - Vector128<ushort>.Count);
            }

            for (; i < n; ++i)
            {
                if (ptr[i] == oldChar)
                {
                    ptr[i] = newChar;
                }
            }
        }
    }
}
```
</details>"
"What's keeping this rooted?  You're probably going to need to add this Timer as a field to TimeoutState, such that the Timer ends up rooting itself.  Otherwise, it looks like if garbage collection kicks in during the operation, the timer could get collected and never fire.  We should ensure we have a test for that."
can you make these all `{ get; } =`.  there's no point allocating this garbage over and over again.
I'm wondering if we should just do the work to find the `MasterDetailPage` again here inside this `if` statement. I don't like the idea of holding a reference to a child page inside of a `NavigationPage`. Seems like we might get a memory leak at best and an `ObjectDisposedException` at worst if the hierarchy changes and that reference doesn't get updated soon enough.
"Oh. Yikes. When it was broken in the debugger, it looked like the string was full of random memory. I guess I assumed that the view was empty, but secretly actually filled with garbage."
"> Can you clarify this? Can the new service return a different type of info that the feature layer can decide what to do with? the LSP versoin can dump them, the VS version can use them.

My concern is all the event-based logic here: http://sourceroslyn.io/#Microsoft.CodeAnalysis.VisualBasic.EditorFeatures/NavigationBar/VisualBasicNavigationBarItemService.vb,113. The logic around `WithEvents` and `Handles` is incredibly VB-specific, and I don't see how it can be exposed in a language-agnostic way without the VB NavBar service basically just using the service to get the types in the file then recomputing all the nested symbols it needs because it needs to determine whether they're `WithEvents` or `Handles`, dig out the events they handle, and reorder.

> In other words, i don't get what this system is buying us given htat we still hvae LSP vs VS, and we still have C# vs VB.

Well, this does allow us to collapse LSP vs VS, for C#. I don't know if LSP is _ever_ going to be able to replicate the VB behavior: the documentSymbols protocol is fundamentally designed around navigation, not around generation (which the VB NavBar service is designed around). I could certainly back off my attempt to make the C# navbar service consume this, though IMO this new service should still live in the Features layer (just feels like the proper layering, as LSP should be about providing an interface to IDE features, not implementing them itself).

> @github: Last time I looked at this, the navigation bar item service is functioning in dual roles:
> 
>     1. Fetching items.
> 
>     2. Implementing the logic for navigating to those items.
> 
> 
> The former doesn't really have any logical dependency to the editor code; the later _does_ due to some tracking spans that it uses. Splitting it into two services where the former lives in Features and the latter lives in EditorFeatures I think should be doable.

In VB, these 2 steps are fundamentally conflated, because it needs to determine a bunch of information about generating items when fetching them."
"I am not aware of a reason for the register logic equivalent not to have the same issue. It is possible we have an bug there, the code sequence I described above, while legal, is incredibly rare, implying sparse test coverage at best."
"Which callsite to `Get_CORINFO_SIG_INFO` looks problematic? Worst case we need a JitInterface change. Or we could also smuggle the ""disable marshalling"" bit in `sig->flags` if getting the scope is hard."
"I've analyzed top (~12) PerfScore regressions in the benchmarks on Win-x64, nothing stands out as ""definitely bad"", the common theme is that we save an additional register (in prolog/epilog) to enregister a CSE. There was one case where a CSE was ""live across a call"", thus spilled/reloaded for no benefit, but we didn't quite know that in the heuristic because the call in question was a write barrier.

I then took a more cursory look at the tests collection on x86, and found a few quite questionable cases ([the worst](https://www.diffchecker.com/94bh94mG)), though overall the size diff was positive (`-100K`).

Finally, I took a look at Win-x64 tests collection, and found a problem that is similar to the one above: [diff](https://www.diffchecker.com/LkpAkB60), where we save/restore (a lot) for a simple (FP) CSE.

Win-Arm64 collections do not appear to have particularly bad examples."
"```suggestion
        /// <returns>The status of the registered garbage collection notification.</returns>
```"
"Tagging subscribers to 'arch-wasm': @github
See info in area-owners.md if you want to be subscribed.
<details>
<summary>Issue Details</summary>
<hr />

This PR introduces a system for defining custom JSâž”Managed and Managedâž”JS marshaling primitives so that any* user-defined type can cross between environments seamlessly. The ideal end goal is to use this new system for marshaling types like ```Uri```, ```DateTime``` and ```Task``` so that they can be linked out if they are not used.

First, some sample code:
```csharp
        public struct CustomDate {
            public DateTime Date;

            private static string JSToManaged_PreFilter () => ""value.toISOString()"";
            private static string ManagedToJS_PostFilter () => ""new Date(value)"";

            private static CustomDate JSToManaged (string s) {
                return new CustomDate { 
                    Date = DateTime.Parse(s).ToUniversalTime()
                };
            }

            private static string ManagedToJS (ref CustomDate cd) {
                return cd.Date.ToString(""o"");
            }
        }
```

General overview:
* The author of the type exposes static methods with specific names and signatures on the type so that the wasm bindings layer can find them.
  * TODO: We need some straightforward way to ensure that the linker doesn't remove these methods from wasm builds unless the type itself is not referenced
* The required methods are a JSâž”Managed mapping and a Managedâž”JS mapping. The types used by these methods have to (at present) be basic types that can cross through the bindings layer without assistance, i.e. double or string. In the future I hope to expand on this to allow marshaling spans or arrays.
* In addition to the direct JSâ¬ŒManaged mapping, you can optionally define additional methods that return JS expression filters. Those expression filters will be evaluated on the JS side of the boundary to process the value - for example since neither JS ```Date``` or C# ```DateTime``` can cross the bindings boundary directly, you can marshal it as a string and then use a filter to map it to/from the JS ```Date``` type.
  * TODO: It would be ideal to expose these as ```const string``` instead of function getters, but right now the only straightforward way to get at managed values is through function calls.
* Alongside all this we add a new ```'a'``` type specifier for method signatures (the strings you pass to ```call_static_method```, ```bind_static_method``` etc) that basically means 'figure it out'. When you use this specifier, the bindings layer will examine the target method and identify the best fit for that parameter. This is meant for the purpose of passing user-defined types so you don't need to think about whether they're classes or structs, but it also can be useful in other cases.
  * The downside is that any signature using this type specifier ends up being method-specific, so you will end up with method-specific generated code instead of the existing generated code that is shared by all methods with a given signature. There's room for some improvement here.

More detailed notes:
* Before this PR we had zero support for passing structs across the JSâ¬ŒManaged boundary in any circumstances (other than DateTime).
* The JSâž”Managed conversion has to box the resulting value (if it's a struct), which introduces some unpleasant overhead. This ends up being less inefficient than you'd think (because currently we have to box *all* values being passed to managed methods from JS - not really sure why) but it's still an opportunity for performance improvement between eliminating some copies and ensuring the GC isn't involved.
* The introduction of support for marshaling custom classes introduces a bit of overhead for classes without custom marshaling implementations, because of the additional check added to the existing Managedâž”JS flow. However, the new check is extremely cheap and the existing flow was very slow ðŸ˜Š 
* There are corner cases where this new system will not run (throwing a runtime exception) or will otherwise fail (for example if the signature of your methods is weird you can get garbage values). I think many of these can be tightened up if we write more test cases and add more error handling and checks to match.
* Many scenarios might call for an automated way to pass JSON blobs or raw byte data across the boundary. You can at least use filters (i.e. ```JSON.parse(value)```) to simplify the former, but the latter really demands integrated support for passing ```Span```s around and I have no idea how we'd do this in the existing setup.
* We could consider having some sort of default fallback marshaling implementation based on ```BinaryFormatter``` or something like that, but it seems out of scope. This system is at least relatively easy to extend to do that as a fallback.

Also in this PR:
* Additional test coverage for some of the existing code, like being able to pass JS ```Date```s. I think that was actually broken, but Blazor works so whatever...
<table>
  <tr>
    <th align=""left"">Author:</th>
    <td>kg</td>
  </tr>
  <tr>
    <th align=""left"">Assignees:</th>
    <td>-</td>
  </tr>
  <tr>
    <th align=""left"">Labels:</th>
    <td>

`* NO MERGE *`, `arch-wasm`

</td>
  </tr>
  <tr>
    <th align=""left"">Milestone:</th>
    <td>-</td>
  </tr>
</table>
</details>"
"```suggestion
                var lowNibbles = Ssse3.Shuffle(Vector128.CreateScalarUnsafe(tupleNumber).AsByte(), s_shuffleMask);
```
Should work too, as on L426 garbage is masked out anyway."
"It produces incredibly long paths. With us including rids in the output path, this goes over the windows long path limit:

D:\work\aspnetcore\.dotnet\sdk\5.0.100-preview.6.20310.4\Microsoft.Common.CurrentVersion.targets(4651,5): error MSB3030: Could not copy the file ""D:\work\aspnetcore\src\ProjectTemplates\BlazorTemplates.Tests\bin\Debug\net5.0\TestTemplates\AspNet.blazorhostedindividualuld.1zbj1ytj2fl\Client\obj\Release\net5.0\browser-wasm\staticwebassets\AspNet.blazorhostedindividualuld.1zbj1ytj2fl.Client.StaticWebAssets.xml"" because it was not found.

^ This file exists. 

I'm not entirely clear on why the name has to be this long. @github has more context, but shortening it seems to work.

"
"more comments were removed.  why!?

These comments are seriously important and really ensure that people reading and maintaining the code know what's happening.  I can't look at this line and have any idea why it's doing what it's doing.  Previously, i would at least have *some* idea why we had this code and what cases it was trying to support.   #Resolved"
"> his isn't how I expected we'd do it. I'd have thought PreferInlineCompletions is set by a developer or by DOTNET_SYSTEM_NET_SOCKETS_INLINE_COMPLETIONS being set, and PreferInlineCompletions is always respected by the engine. That way, we can experiment with all sockets being in this mode (by setting the env var to true) and ASP.NET can experiment with just its sockets being in this mode (by using reflection to set the property to true).

That works for experimentation.

> Prior to shipping, we'd either a) make the property public and remove the environment variable, b) delete the property but keep the environment variable, or c) delete both.

I think we need to ship two knobs:
- one that enables the inlining, which scales up the nr of engines
- one that indicates per socket the usage is safe for inlining

ASP.NET Core apps could somehow enable the inlining by default (cfr using garbage gc). And then for their sockets, indicate they are safe to inline.

Should I change to default PreferInlineCompletions to DOTNET_SYSTEM_NET_SOCKETS_INLINE_COMPLETIONS? Or maybe we keep as-is and do API productization discussion later?"
"> If this is false shouldn't we unwind the control as if the element has been set to null?

If this is false, we just keep going; the Object is active, not being collected. 

Under most conditions, when we are done with the Object we call `Dispose(true)`, which handles all of the managed unwinding, including unsubscribing from the Element's `ElementPropertyChanged` event. At which point the Element no longer holds a reference to this Object, so we don't have to worry if the Element's property changes. And the garbage collectors are free to do their jobs after that.

Under the specific conditions we're dealing with in this PR, the garbage collectors decided that the Element and the Object could be collected, and started that process. We don't get any notification of this, so there's no opportunity for us to call `Dispose(true)` and handle the unwinding. And we don't really need to - all the referenced objects are going away, so our managed cleanup wouldn't have any effect anyway. The only exception is this circumstance where the Element gets ""saved"" by a Binding turning a weak reference into a strong one. When that happens, we run into this ""partially collected/disposed"" scenario, and we have to account for it.

Eventually, the Object _will_ have `Dispose(false)` called (as part of finalization), but there's no cleanup for us to do at that point. 


"
"What would be that magic number? `int.max - 56`, or `2 billion`, or something else?

Until we have such an API, let's add it as a named constant here, similar to `DefaultInitialBufferSize = 256`.

Either approach would be reasonable.

The downside of going all the way to a ~2 billion right away, is that for output data that is around 1.1 to 1.5 billion, we are over-allocating by quite a lot (even if that is relatively fast because we only resize once, it isn't very memory efficient). So, the current approach of incrementing by half-way between current and max, has some benefit there.

On the other hand, the closer to you get to 2 billion by incrementing the size this way, that increasing exponential decay will become small again, making things slow again for those edge cases (possible even 2-100 bytes).

> with a max of 17 seconds if allocating all the way to the 2GB barrier (4K requests).

Can we hit a balance where we get the best of both worlds, and keep the worst case time/number of growths down to < 5 seconds too?"
">  it is using will not be freed until the garbage collector calls the CancellationTokenSource object's Finalize method.

Hrmm... i took a look, and CTS doesn't even have a finalizer afaict... So i'm not sure if these comments are valid.  "
"I made a point to do the worst-case visit last because I am concerned that subsequent visits will overwrite values from previous visits that get used for public API. It might be good to add some public API tests for this PR, for example to check the flow state for `x` on the RHS of `a(x = null)?.b(x = 1) == M(x)`, for example. (we would expect a maybe-null flow state)."
"Two reasons. First, using a strict equality check means that you're only testing one boundary condition: is the cursor exactly at the end of the buffer? JIT would still have to emit its own bounds check to make sure the cursor's not further past the end of the buffer or set to some garbage value like _-42_. The ""unsigned greater than or equal to"" check handles all of those checks using one comparison, and the JIT recognizes this pattern and skips its own bounds check.

Second, the pattern `buffer[scan++]` forces the runtime to dereference _scan_ into a temp, perform the bounds check, increment the temp value, write the temp value back to the _scan_ reference, and finally dereference _buffer_, __all in that order__. This is because the runtime cannot disprove that _scan_ and _buffer_ don't actually reference the same memory address, so operations cannot be reordered. (Search ""C++ pointer aliasing"" for more info.)

By reordering operations so that the dereference from _buffer_ occurs before the write back to the _scan_ reference, this allows data access to be pipelined and makes more efficient use of the CPU."
I'm seriously not changing these explicitly; why is the autoformatter renaming these?
"If there's an exception while setting up the buffers, it looks like iovCount will be maxBuffers.  Previously the finally block would see the array of handles initialized to null, but now with it stackallocated it could contain garbage.  Maybe that's related to the CI failures you're seeing."
"Downclocking can be pretty severe on 14nm parts right now:
From Anandtech's article [Sizing Up Servers: Intel's Skylake-SP Xeon versus AMD's EPYC 7000 - The Server CPU Battle of the Decade?](https://www.anandtech.com/show/11544/intel-skylake-ep-vs-amd-epyc-7000-cpu-battle-of-the-decade/8)
![image](https://user-images.githubusercontent.com/125730/85596303-4639fb80-b652-11ea-8eb7-b7cf96683dba.png)

While that is true, we are still seeing substantial perf boost on a given machine (copy pasting some preliminary results, while I'm still improving the code behind the scenes):

For 64-bit sorting, here is the original + 8way unrolled results, obtained from a shared instance running a Xeon Silver 4216 with nominal speed @ 2.1Ghz:

```
-------------------------------------------------------------------------------------------
Benchmark (<type/vector-isa/unroll>/size/threads)                      Time/N (per Element)
-------------------------------------------------------------------------------------------
BM_full_introsort/65536/threads:1                                      56.55590 ns
BM_full_introsort/131072/threads:1                                     60.63450 ns
BM_full_introsort/262144/threads:1                                     65.21260 ns
BM_full_introsort/524288/threads:1                                     69.03150 ns
BM_full_introsort/1048576/threads:1                                    73.16970 ns

BM_vxsort<int64_t, vector_machine::AVX2, 8>/65536/threads:1            20.14540 ns
BM_vxsort<int64_t, vector_machine::AVX2, 8>/131072/threads:1           21.08380 ns
BM_vxsort<int64_t, vector_machine::AVX2, 8>/262144/threads:1           22.36290 ns
BM_vxsort<int64_t, vector_machine::AVX2, 8>/524288/threads:1           23.05820 ns
BM_vxsort<int64_t, vector_machine::AVX2, 8>/1048576/threads:1          24.21020 ns

BM_vxsort<int64_t, vector_machine::AVX512, 8>/131072/threads:1          8.23500 ns
BM_vxsort<int64_t, vector_machine::AVX512, 8>/262144/threads:1          8.90877 ns
BM_vxsort<int64_t, vector_machine::AVX512, 8>/524288/threads:1          9.94376 ns
BM_vxsort<int64_t, vector_machine::AVX512, 8>/1048576/threads:1        10.70730 ns
```

So it's pretty clear the AVX512 is out-performing AVX2, post-downclocking.
There is also a very clear reason for this staggering improvement, if you are aware of two key points:
* AVX2 is missing some int64 functionality (certain int64 ops are more expensive)
* There is an extra specific AVX512 intrinsic (_mm512_compress_storeu_epiXX), which is removing the entire lookup table + cache reference that was involved for loading the lookup entry (which is a huge win)

As such, the perf bump can be thought of as *ONLY* being 2x-ish due to downclocking, with a clear expectation of seeing 3x with future 10nm parts and below.

As for int32 (which is also probably going to be part of this PR, given that we will probably dynamically switch to using int32 for smaller mark-lists):

```
-------------------------------------------------------------------------------------------
Benchmark (<type/vector-isa/unroll>/size/threads)                      Time/N (per Element)
-------------------------------------------------------------------------------------------
BM_vxsort<int32_t, vector_machine::AVX2, 8>/65536/threads:1            5.47985 ns
BM_vxsort<int32_t, vector_machine::AVX2, 8>/131072/threads:1           5.89806 ns
BM_vxsort<int32_t, vector_machine::AVX2, 8>/262144/threads:1           6.13415 ns
BM_vxsort<int32_t, vector_machine::AVX2, 8>/524288/threads:1           6.49738 ns
BM_vxsort<int32_t, vector_machine::AVX2, 8>/1048576/threads:1          7.10343 ns
 
BM_vxsort<int32_t, vector_machine::AVX512, 8>/65536/threads:1          3.70457 ns
BM_vxsort<int32_t, vector_machine::AVX512, 8>/131072/threads:1         3.94963 ns
BM_vxsort<int32_t, vector_machine::AVX512, 8>/262144/threads:1         4.12517 ns
BM_vxsort<int32_t, vector_machine::AVX512, 8>/524288/threads:1         4.35658 ns
BM_vxsort<int32_t, vector_machine::AVX512, 8>/1048576/threads:1        4.86922 ns
```

While the perf boost from AVX2 to AVX512 is smaller for int32, it is still substantial. Also, please remember these are preliminary results that are somewhat less than optimal given the lack of direct HW I'm experiencing, and that fact that everything I do with AVX512F is simply more painful because of this.

The benchmarks and code are all up-to-date and accessible from the [vxsort-cpp](https://github.com/damageboy/vxsort-cpp)

Finally, there is also a discussion to be had about how long do the downclocking effects linger on **POST** sorting.
As to that, the long answer is that Travis Downs (@github) measured this independently: https://travisdowns.github.io/blog/2020/01/17/avxfreq1.html

The short answer is no more than ~700usec:

What | Time | Description
-- | -- | -- | 
Voltage Transition | ~8 to 20 Î¼s | Time required for a voltage transition, depends on the frequency 
Frequency Transition | ~10 Î¼s | Time required for the halted part of a frequency transition 
Relaxation Period | ~680 Î¼s | Time required to go back to a lower power license, measured from the last instruction requiring the higher license 

It is important to note that on server CPUs, this normally applies for individual cores, where on client CPUs this is ""global"".
For 14nm there are currently only server parts, with the exception of Intel Icelake CPUs.

I think that given we are saving multiple milliseconds with this optimization, even while increasing the mark-list, and paying with 700usec of reduced frequency post that time-saving window directly is more than a reasonable and temporary sacrifice.

My conclusion from this is that the current ""state of the union"" is roughly the worst it can get (although multi-threaded results are missing, due to me not having access to a dedicated machine where I could produce meaningful results: I barely respect the results I have, in that sense since this is not a machine I can kick people off of).

As you mentioned, Icelake and Tigerlake make things only better, and it is hard to imagine AMD doing worse once they come around to supporting AVX512, probably sometime around transitioning to TSMC 5nm.

I still think I can kick things up with a few more iterations (e.g. coming weekends). Peter has inspired me with an idea that can truly go ballistic, and I have some more ammo left regardless. So this really is as ""bad"" as it gets.

Hope this helps clear the fog and justify the risk.

"
"> I removed it because its behavior is wrong on big-endian. I ran the benchmark for 32bit and there is a ~4% regression in the worst-case.

Rather than deleting it, can you instead just make it:
```C#
if (IntPtr.Size == 4 && BitConverter.IsLittleEndian)
```
?"
"We generally don't worry about those kinds of races - the worst that can happen is that we compute the same thing twice. 95% of time the thing we're computing is actually cheap to compute anyway so chances of that mattering are pretty low.

I'm not sure the double-check would actually guarantee we don't compute the same thing twice - the delegate is called outside locks, so we could still end up computing the same things twice."
"Yea, the dispatcher call does. Otherwise it doesn't know how to `resume_foreground`. Yes, seriously, it's mental."
"Ooo I hadn't thought of that. I like it! Lemme give it a whirl. (Always wanted to implement the TryX pattern, seriously.)"
"> It may be interesting to microbenchmark this case, in addition to looking at the disassembly, to see whether the missed inlining makes a difference.

Here's a benchmark of invoking an async method that does basically nothing, so this is purely about the overheads of the infrastructure and the worst case for this:
```C#
public class NonGeneric
{
    [Benchmark]
    public async Task<string> NonGenericWork()
    {
        for (int i = 0; i < 10000; i++) await GetValueAsync();
        return default;
    }

    private async Task<string> GetValueAsync() => default;
}

[GenericTypeArguments(typeof(string))]
[GenericTypeArguments(typeof(int))]
public class Generic<T>
{
    [Benchmark]
    public async Task<T> GenericWork()
    {
        for (int i = 0; i < 10000; i++) await GetValueAsync();
        return default;
    }

    private async Task<T> GetValueAsync() => default;
}
```

I get these numbers:

|            Type |         Method |        Job |           Toolchain |     Mean |   Error |  StdDev | Ratio | RatioSD |
|---------------- |--------------- |----------- |-------------------- |---------:|--------:|--------:|------:|--------:|
|  Generic<Int32> |    GenericWork | Job-HGAKDB | \master\corerun.exe | 142.7 us | 2.35 us | 3.36 us |  1.03 |    0.03 |
|  Generic<Int32> |    GenericWork | Job-KAESXO |     \pr\corerun.exe | 139.3 us | 0.67 us | 0.56 us |  1.00 |    0.00 |
|                 |                |            |                     |          |         |         |       |         |
| Generic<String> |    GenericWork | Job-HGAKDB | \master\corerun.exe | 227.0 us | 0.95 us | 0.79 us |  0.87 |    0.00 |
| Generic<String> |    GenericWork | Job-KAESXO |     \pr\corerun.exe | 260.6 us | 0.95 us | 0.79 us |  1.00 |    0.00 |
|                 |                |            |                     |          |         |         |       |         |
|      NonGeneric | NonGenericWork | Job-HGAKDB | \master\corerun.exe | 135.2 us | 1.15 us | 1.08 us |  1.03 |    0.01 |
|      NonGeneric | NonGenericWork | Job-KAESXO |     \pr\corerun.exe | 131.4 us | 0.42 us | 0.33 us |  1.00 |    0.00 |

So, the non-generic case and the value type case stay basically the same, maybe a smidgen better.  The generic case regresses by 15%.  But that is a super extreme.  While it's unfortunate the lack of inlining contributes such overhead here, I'm tempted to say it's acceptable. (Though it'd be wonderful of course if JIT improvements could remove it in the future.)"
"The native implementation you added currently doesn't write to the status info on error, which means a caller could see statusInfo containing garbage. This function should respect the `out` and guarantee that statusInfo is initialized, either ensuring that the P/Invoke always does so or here default initializing it (`statusInfo = default;`) before calling into the native code."
actually I don't set these in gc1 'cause the code that sets it in gc1 is only defined for regions but I still wanted to be able to assert that they are not garbage values so I init-ed them in init_gc_heap.
"> So this means we're registering one per language

You can re-register an editor factory as many times as you want. because its keyed off of the guid it will still only create one and have one registered. So worst case scenario we register it twice, if there are both C# and VB projects. But we get the advantage of only taking over editorconfig files if a package loads for a programming language we understand"
"I've looked at this code several times and I am not convinced this is the right solution.  Having code which only throws on test does not appear to be the right solution.  The test code can control the `Action` which is passed here and can take other completely supported product actions:
- Turn the exception into an error.
- Save the exception and not its value in the test after the scenario is complete.
- Simply fail because the exception happened.  

Adding rethrow logic here is missing the point of the original bug: analyzer exceptions should never propagate as exceptions to the caller.  

I think we need to seriously consider just changing the test code here.  If the test code cannot operate on the API as defined and needs the `testOnly` members then it suggests strongly that our API is incomplete.  
"
"`case` is worst than `if` for JIT? I did not know this, Did you have any links to I can read more about that?

thanks"
"> Are you sure that it instead refers to that tracked address potentially remaining uninitialized and causing the GC to track garbage? 

Yes, I think it may cause the GC to track garbage.

> As in, it seems that CWT<K, V> is relying on that working fine, given that it regularly reuses allocated handles by just setting targets to null and then eventually just updating them with a new target and then dependent?

I do not think that CWT is reusing the handles. It sets the target to null to neutralize the handle, but it does not ever reuse it by setting null back to non-null. `CreateEntryNoResize` will always create a fresh handle."
"Nice! The worst case for this algorithm is a small string + value that doesn't exist in it (its first character), e.g.:
![image](https://user-images.githubusercontent.com/523221/147987132-0e48cc05-f049-4712-9801-c5b2d64ddf53.png)

for larger strings (once AVX path is kicked in) the regression disappears.

As a possible solution we can use the old one till the first hit (`IndexOf(firstChar)`) and switch to the new algorithm after. So in cases like ^ we only will do a single IndexOf that won't find anything."
"I did some digging and confirmed (from the code and by testing the same program) that this change doesn't affect non-container execution.

Basically, the method being modified `GetCGroupMemoryUsage` is only called (through `GetPhysicalMemoryUsed`) from `GCToOSInterface::GetMemoryStatus` and only if `restricted_limit` is set: https://github.com/dotnet/runtime/blob/69b9000671f1b73b3fa17810a9f6cb31abb612a2/src/coreclr/gc/unix/gcenv.unix.cpp#L1326
`restricted_limit` is only set to a non-zero value if `is_restricted_physical_mem` is set to `true` in [gc.cpp](https://github.com/dotnet/runtime/blob/69b9000671f1b73b3fa17810a9f6cb31abb612a2/src/coreclr/gc/gc.cpp). And this seems possible in only 2 cases:
1. Running in containers.
2. If GC config `GCTotalPhysicalMemory` is set. However I'm not sure if this is a valid scenario since this config is not documented [here](https://docs.microsoft.com/en-us/dotnet/core/runtime-config/garbage-collector) and when I run the test program with this config set to some limit, even with the latest .NET release it seems to be broken (reports `MemoryLoadBytes` significantly higher than this limit, and the program still finishes successfully)."
"I'm not sure what the point of this is. The string is utf-16 before and after. At best some characters would get filtered out, or at worst some would throw."
"@github are you manually updating these logger messages? I used a code-fix to do this in the past because it's far too easy to introduce errors / typos with such a large number of messages and it's incredibly difficult to review these in a PR. https://github.com/pranavkm/LoggerConvert has a version of the codefix, perhaps you want to give that a go (in a separate branch so you don't lose these changes)?"
"Actually, for the ArgIterator, it is worse, as we create an instance for every function call to generate GCRefMap. So we would repeat the classification over and over and also possibly create a lot of memory garbage unless we can somehow get the current thread's CorInfoImpl (or propagate the current CorInfoImpl down to the place where we create the ArgIterator instance). I'm not sure how reasonable that would be."
"FWIW, getting a builder is pooled, so if you're using `null` just to avoid allocations or overhead, getting an ArrayBuilder instance is incredibly cheap. #ByDesign"
amazing! (seriously).  awesome job!
"Thanks for pointing out the leak!

> Is this to capture the listener from onCancellationRequested?

Yes.

> This shouldn't be a problem when the store is always disposed.

You are right, it does not need to be disposed here as the disposables inside of `CancellationTokenSource` are not tracked.
It is very important to always dispose the event subscription though, otherwise the tooling introduced by this PR will recognize that as leak (even though garbage collection does take care of it)."
Can the `Uri` constructor throw (like if you pass in a garbage URI)?
"> if value doesn't have a null terminator?

This is all interop. If you give it garbage native data, that's on you, and behavior is undefined, just like with strlen.  Most likely it'll crash.

>  if value is null

We could catch this and throw for it or return a default span.  Right now it'll null ref.  I don't have a strong opinion."
"Thanks. To understand worst case, can you try counting a long string of all a's with a pattern that's just a single a? "
"Added a comment with rationale, also pasting here since it's interesting:
```
// The following dictionaries are used as caches for operations that recurse over the structure of SymbolicRegexNode.
// These operations are called potentially on every step of the matching process, and they may do linear work in the
// of the pattern in each call. Thus, caching is necessary to avoid a quadratic worst-case over multiple steps of
// matching when simplification rules fail to eliminate the portions being walked over.
```
Currently with the stress tests we have removing these caches wouldn't be a problem. However, we don't have a proof that our current simplification rules always avoid these worst-cases. There might be hope for removing some of these caches with future work on the theory side."
"I haven't measured, but it's my understanding that in the worst case (when headers+body) happen to fit in a single TCP packet, you'll need an extra call to Http.Sys to schedule another IOCP callback.

@github ?"
"Ughhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh

can we have a CommonAwaitExpressionInfo? (seriously...)?"
"The overload accepting a char[] destination was only called from ReCreateParts, after [this conservative worst-case capacity calculation](https://github.com/dotnet/runtime/blob/c991d9bc35b38016d8a48e745f842b648678fb9a/src/libraries/System.Private.Uri/src/System/Uri.cs#L2711-L2718). I believe it was thankfully unreachable code, as we never needed a resize."
"Did a first quick review of the PR.
It would be great if this also worked when Drag and Drop from the desktop. Since currently it only works when drag and dorp from inside the Explorer. And ideally those two experiences would be the same.

I have also found out that the mac finder behaves like the explorer currently does, before this PR. So leaving it like it is is not the worst solution.

Apart from that: we do not ask the dialog for deeply rooted children correct, we just check for differences on the first level? Which I think we should just do it on the first level."
"We seriously don't have an existing resource with ""Windows Terminal"" already?"
"Previously this would throw a NullReferenceException if `library` was `null`. Now this will append garbage into the string.

We shouldn't change behavior here."
"I intentionally avoided VSB here for two reasons. First, though the expectation is that lines are under 100 chars, that's certainly not a requirement. Second, since connections can drop and throw exceptions mid-read, we could generate garbage. My experience is that StringBuilder is a bit more forgiving than VSB at this, since if a failure occurs while VSB is on the stack, it negatively impacts the performance of other components which are using the array pool.

Basically, I don't foresee better perf in the typical use case, and I do foresee worse perf in the extreme use case, when compared to a standard StringBuilder. It didn't seem like a good tradeoff since trying to account for this would involve complicating the code."
"> Is this a regression? What is the impact of this change for someone using Large Pages + hard limit?

@github - In short - yes, it is - and this change will make large Pages + hard limit better.

The story started with [your discovery](https://github.com/dotnet/runtime/pull/35802#issuecomment-624337471) that the 3x commit introduced by POH is a blocker for you. 

The root cause of the 3x commit is because when given a single hard limit, we do not know if the application is going to use that for which object heap, and therefore we assumed the worst. 

For the normal scenario without large pages, that is okay because we just reserved the memory without committing it.

With large pages, implementation forces us to commit upfront, and therefore we committed 3x the hard limit, and that is not okay

Therefore I introduced a new way to specify the hard limit per object heap, that eliminated the guesswork the runtime is doing:

- https://github.com/dotnet/runtime/pull/36731
- https://github.com/dotnet/runtime/pull/37166

By specifying the hard limit for individual object heaps, the runtime will commit exactly as specified.

As an example, I wish to have 1G in SOH, 2G in LOH, and 500M in POH, you would specify
```
COMPLUS_GCHeapHardLimitSOH=1G (in hex)
COMPLUS_GCHeapHardLimitLOH=2G (in hex)
COMPLUS_GCHeapHardLimitPOH=500M (in hex)
```
The runtime will reserve (or commit in large page case) 3.5G upfront and distribute them as instructed.

Just to be clear, if the application happens to allocate more than 1G in SOH, then it will OOM, regardless of whether or not we still have memory in LOH or POH.

After that, I worked on testing it. After https://github.com/dotnet/runtime/pull/37725, the code works functionally, but it has a performance bug. When individual heap hard limits are provided, the heuristic to determine which generation to condemn is broken. In particular, it chose to perform a gen 2 background GC always.

This is caused by my ignorance. I thought the field `heap_hard_limit` is used only for checking whether or not we exceed the limit, turn out it is also used to determine available memory, and thus impact the choices the heuristic would make. 

By setting `heap_hard_limit` to approximately what it should be, this change fixed the heuristic, and it will choose the same generation to condemn as it sees fit.

So for a longer summary:

- The 3x commit issue is solved
- My implementation had a couple of bugs, but they are fixed by now."
"It makes the full queue benchmark slightly slower, and the one-at-a-time benchmark slightly faster. The effective change is that garbage collection happens as the queue empties, rather than as it fills back up. This shifts work earlier, and into times where the server has more capacity. Good change."
"The semantics of clone have been a garbage fire since .NET Framework 1.0 ðŸ˜„ "
"this is very costly.  each `+=` will generate a new string (and the previous one will be garbage).

1. see if we have a helper that produces one string out of `classifiedText.Runs` elsewhere.
2. if not, then use a PooledStringBuilder so you can do this without garbage."
I'll try it out. Hopefully it works because `PrimaryWorkspace` was the worst part of this.
"I can't tell what you're advocating for; your comments contradict each other.

The pattern you're citing from MSDN is exactly what's already happening in that current code, except that their version sets the `disposed` flag to true after executing the `if(disposing)` block instead of before. (Whether it's better to set that flag before or after is a separate discussion, but I believe that ""before"" is the safer option.)

> When you call Dispose(false), it sets _disposed to true but does NOT actually dispose (the if block doesn't get executed).

""the if block doesn't get executed"" is not the same thing as ""does not dispose"". The `if(disposing)` block is for cleaning up managed resources when our code calls `Dispose()`. We can't clean up managed resources during `Dispose(false)` because we're being finalized, and the state of the managed resources is indeterminate. (It wouldn't do much good anyway, since by this point they're likely being garbage collected.)

> When you call Dispose(true) later on, it assumes the object is already disposed, so it returns.

That's not a thing. `Dispose(false)` signals that the finalizer is calling `Dispose()`; the object is being destroyed/finalized. There is no subsequent call to `Dispose(true)`. 

The converse is also true; if `Dispose(true)` has been called, `Dispose(false)` never happens, because the root class's `Dispose()` implementation calls `GC.SuppressFinalize()`.

> MasterDetailPageRenderer.cs and NavigationPageRenderer.cs have the correct implementation.

Their implementations are subtly wrong. In both, if `Dispose()` is called by the finalizer (i.e., `Dispose(false)`), `_disposed` will never be set to true, and nothing will prevent a double-Dispose situation. In those two classes we happen to be fine because there are no unmanaged resources to handle (i.e., nothing outside of the `if(disposing)` block), so no disposal logic gets run multiple times. We _are_ incurring a slight performance penalty, though, and that's something I hope to fix soon.
"
"I believe this got to a point which is incredibly hard to understand. I decided to take 30 minutes to unwrap it, got lost in all the spaghetti and gave up.

I suggest to document the exact order and direction of all the expected messages during handshake, as well as what should happen between each message.

When documenting, I suggest to drop the client/server nomenclatures and simply talk about _shared process_ and _main_, especially because the roles are reversed when referring to the Electron IPC communication vs. the named pipe communication, via which both processes are connected:

1. When the shared process is spawned, during its handshake, it acts as a client via Electron IPC
2. After the handshake, while communicating to main, it acts as a server via the named pipe

Feel free to merge if it works. :+1: "
"Seriously, on every branch the feature tests fail? Is there a problem with the CI and running from forks or something?"
"Done.  I made the buffers one character longer also to accommodate there always being a null char even at max actual length.  I seriously doubt it would ever get there with this particular API's typical values, but you never know."
"CWT is basically pool. why create so many redundant weak reference objects? if there are 300 analyzers (basically install style cop analyzers), worst case will be 300 x number of projects. which pointing to same workspace. why just use 1 rather than creating all those weak reference?
"
"@github sorry this has taken so long, but weâ€™ve decided this is looking good for the 4.2 release, which means we could merge as soon as 4.1-rc is out. The type checking behavior should behave just like there are colons in the element names and element attributesâ€”no nested objects. This means the type checking should already work, but it would be good to see a test or two:

```ts
// @noImplicitAny: true
// @jsx: preserve

declare namespace JSX {
  interface IntrinsicElements {
    ""ns:element"": {
      ""ns:attribute"": string;
    }
  }
}

const e = <ns:element ns:attribute=""yep"" />;
```

As an aside:

> By making it nested, it is possible to reuse attributes... Without this, I have to write `""namespace1:attr1"": attributes['attr1']`, `""namespace2:attr1: attributes['attr1']` because ts can't calculate string literal in type level

I think this should be possible with #40336 now ðŸ˜„ 

We also decided that thereâ€™s not a great reason to limit this to `jsx: preserve`. `jsx: react` can be used for more than just React with JSX pragmas, so it seems overly prescriptive to disallow it. Youâ€™ll still get type-checking errors if you try to put attributes with colons on the intrinsic elements defined by @types/react, because those intrinsic elements lack any attributes with colons. You would not get type errors on your own component if you defined its props as `any`, or included prop names with colons, and I guess React wouldnâ€™t like that, but that seems outside the scope of what TypeScript is supposed to help you with.

We also need to make sure that we gracefully error on `<Capitalized:component />` and hopefully not emit total garbage with `--jsx=react`, since this doesnâ€™t make any sense.

Thanks for your patience! Iâ€™ll be keeping an eye out to make sure this is ready for 4.2 and to help with whatever you need."
"> I am simply curious why we need to keep the usage of DiagnosticSource with Activity?

While I wouldn't encourage most users to do it, there are still some scenarios that can't be accomplished any other way. We also haven't yet replicated every relevant piece of useful guidance onto the official docs for the people who aren't using DiagnosticSource with Activity. I am making a bet (and perhaps it is a bad bet) that the people who find this doc and choose to keep reading it after seeing the obsoletion warning at best will learn something useful and at worst they'll decide it was a waste of their time to read and forget what they read in it."
"Thanks - added comment:

```
    // Define a separate cache for each base href, so we're isolated from any other
    // Blazor application running on the same origin. We need this so that we're free
    // to purge from the cache anything we're not using and don't let it keep growing,
    // since we don't want to be worst offenders for space usage.
```

It's not enough to rely on the browser clearing stuff from its cache when storage space is low. End users short on space (especially on mobiles) do look at the lists of which sites are using the most space and will have a negative opinion of the ones at the top of the list, and are more likely to target those for manual removal from the cache."
"I removed it because its behavior is wrong on big-endian.
I ran the benchmark for 32bit and there is a ~4% regression in the worst-case.

If we believe this kind of optimization is necessary, I can add an alternative version of it - initial testing showed it can save ~15% from ctor time for simple Uris."
"I seem to recall it was skipped due to streaming issues.  But since the link no longer works, we won't be able to diagnose it.  The handler as well has been through a bunch of issues since the issue was filed, so it may not even be applicable.  If these are running stably for you, I think we should just unskip all these.  At worst we have to disable them again, but we'll have updated diagnostic information."
"> Since the code order is exactly the same, it shouldn't have any other side effects.

The managed code is in GC safe mode and the unmanaged code is in GC unsafe mode. The transition happens at different point in this PR which may have a side effect. Now the garbage collector can run between the two statements while it previously could not.

That said, in this particular case I don't see any issue with it."
"@github You say that, and I've said that, but that's not true. We parse `jsdoc` even in TS files for the LS (that's why you can still use `Go To Definition` on a typedef in a `.ts` file) - we just don't consider them as part of the types of any associated nodes in the checker. Seriously - `addJSDocComment` in `parser.ts` is unconditional. Parts of the binding process are also unconditional (not typedef binding, that's only done in JS; but parent pointer setup is always done)."
"> Why exclude them in release builds? If the state and conditions being valid is important we 
> should validate it at runtime in all cases, unless the performance hit is too significant. Not
> validating it could lead to larger bugs and (in the worst case) potential security problems

CoreCLR traditionally doesn't check asserts in the release build.

For Mono, `EP_ASSERT` is defined as `g_assert` which is always checked. We also have `g_assert_checked` - which is rarely used - which is only defined when configured with `--enable-checked-build=all`.  We can switch to that, or add an additional `EP_ASSERT_HEAVY` for particularly expensive checks."
"So my take on this is: 

I think the existing parser code is trying to be way to 'slim' and goes through incredibly complex hoops to try to figure out what's going on.  A *much* simpler approach, for me, would be to

1. mark the starting point of the potential modifiers.
2. eagerly consume *anything* that could be the modifiers of a type/member, and keep track of if you saw contextual modifiers.
3. If you didn't see partial/async (or any other contextual modifiers we add in the future), great! Just return the modifiers you parsed out.  Do not reset, just release your rewind point.
4. if you did have contextual modifiers, now actually examine them to ensure they're ok.  this is hte point where we can do complex checks between them and hte other modifiers, and/or the tokens that follow.
5. If you realize that one of these contextual modifiers was not actually a modifier, store that index.  Rewind.  Consume modifiers up to that part, then actually go do the rest of the parsing.

This just seems much simpler and saner for me.  it feels like right now we're trying to eat a token at a time, trying to keep track of everything and doing complex incremental moving forward and peeking.  I'd just rather get the modifiers, and hten see if partial/async are used properly based on if there are other modifiers that follow and/or you can properly see the start of a member/type afterwards.





"
note: as per my conversation with @github  i think the case that VB is handling is incredibly esoteric.  I would be ok with having VB just checking for that case and *not* offerring invert-if if it helps simplify the rest of the algorithms here.  I think it's a practically irrelevant case in practice and i would be fine with no invert-if for it.
"> typically from Program Files

Wait, seriously? I thought it was supposed to be using the one restored in the repo under `<repoRoot>\.dotnet`. Seems like a bug in our test infrastructure / something we should file an issue for if we are expecting / using the system's global install."
"calling either Wait or Result on a task in background thread will make that thread wasted until the given task returns.

depends on how heavy the task is, it could sometimes block thread quite long time. so, we try to make sure anything that runs in background never calls Wait/Result.

the worst thing that can happen (which happened before) is that multiple caller wait on heavy tasks on multiple threads causing thread poll to create +1000 threads (which gets worse as it goes since more threads mean slower progress in this situation).

once lesson learned, whenever we find mis-usage like this, we fix it. even though it can propagate quite big.
"
"Thanks for the suggestion! I looped through the `_isBackwards` array one extra time to count the number to allocate. It seems a bit weird to loop twice, but having a counter for the `nonBackwardsTZIDs` in the class would be allocating for an `int` for the entire lifetime of the class, and thats worse than just the lifetime of `GetTimeZoneIds` right?

Out of curiosity, what specifically are we avoiding in allocating the list? I am seeing that a list allocates space for a 2<sup>n</sup> array where its doubled each time it runs out of space. Are we avoiding that worst case 2<sup>n-1</sup>-1 space allocation or are we mainly avoiding the extra variables being allocated through the `List<T>` object?"
"Garbage-In/Garbage-Out is hard, since there's not a good bypass in the AsnWriter, and that CA is already on shaky grounds for interop.  Rather than do something like CertificateRequest's NormalizeSerialNumber (which helps let someone do RNG.Fill and not have to worry about it) I think rethrowing it with a corrected parameter name is probably the best bet.

Hm."
"Is ByRef and IntPtr overlap a problem? We decided not to check this for ByRef-like types.

ByRef pointing to unmanaged memory or other garbage should be fine because that's what happens for Span pointing to unmanaged memory.

Or is this futureproofing for when we allow byrefs on the GC heap and we'll need to make sure these writes go through a write barriers when that happens?"
"I found https://blogs.msdn.microsoft.com/yunjin/2005/07/05/special-threads-in-clr/

>Because there is only one thread to run all finalizers, if one finalizer is blocked, no other finalizers could run. So it is discouraged to take any lock in finalizer. Also see [Maoni Stephens's blog](http://blogs.msdn.com/maoni/archive/2004/11/04/252697.aspx) for details about finalizer thread.

It recommends not taking any lock, but only because it can stop other finalizers from running. It doesn't say whether the GC suspends normal threads while the finalizer is running.

[This doc](https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/fundamentals#concurrent-garbage-collection) seems to indicate the GC will not suspend other threads in .NET Core unless concurrent garbage collection is disabled. That said, as a server, we need to make sure we don't deadlock when concurrent garbage connection is disabled considering we tell customers ""[t]o improve performance when several processes are running, disable concurrent garbage collection.""

@github Please feel free to correct any inaccuracies in my understanding.
"
"Yes, the typical case for this is that something is wrong in the setup and either the process doesn't get created at all or the command executed but fails instantly. The worst outcome of this is a potential missing warning, if we wanted to be more accurate we can attach to the exit event handler and do it there, but I don't think its worth the additional complexity"
"Setting the length to zero clears the file, so you could end up with garbage at the end of the file if the new file is shorter than the old one."
"We are, but I have been mostly focusing on C#, as a reultVB side might not be well thought out and needs more work. Given this is an experimental feature, it's probably OK worst case we just disable it for VB for now."
I also don't like it but I wasn't aware it's considered legacy now. There is nothing in the docs that says so: https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/implementing-dispose
Is this seriously the only way to do `isMember` with a `string_view`?
"But what if we have 10,000 preprocessor keywords? :smile:

More seriously, not sure if easier to just make the sortText be ""_0"" + name; that way they stay together due to the common prefix but are otherwise sorted in whatever way they should be sorted."
"This is going to be ""expensive"" for production code. I think we can just insert `null` at the first correct position.

We already have what could be considered ""garbage"" data in the rest of the span due to `SkipLocalsInit` and so we rely on `\0` correctly marking the end."
This follows the disposing pattern - https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/implementing-dispose
"![image](https://user-images.githubusercontent.com/16246502/142055246-88902307-b8b9-4a9b-8b60-48bd2e472045.png)

More seriously though, should we just expose this / state in general?"
"There is no point in saving/restoring volatile registers as at return site they contain garbage.
lr,fp are saved as a part of stack alloc/dealloc, so no need to save lr again.
NZCV was not actually stored and there is no need."
"> I suppose you have run them already

I ran a worst-case test that would cause us to perform a timeout check at essentially every step, and I saw no measurable impact from calling Environment.TickCount64 at each check rather than maintaining a counter and calling Environment.TickCount 1/1000 such checks; TickCount{64} is cheap, and the other costs involved simply dominate.  This also removes the checks on paths that have less overhead and are hotter, e.g. as part of doing the initial linear match of a loop, so all of the timeout overhead goes away in those cases."
"> Expecting culture-sensitive result on en-US, but getting ordinal result?

Yes.  I find it highly questionable that people would write code that needs culture sensitive results in en-us.  And even if they did, i'm ok with that scenario regressing given the gain here.

> Expecting the same speedup as en-US on a different culture when data is in ASCII range?

I'm also ok with that.  At worst the code performs the same as today for non-en-us.  But for the common case of en-us and english code, we get a boost.  "
"> Approving on the diff, but not sure if there may be external considerations for this change. I'd hold of on merge till more approvals are in ðŸ˜„

This should be fine given how we do this for other assemblies. I need this to get access to `HashCodeCombiner`. Setting auto-merge to get the update dependencies to trigger. We can revert in the worst case ðŸ˜„ "
"I'm very glad we got that test failure, because this has identified a possible rendering bug. Admittedly it would be incredibly hard to repro, so much that it's possible nobody ever has done, but if you:

 * Set up a component that mutates the DOM in such a way that the mutation itself triggers an event (this is difficult to do, but our ""reordering focus retention"" E2E test case does it)
 * ... and then during the inner rendering loop, the component renders 2 or more times (so as to overwrite the original render tree in memory)

... then you could end up with an exception or undefined behavior.

I've implemented what I think is roughly the correct fix, but I want to add some proper E2E test cases dedicated to this edge case before merging this."
"No need for lock here.  At worst, we miss some write of 'true' to disposed.  But then the UI request that executes will immediately bail."
"This looks incredibly suspicious, as do the above two lines."
"@github @github  on this.  i'm just not getting it.  there' no reason i would expect or want to tie this option to a solution.  

> because that could have occurred while the user had a different solution open. 

If i go to option and turn on decompilaiton .i should get a prompt.  that prompt should be remembered forever.  Best case is that it's associated with my account and roamed.  Worst case it's remembered on the local machine.  But i can't understand any sort of design that ties this to somethinglike the solution or to navigation.  That makes no sense on any level, and does not give the user any sort of expected experience around decompilation that is sensible.

The fallout of that is the types of experiences i've been running into :-/"
use ArrayBuilder to make a temp buffer to copy the values into that we can then report (then free the temp buffer).  This way we're not just producing lots of garbage here.
"I found this code in particular so incredibly hard to understand that I changed it into simple C code.
In order to understand the existing code you have to understand that:
* `til::point` has a constructor for `COORD` (`options.coordCursor`)
* `til::rectangle` has a constructor for a point without size, which creates a rectangle at that origin of size 1x1
* `scale_up` multiplies the size of the rectangle, but not the origin
* `til::rectangle` implicitly turns into a `D2D1_RECT_F` using a conversion operator

The interaction between the second and third point are outright confusing for me and it took me more time to understand that code than I'd like to admit. I'd be fine with this code:
```cpp
D2D1_RECT_F rect = til::rectangle{ til::point{ options.coordCursor }, glyphSize };
```

But at that point I asked myself why you'd want to construct a til::rectangle anyways, just to turn it into a `D2D1_RECT_F` right after. If you'd like me to use that alternative code anyways, I'd be absolutely fine doing so of course! But personally I preferred the C-style code."
">  After #10615 (seriously guys, take a look and you can see how effective that PR is) and future PR lands

Dude its in my queue, it's just a big deal so I've been procrastinating. I feel seen. (I plan to look Monday... you're just catching me feeling too sleepy to ponder long term consequences...)
"
"quibble: these properties are not ""imaginary"". They are ""possibly existing"" properties.

Seriously, though, I'd say ""expando properties"". I also don't think that referencing the old implementation helps, so I'd delete that line or at least change it to ""This mirrors the behaviour of index signatures""."
"The only thing going for it is the `UseMany` scenario, and IMO it looks invalid. Notice how it *never* reassigns `queue2`, yet it repeatedly resets `queue` to be a modification of `queue2`. So we fill up a queue to be very large, then snap it as `queue2`. Then (for some reason) we enqueue one more element to `queue2` before dequeuing it. But the net effect is this loop does _not_ emulate adding and removing from a queue in sequence. All it does is exercise the very worst perf, which is ""first time dequeue"" in a loop, but made to look like a regular loop of enqueuing and dequeuing:

```cs
    [Benchmark]
    public IImmutableQueue<int>[] UseMany()
    {
        var queue2 = defaultQueue;
        var result = new IImmutableQueue<int>[N];
        for (int i = 0; i < result.Length; i++)
        {
            var queue = queue2.Enqueue(i);
            queue = queue.Dequeue();
            result[i] = queue;
        }
        return result;
    }
```

So no, that's not a scenario I've ever seen nor would expect to ever see and we should not optimize for it, particularly at such great expense to the common use cases. 

@github, I think a fair `UseMany` scenario (if we define it as a rather large queue that is repeatedly added to and removed from) would be something like this:

```cs
    [Benchmark]
    public void UseMany()
    {
        var queue = defaultQueue;
        for (int i = 0; i < N; i++)
        {
            queue = queue.Enqueue(i);
            queue = queue.Dequeue();
        }
    }
```

That I expect to perform *much* better, since the long queue length setup before this method runs would only be ""flipped"" occasionally instead of on every single dequeue operation."
@github that's what I'd recommend given it's incredibly easy to verify it's correct and you presumably want to merge today. But I've no objection to this (if perhaps someone else reviews)
"`workload install` is installing `6.0.2-mauipre.1.22102.15` version for the packs, which it gets from the mono.toolchain manifest for `6.0.300` .
```
  ** workload install **

  Running: /workspaces/runtime/artifacts/bin/dotnet-workload/dotnet workload install --skip-manifest-update --no-cache --configfile ""/tmp/elz3pk5g.cfm"" wasm-tools
  Using working directory: /tmp/
  Installing pack Microsoft.NET.Runtime.WebAssembly.Sdk version 6.0.2-mauipre.1.22102.15...
  Skip NuGet package signing validation. NuGet signing validation is not available on Linux or macOS https://aka.ms/workloadskippackagevalidation .
  Writing workload pack installation record for Microsoft.NET.Runtime.WebAssembly.Sdk version 6.0.2-mauipre.1.22102.15...
  Installing pack Microsoft.NETCore.App.Runtime.Mono.browser-wasm version 6.0.2-mauipre.1.22102.15...
....
                     Garbage collecting for SDK feature band(s) 6.0.300... (TaskId:444)
                     Successfully installed workload(s) wasm-tools. (TaskId:444)
```"
"If it is primarily for UTF-16 to UTF-8/etc, then 3x is the upper bound.

single UTF-16 character -> 1-3 UTF-8 bytes
surrogate pairs (i.e. 2 UTF-16 characters) -> 4 UTF-8 bytes (i.e. factor is still <= 3x)

If we expect arbitrary encodings, then ignore my comment, since I don't know what the worst case factor would be in that case."
"Yes, I had the same concern and experimented a bit with different syntaxes. Worst case scenario (unlikely)  we get a ""smalish"" bug here, that has an easy workaround and is easy to fix."
"Can these allocations for the intermediate string be avoided?
In the worst case it's 6 allocations. 
One for the substring, then one from string.Replace and another one for UnescapeDataString. And this for name and value, so 6 in total.

Ideally Uri.UnescapeDataString should have an overload for {RO}Span<char> too, so some intermeidate allocations can be avoided.

So should the queryString be copied to a Span<char>-buffer on which is operated?
The substring will instead be a slice of that span, avoiding one allocation.
string.Replace can be done with a helper on that span-slice (unforunately there's no Span<T>.Replace-API / extension).
Instead of 3 allocations there's only 1 left, and with Uri taking a span then this is can go away too.

<details>
   <summary>Helpers code</summary>

To replace `+` with ` `, and assuming length won't be short enough, so no vectorization needed:
```c#
internal static class MySpanExtensions
{
    public static void ReplaceInPlace(this Span<char> span, char oldChar, char newChar)
    {
        foreach (ref char c in span)
        {
            if (c == oldChar)
            {
                c = newChar;
            }
        }
    }
}
```

Or with vectorization (x86 only): Use methode `ReplacePlusWithSpaceInPlace` for best codegen.
```c#
internal static class MySpanExtensions
{
    public static void ReplacePlusWithSpaceInPlace(this Span<char> span)
        => ReplaceInPlace(span, '+', ' ');

    [MethodImpl(MethodImplOptions.AggressiveInlining)]
    public static unsafe void ReplaceInPlace(this Span<char> span, char oldChar, char newChar)
    {
        nint i = 0;
        nint n = (nint)(uint)span.Length;

        fixed (char* ptr = span)
        {
            ushort* pVec = (ushort*)ptr;

            if (Sse41.IsSupported && n >= Vector128<ushort>.Count)
            {
                Vector128<ushort> vecOldChar = Vector128.Create((ushort)oldChar);
                Vector128<ushort> vecNewChar = Vector128.Create((ushort)newChar);

                do
                {
                    Vector128<ushort> vec = Sse2.LoadVector128(pVec + i);
                    Vector128<ushort> mask = Sse2.CompareEqual(vec, vecOldChar);
                    Vector128<ushort> res = Sse41.BlendVariable(vec, vecNewChar, mask);
                    Sse2.Store(pVec + i, res);

                    i += Vector128<ushort>.Count;
                } while (i <= n - Vector128<ushort>.Count);
            }

            for (; i < n; ++i)
            {
                if (ptr[i] == oldChar)
                {
                    ptr[i] = newChar;
                }
            }
        }
    }
}
```
</details>"
"What's keeping this rooted?  You're probably going to need to add this Timer as a field to TimeoutState, such that the Timer ends up rooting itself.  Otherwise, it looks like if garbage collection kicks in during the operation, the timer could get collected and never fire.  We should ensure we have a test for that."
can you make these all `{ get; } =`.  there's no point allocating this garbage over and over again.
I'm wondering if we should just do the work to find the `MasterDetailPage` again here inside this `if` statement. I don't like the idea of holding a reference to a child page inside of a `NavigationPage`. Seems like we might get a memory leak at best and an `ObjectDisposedException` at worst if the hierarchy changes and that reference doesn't get updated soon enough.
"Oh. Yikes. When it was broken in the debugger, it looked like the string was full of random memory. I guess I assumed that the view was empty, but secretly actually filled with garbage."
"> Can you clarify this? Can the new service return a different type of info that the feature layer can decide what to do with? the LSP versoin can dump them, the VS version can use them.

My concern is all the event-based logic here: http://sourceroslyn.io/#Microsoft.CodeAnalysis.VisualBasic.EditorFeatures/NavigationBar/VisualBasicNavigationBarItemService.vb,113. The logic around `WithEvents` and `Handles` is incredibly VB-specific, and I don't see how it can be exposed in a language-agnostic way without the VB NavBar service basically just using the service to get the types in the file then recomputing all the nested symbols it needs because it needs to determine whether they're `WithEvents` or `Handles`, dig out the events they handle, and reorder.

> In other words, i don't get what this system is buying us given htat we still hvae LSP vs VS, and we still have C# vs VB.

Well, this does allow us to collapse LSP vs VS, for C#. I don't know if LSP is _ever_ going to be able to replicate the VB behavior: the documentSymbols protocol is fundamentally designed around navigation, not around generation (which the VB NavBar service is designed around). I could certainly back off my attempt to make the C# navbar service consume this, though IMO this new service should still live in the Features layer (just feels like the proper layering, as LSP should be about providing an interface to IDE features, not implementing them itself).

> @github: Last time I looked at this, the navigation bar item service is functioning in dual roles:
> 
>     1. Fetching items.
> 
>     2. Implementing the logic for navigating to those items.
> 
> 
> The former doesn't really have any logical dependency to the editor code; the later _does_ due to some tracking spans that it uses. Splitting it into two services where the former lives in Features and the latter lives in EditorFeatures I think should be doable.

In VB, these 2 steps are fundamentally conflated, because it needs to determine a bunch of information about generating items when fetching them."
"I am not aware of a reason for the register logic equivalent not to have the same issue. It is possible we have an bug there, the code sequence I described above, while legal, is incredibly rare, implying sparse test coverage at best."
"Which callsite to `Get_CORINFO_SIG_INFO` looks problematic? Worst case we need a JitInterface change. Or we could also smuggle the ""disable marshalling"" bit in `sig->flags` if getting the scope is hard."
"I've analyzed top (~12) PerfScore regressions in the benchmarks on Win-x64, nothing stands out as ""definitely bad"", the common theme is that we save an additional register (in prolog/epilog) to enregister a CSE. There was one case where a CSE was ""live across a call"", thus spilled/reloaded for no benefit, but we didn't quite know that in the heuristic because the call in question was a write barrier.

I then took a more cursory look at the tests collection on x86, and found a few quite questionable cases ([the worst](https://www.diffchecker.com/94bh94mG)), though overall the size diff was positive (`-100K`).

Finally, I took a look at Win-x64 tests collection, and found a problem that is similar to the one above: [diff](https://www.diffchecker.com/LkpAkB60), where we save/restore (a lot) for a simple (FP) CSE.

Win-Arm64 collections do not appear to have particularly bad examples."
"```suggestion
        /// <returns>The status of the registered garbage collection notification.</returns>
```"
"Tagging subscribers to 'arch-wasm': @github
See info in area-owners.md if you want to be subscribed.
<details>
<summary>Issue Details</summary>
<hr />

This PR introduces a system for defining custom JSâž”Managed and Managedâž”JS marshaling primitives so that any* user-defined type can cross between environments seamlessly. The ideal end goal is to use this new system for marshaling types like ```Uri```, ```DateTime``` and ```Task``` so that they can be linked out if they are not used.

First, some sample code:
```csharp
        public struct CustomDate {
            public DateTime Date;

            private static string JSToManaged_PreFilter () => ""value.toISOString()"";
            private static string ManagedToJS_PostFilter () => ""new Date(value)"";

            private static CustomDate JSToManaged (string s) {
                return new CustomDate { 
                    Date = DateTime.Parse(s).ToUniversalTime()
                };
            }

            private static string ManagedToJS (ref CustomDate cd) {
                return cd.Date.ToString(""o"");
            }
        }
```

General overview:
* The author of the type exposes static methods with specific names and signatures on the type so that the wasm bindings layer can find them.
  * TODO: We need some straightforward way to ensure that the linker doesn't remove these methods from wasm builds unless the type itself is not referenced
* The required methods are a JSâž”Managed mapping and a Managedâž”JS mapping. The types used by these methods have to (at present) be basic types that can cross through the bindings layer without assistance, i.e. double or string. In the future I hope to expand on this to allow marshaling spans or arrays.
* In addition to the direct JSâ¬ŒManaged mapping, you can optionally define additional methods that return JS expression filters. Those expression filters will be evaluated on the JS side of the boundary to process the value - for example since neither JS ```Date``` or C# ```DateTime``` can cross the bindings boundary directly, you can marshal it as a string and then use a filter to map it to/from the JS ```Date``` type.
  * TODO: It would be ideal to expose these as ```const string``` instead of function getters, but right now the only straightforward way to get at managed values is through function calls.
* Alongside all this we add a new ```'a'``` type specifier for method signatures (the strings you pass to ```call_static_method```, ```bind_static_method``` etc) that basically means 'figure it out'. When you use this specifier, the bindings layer will examine the target method and identify the best fit for that parameter. This is meant for the purpose of passing user-defined types so you don't need to think about whether they're classes or structs, but it also can be useful in other cases.
  * The downside is that any signature using this type specifier ends up being method-specific, so you will end up with method-specific generated code instead of the existing generated code that is shared by all methods with a given signature. There's room for some improvement here.

More detailed notes:
* Before this PR we had zero support for passing structs across the JSâ¬ŒManaged boundary in any circumstances (other than DateTime).
* The JSâž”Managed conversion has to box the resulting value (if it's a struct), which introduces some unpleasant overhead. This ends up being less inefficient than you'd think (because currently we have to box *all* values being passed to managed methods from JS - not really sure why) but it's still an opportunity for performance improvement between eliminating some copies and ensuring the GC isn't involved.
* The introduction of support for marshaling custom classes introduces a bit of overhead for classes without custom marshaling implementations, because of the additional check added to the existing Managedâž”JS flow. However, the new check is extremely cheap and the existing flow was very slow ðŸ˜Š 
* There are corner cases where this new system will not run (throwing a runtime exception) or will otherwise fail (for example if the signature of your methods is weird you can get garbage values). I think many of these can be tightened up if we write more test cases and add more error handling and checks to match.
* Many scenarios might call for an automated way to pass JSON blobs or raw byte data across the boundary. You can at least use filters (i.e. ```JSON.parse(value)```) to simplify the former, but the latter really demands integrated support for passing ```Span```s around and I have no idea how we'd do this in the existing setup.
* We could consider having some sort of default fallback marshaling implementation based on ```BinaryFormatter``` or something like that, but it seems out of scope. This system is at least relatively easy to extend to do that as a fallback.

Also in this PR:
* Additional test coverage for some of the existing code, like being able to pass JS ```Date```s. I think that was actually broken, but Blazor works so whatever...
<table>
  <tr>
    <th align=""left"">Author:</th>
    <td>kg</td>
  </tr>
  <tr>
    <th align=""left"">Assignees:</th>
    <td>-</td>
  </tr>
  <tr>
    <th align=""left"">Labels:</th>
    <td>

`* NO MERGE *`, `arch-wasm`

</td>
  </tr>
  <tr>
    <th align=""left"">Milestone:</th>
    <td>-</td>
  </tr>
</table>
</details>"
"> > Worst case I'll need to remove the DIM and manually declare the implementation for all 20 types.
> 
> I've done this with [baf69de](https://github.com/dotnet/runtime/pull/69756/commits/baf69de8576f8528125e6ddee518d3dd310e9e9b). You'll want to remember to revert this if trying to repro the failures above

#69783 will also fix this from the NativeAOT side."
"@github sorry for the big pause, was busy with some other urgent stuff - will now work to merge this ASAP.

> That's a lot of code :) I skimmed through it and left a few very minor nits, but my assumption is that almost all of the code came over with little to no semantic or meaningful changes, yes? I agree with one of the comments you left about not doing work to optimize.

Yes, that's true - I've done my best to change absolutely nothing and simply convert C++ to C# here, only doing meaningful changes where absolutely necessary (in rare cases). I generally doubt this code is used in a perf-sensitive way which would justify the risk of optimizing/refactoring, at least not at this point...

> I also see you have tests; do they provide enough coverage?

Not enough... I've done various manual testing against SQL Server to confirm that things work, but automated testing support is lacking. Automated testing is difficult since it involves coordinating between multiple processes, crashing them at specific points, and for some (extreme0 edge cases, even doing stuff with the Windows MSDTC service. I'll be spending some time in the next few days to try to improve coverage at least to some extent - at the worst case maybe I'll continue working on test coverage after merging for rc1...

How does that sound?

@github @github can you take a look at https://github.com/dotnet/runtime/pull/72051#issuecomment-1195587152 and confirm whether you really want the serialization support removed?"
"Max message size is the worst case scenario, we can do way better than that, limiting the amount of rendered components to those generated as part of an HTTP response by the app. Which will be a way lower number, more predictable and app dependent, so the app decides on a per request basis how many components are acceptable to render. with MAX message size being the only limitation in the worst case scenario."
"Unless someone objects, I'm going to assume the worst case here would be silently compiling against 5.0.x (newer) versions of these packages when servicing. Will file an issue about testing that `%(RTMVersion)` metadata is respected. That test will likely be a bit painful since we won't be building our targeting pack when servicing and checks will require looking at all references in our implementation assemblies for the relevant assemblies then checking their `[AssemblyInformationalVersion()]` attributes against the expected package version."
I seriously do not know why these keep changing.
"At this point we are O(n^2) in every case, not just the worst case. "
"I think it was a pretty common issue in tests which were hijacking threads to do garbage collection.

I opened #42955 to track fixing this."
"Wow that's incredibly thorough. We should have this saved somewhere (niksa.md maybe) as a reference that `til::point`&`size` should always be by value and `rect` always by ref.

Thanks!"
"I noticed on the Microsoft App Store you all mentioned that this is an open source project and encourage community participation. I'm not sure if this is wasting your time posting comments like this here but I extremely appreciate you all trying to fix the Solarized Light and Dark themes (even if some of you might not particularly like the themes at all). I have a really bad medical condition that induces severe migraines when I look at something with too much contrast. Using Solarized, along with other visual tools, has been one of the only themes to seriously help me over the last 7 years that I have been using it. It might sound counterintuitive but I feel that for many a theme that makes things stand out less is kind of a blessing. I feared you all were going to remove Solarized from the default themes and try to make the users just figure out a custom Solarized theme, but if it is not trivial for you all to get working, I can't imagine it being easy for a common user to resolve. I read up through other pull requests regarding this issue and can definitely tell it has been frustrating to handle and I just wanted to express gratitude for the work you are all putting forward to resolve this."
"Worst case we can make the fwlink forward to Bing. But I'd much rather we write-up a full writeup, and post that somewhere. I'm more than happy to do the writing.
"
"the array for ther ImmutableArray is allocated every time no matter what.  The pooled builder approach means that hte builder itself isn't garbage.

There's also the builder's internal array which can be pooled.  But that isn't an issue here since we know the exact lengths, and thus can use MoveToImmutable.

So really, it's just about saving the allocation for the builder obj. :)"
"seriously ? :open_mouth: okay, will do.
"
"If you want an API added to netstandard2.1 you should request it on dotnet/standard, e.g. https://github.com/dotnet/standard/issues/1144

(as far as I know. Worst case they can only say no ðŸ˜‹)"
"> Yes, that's an interesting candidate.

Let's do a separate PR. Worst case, it fails in the next PR and we revert/fix parts of this."
"> My gut is saying no because the parser would parse as a parenthesized expression

In general, we would not. The typeinferrer is all about best-effort trying to figure out waht's going on.  It's rare that we do something particular bad in error cases (at worst, we just infer object or some other reasonable type).  Instead, we hear about broken cases we could do better in, and we put the tweak to this system to do that."
"I would prefer to use try_as but add a check before I insert to the hashset. In this way, if the object turn out to be null, then at worst we just can not focus on it. "
"I needed the length; I am a cargo cult programmer so I used `ImmutableArray` because we use it everywhere else instead of lists.

More seriously, is there a perf preference between list and regular array? My intuition here is to use `.ToArray()` as we only need the length and to iterate over it."
"Reading https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/implementing-dispose#the-dispose-method, it indicates to me that the recommended pattern is to include it in the base class that exposes `Dispose()`.

> to indicate that the finalizer, if one is present, doesn't have to run.

and 

> Therefore, the call to the SuppressFinalize method prevents the garbage collector from running the finalizer. If the type has no finalizer, the call to GC.SuppressFinalize has no effect.

And later, when discussing how to [implement a derived class](https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/implementing-dispose#implement-the-dispose-pattern-for-a-derived-class), it makes no mention of calling `GC.SuppressFinalize`. It would be odd to tell derived classes that they would need to look at the base class to see if it calls `GC.SuppressFinalize`, and then call it yourself, if the base doesn't."
"Yes, it should be more correct and cleaner in code also.
Then, is it ok to produce garbage on every overtype case? Or, it should have some memory space for it, 50-100 bytes, for example, with ability to increase temporarily. One per resource? How to detect if it expired?"
"It's declared but not initialized, so with `{}` it will get initialized to default value rather than holding a garbage value. "
"Seriously?  Fixed.
"
"The BCL seriously still has no identity function?
"
"Thanks, @github.  A 30% regression, even worst-case, is a big pill to swallow.

I suggest you remove the change from this PR.  I'm completely on board with the goal of getting rid of that delegate-based implementation, but we need to first drive down the overheads of the other code paths.  I'd added the delegate-based path in .NET 5 because there was so much overhead associated with getting from the public API into the meat of the matching, and the delegate approach enabled bypassing most of that by keeping things in a tight loop around what's now Scan.  If we could drive down the worst-case regression from 30% to, say, 10%, I think it'd be worth making the switch and getting rid of the duplication/complication."
"If we use ```navigator.hardwareConcurrency``` we need to make sure we have fallbacks if it ends up being a garbage number, since it seems an obvious target for anti-fingerprinting measures (i.e. making it a lie)"
"Current changes vs master baseline (5 iterations of perf test):

```
Project                         Baseline (ms)     Current (ms)      Delta (%) Best (ms) Worst (ms)
-------                         -------------     ------------      --------- --------- ----------
Compiler - tsc (ParseTime)      0.73 (Â± 46.94 %)  0.60 (Â± 2.76 %)   -17.71 %       0.59       0.62
Compiler - tsc (BindTime)       0.24 (Â± 4.36 %)   0.24 (Â± 2.88 %)   -0.84 %        0.23       0.24
Compiler - tsc (CheckTime)      1.11 (Â± 2.55 %)   1.07 (Â± 1.32 %)   -3.42 %        1.06       1.09
Compiler - tsc (EmitTime)       2.02 (Â± 12.41 %)  1.92 (Â± 7.18 %)   -4.94 %        1.73       1.99
Compiler - tsc (TotalTime)      4.11 (Â± 13.65 %)  3.83 (Â± 3.09 %)   -6.62 %        3.67        3.9
Monaco - tsc (ParseTime)        2.47 (Â± 70.11 %)  1.70 (Â± 1.11 %)   -31.45 %       1.68       1.72
Monaco - tsc (BindTime)         0.70 (Â± 2.67 %)   0.66 (Â± 1.69 %)   -6.82 %        0.65       0.67
Monaco - tsc (CheckTime)        3.35 (Â± 12.81 %)  3.22 (Â± 11.00 %)  -3.88 %        2.99       3.57
Monaco - tsc (EmitTime)         6.61 (Â± 22.60 %)  6.30 (Â± 15.68 %)  -4.69 %        5.55       7.22
Monaco - tsc (TotalTime)        13.14 (Â± 18.82 %) 11.87 (Â± 11.16 %) -9.62 %       10.93      13.14
TFS - tsc (ParseTime)           1.46 (Â± 70.93 %)  1.08 (Â± 1.41 %)   -26.23 %       1.06       1.09
TFS - tsc (BindTime)            0.49 (Â± 14.66 %)  0.46 (Â± 1.47 %)   -4.92 %        0.46       0.47
TFS - tsc (CheckTime)           2.28 (Â± 1.83 %)   2.45 (Â± 6.33 %)   7.73 %         2.29       2.57
TFS - tsc (EmitTime)            3.65 (Â± 5.27 %)   3.85 (Â± 22.65 %)  5.59 %         3.49       5.11
TFS - tsc (TotalTime)           7.88 (Â± 16.79 %)  7.85 (Â± 12.08 %)  -0.41 %        7.36        9.2
Encyclopedia - tsc (ParseTime)  0.42 (Â± 22.49 %)  0.38 (Â± 1.81 %)   -9.62 %        0.37       0.38
Encyclopedia - tsc (BindTime)   0.14 (Â± 9.58 %)   0.14 (Â± 7.71 %)   1.41 %         0.13       0.15
Encyclopedia - tsc (CheckTime)  0.50 (Â± 1.11 %)   0.51 (Â± 4.71 %)   3.21 %          0.5       0.54
Encyclopedia - tsc (EmitTime)   0.08 (Â± 28.47 %)  0.08 (Â± 28.47 %)  0.00 %         0.07       0.11
Encyclopedia - tsc (TotalTime)  1.13 (Â± 10.39 %)  1.11 (Â± 3.40 %)   -2.12 %        1.08       1.15
Compiler - node (ParseTime)     0.78 (Â± 8.57 %)   0.75 (Â± 1.81 %)   -4.08 %        0.74       0.76
Compiler - node (BindTime)      0.39 (Â± 10.07 %)  0.36 (Â± 2.90 %)   -8.21 %        0.35       0.37
Compiler - node (CheckTime)     1.70 (Â± 10.04 %)  1.64 (Â± 3.80 %)   -3.30 %        1.57       1.69
Compiler - node (EmitTime)      1.97 (Â± 4.16 %)   1.90 (Â± 11.51 %)  -3.26 %        1.74       2.18
Compiler - node (TotalTime)     4.84 (Â± 6.30 %)   4.66 (Â± 5.44 %)   -3.84 %        4.42       4.98
Monaco - node (ParseTime)       1.92 (Â± 8.11 %)   1.90 (Â± 7.76 %)   -1.14 %        1.78       2.08
Monaco - node (BindTime)        0.99 (Â± 7.98 %)   0.95 (Â± 6.98 %)   -4.23 %        0.89       1.03
Monaco - node (CheckTime)       5.59 (Â± 4.20 %)   5.60 (Â± 4.81 %)   0.07 %         5.32       5.88
Monaco - node (EmitTime)        5.96 (Â± 1.06 %)   5.87 (Â± 5.12 %)   -1.48 %        5.62       6.21
Monaco - node (TotalTime)       14.47 (Â± 2.89 %)  14.32 (Â± 4.56 %)  -1.05 %        13.6      14.81
TFS - node (ParseTime)          1.33 (Â± 4.01 %)   1.32 (Â± 3.88 %)   -0.45 %        1.28       1.39
TFS - node (BindTime)           0.83 (Â± 2.36 %)   0.84 (Â± 2.25 %)   0.72 %         0.82       0.86
TFS - node (CheckTime)          3.75 (Â± 2.66 %)   3.77 (Â± 2.40 %)   0.53 %         3.65       3.84
TFS - node (EmitTime)           4.13 (Â± 4.28 %)   4.20 (Â± 2.82 %)   1.55 %         4.13       4.36
TFS - node (TotalTime)          10.04 (Â± 2.89 %)  10.13 (Â± 2.27 %)  0.82 %         9.98      10.45
Encyclopedia - node (ParseTime) 0.29 (Â± 1.93 %)   0.30 (Â± 1.86 %)   3.47 %         0.29        0.3
Encyclopedia - node (BindTime)  0.22 (Â± 2.50 %)   0.23 (Â± 4.75 %)   5.41 %         0.22       0.24
Encyclopedia - node (CheckTime) 1.03 (Â± 2.41 %)   1.08 (Â± 3.40 %)   4.66 %         1.03        1.1
Encyclopedia - node (EmitTime)  0.13 (Â± 13.22 %)  0.13 (Â± 12.65 %)  1.59 %         0.12       0.15
Encyclopedia - node (TotalTime) 1.67 (Â± 1.93 %)   1.73 (Â± 2.17 %)   3.84 %         1.68       1.75
```

It causes minimal/no changes to overall perf in real-world scenarios. (Though I imagine this is the kind of change were you'd only see perf changes in a micro-benchmark anyway)
"
"I find it rather curious how the following incredibly simple reproduction detects this issue while the `Xunit.Assert.Equal` method did not.
```CS
using System;
using System.Numerics;

Console.WriteLine(64 == BitOperations.PopCount(18446744073709551615UL));
```
```
dotnet run -r win-x86 -c Release
False
dotnet run -r win-x64 -c Release
True
```
I will need some more time to investigate this.

Edit: the reproduction above does not actually work, ignore it."
"Ultra-nit: the most common case will be `receiver == action.Target`, so we could swap the order of these conditionals for an incredibly minor perf gain."
"Can we simplify this to just always call GC.SuppressFinalize(this)?  Worst case is we call it multiple times if Dispose is called multiple times, right?

We also shouldn't need to null out _handler; worst case is we try to remove it multiple times, with subsequent times being a nop?

e.g. seems like it could be simplified to:
```C#
private sealed class NetworkChangeCleanup : IDisposable
{
    private readonly NetworkAddressChangedEventHandler _handler;

    public NetworkChangeCleanup(NetworkAddressChangedEventHandler handler) => _handler = handler;

    ~NetworkChangeCleanup() => NetworkChange.NetworkAddressChanged -= _handler;

    public void Dispose()
    {
        NetworkChange.NetworkAddressChanged -= _handler;
        GC.SuppressFinalize(this);
    }
}
```"
"The PR won't get garbage collected, it can remain closed until we decide to go along with it."
"I think there's a potential ordering issue here.  (In part because `Process` is like THE WORST API in .NET...)

I believe that setting `p.OutputDataReceived` and `p.ErrorDataReceived` needs to be done *before* calling `p.Start()`, to ensure that you don't miss any output."
"> Is there an upper limit in the worse case? 

I can add one, it's related to the comment here https://github.com/dotnet/runtime/blob/7fb611ec5aa9ef9c7080b20a3a386481f873bff6/src/libraries/Microsoft.Extensions.DependencyInjection/src/ScopePool.cs#L65. I'm not sure it matters though because in the most common cases, you end up constantly (per request) allocating a dictionary with that many entries anyways. It'll never get bigger than what your code was trying to use. I fear if we throw away entries that are too big then they'll be repeatedly allocated and thrown out. I could add a worst cast just in case (some extremely high number just to avoid pooling it at all).

> The TLS caches like this typically have to have upper bound to prevent unbounded memory leak. 

Aren't you intrinsically throttled by the number of threads? In practice, I don't see how this could lead to an unbounded memory leak. "
"PAL has to be already fully initialized when coreclr runtime starts. The flag is always passed when PAL in coreclr is being initialized, the cases when it is not passed are when PAL is used by other components. But you a have a good point that from that it would be still better to not to let the new PAL function you have added return a garbage even in that case.
I have looked at the C++ statics initialization rules and it actually seems I was mistaken and that when the members are set to constants, the compiler should not invoke a constructor. So the change would be ok then."
"**This is a warning**.

We have recently observed the presumably well-intentioned act of the author of this PR (@inclusive-coding-bot), who has been spamming pull requests to many (~50) github repos, claiming to ``switch to gender neutral pronouns''. In reality, the bot performs a dictionary replace of gendered nouns and pronouns, and the outcome is questionable at best, and literally harmful at worst. See:
- The bot did a terrible job at replacing these words - [EbookFoundation/free-programming-books#6801](https://github.com/EbookFoundation/free-programming-books/pull/6801/commits/5257301642c173e2dc4f034f8c0460ce7ea99de6)
- The code fails to compile at [rust-lang/rust#95508](https://github.com/rust-lang/rust/pull/95508)
- The bot ignores all context whatsoever at [moby/moby#43441](https://github.com/moby/moby/pull/43441)
- After initial PR being closed, the bot keeps sending PRs, effectly spamming the repos. For example, these 5 PRs were sent to the same repo within 2 days [#1](https://github.com/EbookFoundation/free-programming-books/pull/6801) [#2](https://github.com/EbookFoundation/free-programming-books/pull/6803) [#3](https://github.com/EbookFoundation/free-programming-books/pull/6804) [#4](https://github.com/EbookFoundation/free-programming-books/pull/6805) [#5](https://github.com/EbookFoundation/free-programming-books/pull/6806)

We have reviewed the changes in this PR. Most of the changed words are ""accidental gender pronouns"" - `he` as a prefix of `hello`, or `his` as a suffix of `this`, and both examples come from test cases where inline comments are used to split words (like `t/**/his`), and the PR wants to change them into `t/**/theirs` etc.. These changes are obviously not helpful at all.

To save more time for the open-source community, we recommend the maintainers of this repo to close this PR and ban this bot from further spamming."
"> > Bring in some more QCall infrastructure that matches the CoreCLR
> 
> In this specific case, it would be even better to move the unmanaged part to src/libraries/Native (it is done that way in NativeAOT already) so the whole thing can be shared. I understand that it would be more work.

I didn't want to do that initially since Mono already has some special handling for `__Internal` that I wanted to piggy-back on and it felt weird to have most of `NativeLibrary` be implemented in the runtimes but have one method implemented in `src/libraries/Native`, but I can move it there if we want.

If I do move it, I'd like to leave the mono QCall infra in this PR (or worst case move it into another PR) to make it easier to add QCalls to Mono in the future."
"ArrayBuilder+ToImmutableAndFree?  Otherwise, this will generate garbage.   #Resolved"
"I now compute minimal edits only at the end.
I also added a quick exit to the `hasIntersectingEdit` function.

> I think it should be fine if you just test if any two ranges touch the same line. Then you could use a set of touched line numbers and get an algorithm than runs in O(line numbers).

It's not that easy, not only do I need to know that 2 edits intersect, but I also need to know which 2 ranges are at the original of these, and the list of all edits produced by these 2 ranges so I can merged them. 
So I need to iterate all combination of ranges in any case. That's already O(n^2). Then I need to know if the edits intersect. That's O(m^2). I guess the overall complexity of O(n^2*m^2) is what you refer to as O(n^4).
I don't think I can do much better while retaining correctness and completeness, at least not at the cost of an overly complicated algorithm (usefulness vs maintainability). 

Now that minimal edits are only computed at the end, I expect the problem size to stay rather small, which means complexity should not be an issue that much.
If you're worried I still have a few tricks:
1. Quick exit on `hasIntersectingEdits`, brings down O(m^2) down to O(m) in most cases (not really O then, agreed), worst case is not affected, but we improve most use cases while retaining correctness. (already implemented)
2. Assume only consecutive ranges can produce intersecting edits: although not guaranteed, I don't see any case where a sane formatter would not respect this. That would bring O(n^2) down to O(n) at the cost of potentially not merging 2 ranges...which would be just as good as what we're doing before this PR
3. Skip this algorithm if the problem size is too big. Format range merging would still work as long as there are not too many edits. Again, strictly not worse than the current state of things. The problem with this is finding the correct threshold.

Let me know what you think, or if you have any other ideas"
"I don't think we can use `wil::unique_cotaskmem_string` since that deallocates in its destructor. This function is designed to hand ownership of the string over to the caller, in this case that is .NET which will use `CoTaskMemFree` when the garbage collector decides that the string can be freed."
"This is probably the only one that stands out to me as needing to be spellchecked. Though, that file does look like it's full of a bunch of garbage strings for testing, so it's probably fine for now "
"25%-seriously: if someone's barbaric enough to store their `commandline` separated by tabs instead of spaces, then they deserve to get pinged with this waring dialog ðŸ˜ 
"
"ðŸ“ The link was not rendered correctly.

```patch
-[Cleaning Up Unmanaged Resources](/dotnet/standard/garbage-collection/unmanaged)
+<see href=""https://docs.microsoft.com/dotnet/standard/garbage-collection/unmanaged"">Cleaning Up Unmanaged Resources</see>
```"
"> do our consumers (like StringBuilder) need to be defensive against somebody returning a negative charsWritten value?

ValueStringBuilder should be fine, as it uses span functionality that won't end up trusting the tracked count/position; worst case, bad input, bad output, but no corruption.  Same for InterpolatedStringBuilder.  For StringBuilder, AppendSpanFormattable isn't public, but I'll add some validation to it, just in case."
"While I'm fine with this change in general, I'm not fine with considering it a fix for that bug. Yes, it's a worst-case scenario. But given that there were literally no nullable suppressions in syntax in the file I was editing, I don't think that it's OK to not have a fast-path."
"Worst that will happen is the dependency PR needs a bit more work @github ðŸ˜€ "
"To be more pedantic, `unordered_set.contains` has worst case runtime of linear still :)"
"I love it. I absolutely love it. Heaps of screenshots of the combination of this and 6506 inside.

<details>
<summary>Screenshots (good case)</summary>

![image](https://user-images.githubusercontent.com/189190/85917115-6c0b0000-b80c-11ea-9add-101494080e47.png)

![image](https://user-images.githubusercontent.com/189190/85917122-76c59500-b80c-11ea-8ece-f3c664ca6a46.png)

![image](https://user-images.githubusercontent.com/189190/85917152-9e1c6200-b80c-11ea-9247-772b6a305e62.png)

![image](https://user-images.githubusercontent.com/189190/85917166-ba200380-b80c-11ea-8f10-e7d230bfe678.png)

![image](https://user-images.githubusercontent.com/189190/85917186-d623a500-b80c-11ea-80a9-4b659f5983cd.png)

![image](https://user-images.githubusercontent.com/189190/85917190-dde34980-b80c-11ea-9708-643762375f7c.png)

</details>


<details>
<summary>Screenshots (unusual case)</summary>

![image](https://user-images.githubusercontent.com/189190/85917275-88f40300-b80d-11ea-99a7-c69cdc352196.png)


</details>

<details>
<summary>Screenshots (bad case)</summary>

![image](https://user-images.githubusercontent.com/189190/85917164-b2605f00-b80c-11ea-90e0-c0a5aa44fd1d.png)

</details>

There's literally only the one bad thing, it just shows up a lot. The FAR manager thing isn't new (!) and it's definitely acting as intended for this feature.

I'm pushing on this with the PSReadline folks. Absolute worst case scenario, we have to quirk background handling for powershell.exe and pwsh.exe. Given that inbox Windows PowerShell will never take on a new version of PSReadline, that might be our best worst bet."
"Wow this is an incredibly creative way of tinkering with the tables inside user32. Props to the guy who discovered it.

I did go read behind `SetKeyboardState` and it is definitely inadequate for the dead key scenario relative to what happens through `ToUnicodeEx`. The former only updates some bit flags related to things being up/down. The latter does that AND goes through the whole dance of manipulating Alt+Numpad entry state, dead key cached characters, and more. 

To be clear, both of these functions are updating a thread specific table about keyboard state. It's not strictly global. It's just that `SetKeyboardState` looks at and updates just one specific member of the state table while `ToUnicodeEx` roots around in pretty much every bit of it.

Why there isn't a more adequate way of doing this is likely lost to the sands of time. If this does the job, I'm perfectly OK with it."
"The attribute annotations are ""the worst case"" - linker is free to not warn if it knows better (same for analyzers and so on). In this case the annotation is there for cases where we don't have intrinsic handling. Some of these are in the linker, but any other tool might be in the same position. We have to basically ""play it safe""."
"1. Collapsed regions work fine.  They will just mean we have a larger 'visible span'.  But in the worst case, we end up just having to reclassify the whole file (which is no worse than where we are today).
2. If you have the `scroll map` feature turned on, then it will _not_ have semantic classifications in it for the region outside what is in view.  For TS we decided that was absolutely fine with us.  `scroll map` is an area that can have a slightly degraded experience when we're providing such a benefit to the mainline case of just editing.

Note that syntactic/lexical classification will still continue running inside `scroll map`.  So most things will look good (and things like class declarations will still be teal).  

You'll have to decide if you're ok with this behavior.  IMO scroll map is nice, but it doesn't warrant all the extra CPU and memory necessary to do a full file semantic classification.

## 

As an alternative.  We could consider only enabling this specialized behavior for very large files.  i.e. once your file gets above 100k, then we cap things like this.  Then on smaller files, the entire experience is good (albeit with some extra memory cost).  But on large files we start introducing throttles to keep memory/cpu usage low, but in ways that may be noticeable if you use scroll map.
"
"Note that the logic for ""partial-vs-full"" must consider what struct the call originally returned, and the kind of local address we're storing to.

I. e. for this method it would ideally look like this:
```c
if (OperIs(GT_CALL))
{
    GenTree* addr = call->GetLclRetBufArgNode(); // Note it must return the whole ""ADDR(LCL)"" tree.
    if (addr == nullptr)
    {
        return false;
    }

    unsigned size = typGetObjLayout(call->gtRetClsHnd);
    return addr->DefinesLocalAddr(comp, size, pLclVarTree, pIsEntire);
}
```
Since this `GetLclRetBufArgNode` method is used above, I'll also outline what I came to with https://github.com/dotnet/runtime/pull/64130#discussion_r814962297, after considering a few options (I am not incredibly happy with the end result, but the other options were worse in one way or another):
1) In local address visitor, save the `Use*` for the return buffer argument. The presence of this `Use*` now marks the call as potentially defining a tracked local (this is a throughout optimization to avoid looking at return buffer args that we know are not local).
2) In morph, update the `Use*` if the return buffer is moved to the late arg list with the late use in case it was moved ""fully"" (i. e. replaced by an `ARGPLACE`), leave as-is if it was moved via a setup arg.
3) After morph, use `GetLclRetBufArgNode`, that will roughly look like this:
```c
// I really don't like how we create a dependency on the setup args here, but hope to
// eliminate it in the future by using local address nodes instead of ADDRs here, always.
assert(use != nullptr);
return use->GetNode->OperIs(GT_ASG) ? use->GetNode()->gtGetOp2() : use->GetNode();
```
To get the address node and use it in `DefinesLocal`.

As I mentioned, this is not ""the best it could be"" (the best would be to represent these cases with an actual `ASG`), but ""better"" in that it aligns as much as reasonable with how we treat ordinary stores.

It would also be cleaner to save the return buffer arg `Use*` always, not just when it's local. Not sure how many of return buffers end up storing to a local, there may be a large enough percentage of them that we'd get away with it TP-wise."
"Like @github already said, changing both sides of the fixed/rep-stos paths makes it hard to compare perf.
Also now that the rep-stos path has 14 instructions (was 4) the code size tradeoff is even more unclear.
Is the 32-byte alignment really worth it? The worst case perf for rep-stos is probably for 1-byte aligned cases, not for 16-byte aligned (or potentially 8 in leafs)? This really needs some measuring instead of just dry asm comparing.

The code size wins come from some register use improvements?

For a meaningful and fair comparison, it might be best to improve the fixed (xmm) path (e.g., by zero-initing only the parts that actually contains GC refs) and work on the rep-stos side in a separate PR later?"
"This change has made upgrading to TS4.0+ incredibly difficult for the [vscode-docker](https://github.com/microsoft/vscode-docker) extension. We share a UI framework with quite a number of other Azure-related VSCode extensions; this framework heavily (ab)uses the ability to override properties with accessors. Never had any problems. _Hundreds_ of files across dozens of repos would need to be fixed to be able to upgrade.

It would be very nice to have either the ability to suppress this specific error only in source (i.e. #19139) or globally.

Aside from it being weird, I'm not sure what would be wrong with overriding a `readonly` property with only a `get` accessor? Or vice versa?"
"I ran an experiment a while back that involved touching a few more files, since I was trying to get array and span and string indexers all in one shot. https://github.com/GrabYourPitchforks/coreclr/commit/d14b9773a6d2ae96c3255ecddd19cbfae549c972

Though TBH I didn't have a clue what I was doing and it's entirely possible that prototype code is hot garbage. :)"
"> Question: RuntimeHelper.Equals compares value types for value and padding

the problem is not only in paddings (which might contain garbage values) but also with e.g. floats - `RuntimeHelpers.Equals(float.NaN, float.NaN)` currently returns `true` which is not correct. It also, I guess, compares gc refs by pointers. Not sure anyone actually uses this method and it probably can be just deprecated (the docs don't mention the pit falls)

"
I added a commit to do something minimal that I think solves the problem in the worst case. Please review and let me know if I missed something about the existing code.
"this needs documentation, or a better name.  It reads really weird to have a ResettableDelay.CompletedDelay.  Specifically, you are exposing a ResettableDelay that can't actually be reset and which will not ever call a timer.  So it's both not something to reset, and it's not a delay.  So the name itself is incredibly strange and indicates something wonky."
"don't allocate or add to this list unless ArrangeNativeChildren == true, otherwise we generate unneeded garbage
"
"If the concern is GC use and reducing the garbage created, wouldn't we want to *avoid* the `DirectoryInfo` creation and use of `root.EnumerateFileSystemInfos()` here?"
"(Maybe it's not needed, but it's sort of a ""there's one test for garbage data, do we need to care if this edge case is garbage on all platforms?"")"
"Squash-and-merge Commit Message Summary:

```
[Xamarin.Android.Tools.BootstrapTasks] api-compat & netcoreapp3.1 (#4497)
```

Body:

```
Context: bcb89eaa599bc14ecfada1130b87a607f08d0486
Context: https://github.com/xamarin/xamarin-android/pull/4485

Bumps to Microsoft.Dotnet.ApiCompat 5.0.0-beta.20181.7

Changes: https://github.com/dotnet/arcade/compare/a5a6a988e58bd0c73b39ca1f5993810c5226d1b8...6ee69696ea00da39f35066c22c21a00ceab93d00

The new `Microsoft.DotNet.ApiCompat` package allows us to:

  * Provide a baseline file with validations to make sure baseline
    content has no unused/garbage on it.  This allows us to do a major
    cleanup on the `<CheckApiCompatibility/>` task.

  * Provide assemblies as command options instead of folders, so now we
    don't need to move files around anymore to run it.

We've also cleaned up the `<CheckApiCompatibility/>` task so that it
produces ""API diff"" content internally, instead of using `git diff`
when an API breakage is detected:

 *** CodeGen missing items***
  
 namespace Android.Telecom
  {
    public abstract partial class Connection : Android.Telecom.Conferenceable
    {
      -public void SetVideoState([Android.Runtime.GeneratedEnumAttribute] Android.Telecom.VideoProfileState videoState) { }
    }
  }

Commit 73be59cf had an unanticipated side effect: it caused *errors*
from the `<CheckApiCompatibility/>` task to be *ignored*, e.g. from
[the master build for 183d1cf2][0]:

	_CheckApiCompatibility:
	  CheckApiCompatibility for ApiLevel: v10.0.99
	  CompatApi command: â€¦/xamarin-android/packages/microsoft.dotnet.apicompat/5.0.0-beta.20162.4/tools/net472/Microsoft.DotNet.ApiCompat.exe ""â€¦/xamarin-android/bin/Release/lib/xamarin.android/xbuild-frameworks/MonoAndroid/v10.0/ApiCompatTemp"" -i ""â€¦/xamarin-android/bin/Release/lib/xamarin.android/xbuild-frameworks/MonoAndroid/v10.0.99/ApiCompatTemp"" --allow-default-interface-methods --exclude-attributes ../../tests/api-compatibility/api-compat-exclude-attributes.txt 
	  CompatApi command: â€¦/xamarin-android/packages/microsoft.dotnet.apicompat/5.0.0-beta.20162.4/tools/net472/Microsoft.DotNet.ApiCompat.exe ""â€¦/xamarin-android/bin/Release/lib/xamarin.android/xbuild-frameworks/MonoAndroid/v10.0/ApiCompatTemp"" -i ""â€¦/xamarin-android/bin/Release/lib/xamarin.android/xbuild-frameworks/MonoAndroid/v10.0.99/ApiCompatTemp"" --allow-default-interface-methods --exclude-attributes ../../tests/api-compatibility/api-compat-exclude-attributes.txt 
	  CheckApiCompatibility found nonacceptable Api breakages for ApiLevel: v10.0.99.
	  Compat issues with assembly Mono.Android:
	  MembersMustExist : Member 'public void Android.Telecom.Connection.SetVideoState(Android.Telecom.VideoProfileState)' does not exist in the implementation but it does exist in the contract.
	  Total Issues: 2
	â€¦/xamarin-android/src/Mono.Android/Mono.Android.targets(193,5): error : CompatApi command: â€¦/xamarin-android/packages/microsoft.dotnet.apicompat/5.0.0-beta.20162.4/tools/net472/Microsoft.DotNet.ApiCompat.exe ""â€¦/xamarin-android/bin/Release/lib/xamarin.android/xbuild-frameworks/MonoAndroid/v10.0/ApiCompatTemp"" -i ""â€¦/xamarin-android/bin/Release/lib/xamarin.android/xbuild-frameworks/MonoAndroid/v10.0.99/ApiCompatTemp"" --allow-default-interface-methods --exclude-attributes ../../tests/api-compatibility/api-compat-exclude-attributes.txt  [â€¦/xamarin-android/src/Mono.Android/Mono.Android.csproj]
	â€¦/xamarin-android/src/Mono.Android/Mono.Android.targets(193,5): error : CheckApiCompatibility found nonacceptable Api breakages for ApiLevel: v10.0.99. [â€¦/xamarin-android/src/Mono.Android/Mono.Android.csproj]
	â€¦/xamarin-android/src/Mono.Android/Mono.Android.targets(193,5): error : Compat issues with assembly Mono.Android: [â€¦/xamarin-android/src/Mono.Android/Mono.Android.csproj]
	â€¦/xamarin-android/src/Mono.Android/Mono.Android.targets(193,5): error : MembersMustExist : Member 'public void Android.Telecom.Connection.SetVideoState(Android.Telecom.VideoProfileState)' does not exist in the implementation but it does exist in the contract. [â€¦/xamarin-android/src/Mono.Android/Mono.Android.csproj]
	â€¦/xamarin-android/src/Mono.Android/Mono.Android.targets(193,5): error : Total Issues: 2 [â€¦/xamarin-android/src/Mono.Android/Mono.Android.csproj]
	  Build continuing because ""ContinueOnError"" on the task ""CheckApiCompatibility"" is set to ""ErrorAndContinue"".

We *do* want to continue on error, so that we collect as many errors
as possible, but we still expected the *overall* build to fail, and
instead the error was swallowed.  The `make jenkins` step is ""green""
(success) when it should be ""red"" (failure).

PR #4485 / commit bcb89eaa (on d16-6) contains the fix for the reported
`Connection.SetVideoState()` API breakage, and is included here.

Fix the ""swallowing of errors"" by enabling `netcoreap3.1` assemblies
to be part of the API Compatibility checks.  The `netcoreapp3.1`-
profile `Mono.Android.dll` (a2687c42) will only be checked against
the ""reference"" `Mono.Android.dll` (2cfce14b).

[0]: https://devdiv.visualstudio.com/DevDiv/_build/results?buildId=3606271&view=logs&j=96fd57f5-f69e-53c7-3d47-f67e6cf9b93e&t=65256bb7-a34c-5353-bc4d-c02ee25dc133
```"
"> Basically, I don't foresee better perf in the typical use case, and I do foresee worse perf in the extreme use case, when compared to a standard StringBuilder.

I would expect VSB to have at least slightly better perf in the typical use case, as it's able to use stack memory instead of paying for a TLS access.

But more importantly, for lines longer than 360 characters (which isn't that unusual in situations where each paragraph is a line), VSB will use the ArrayPool whereas StringBuilderCache will allocate new StringBuilders/underlying char[]s each time.

It's only in the extreme worst case scenario that we could end up creating LOH arrays with VSB where we wouldn't with StringBuilder, but even there, they'll be pooled with VSB whereas they won't be with StringBuilder.

> My experience is that StringBuilder is a bit more forgiving than VSB at this, since if a failure occurs while VSB is on the stack, it negatively impacts the performance of other components which are using the array pool.

I didn't understand this.  If you want to return the poolable object in the case of exception, that needs to be done in a finally for both VSB and StringBuilderCache.  And with VSB, you'll be using a poolable object much less often, since the suggested typical case of 80-100 chars won't involve a pooled object at all."
"I think it would be better to walk up parentheses at the beginning of the function, then run *both* of the existing checks.
At worst it's written the way the rest of the compiler is, at best it allows parentheses inside assignment pattern + future things that might be added here."
"I have a clue now. The `address` is passed in as `string`, so the runtime needs to marshal it, and the marshalled string will only last until `SystemNative_GetHostEntryForNameAsync` returns. Then at the callback this reference has gone and it crashes.
This explains why the native tests pass, and the managed ones fail.

This can be seen in the following trace:
```
MANAGED: start with hostName: microsoft.com
MANAGED: addr context: 0x7f8dc8003560
MANAGED: addr state: 0x7f8e8f731220
NATIVE: arg address: microsoft.com
NATIVE: entry: 0x7f8dc8003560
NATIVE: callback: 0x7f8e13a93544
MANAGED: result from os-call: errorCode: 0
MANAGED: returning task
NATIVE (callback): entry: 0x7f8dc8003560
NATIVE (callback): callback: 0x7f8e13a93544
NATIVE (callback): address: Â BÃŽwÅ½
NATIVE (callback): gai_error: -2
NATIVE (callback): pal_error: 5
MANAGED: entering callback with error: 5
MANAGED: entering ProcessResult with errorCode: HostNotFound
MANAGED: addr context: 0x7f8dc8003560
MANAGED: addr state: 0x7f8e8f731220
```
In the callback the address is garbage.

A trivial workaround / fix would be duplicating the string in the native side (`strdup`).
Is there any .NET-way to tell P/Invoke how long the marshalled memory (string) should last?"
"```asm
; Assembly listing for method System.Runtime.Intrinsics.Vector128:CreateScalar(ubyte):System.Runtime.Intrinsics.Vector128`1[Byte]
; Emitting BLENDED_CODE for generic ARM64 CPU - Windows
; optimized code
; fp based frame
; partially interruptible
; Final local variable assignments
;
;  V00 arg0         [V00,T00] (  3,  3   )   ubyte  ->   x0        
;# V01 OutArgs      [V01    ] (  1,  1   )  lclBlk ( 0) [sp+0x00]   ""OutgoingArgSpace""
;* V02 tmp1         [V02    ] (  0,  0   )  simd16  ->  zero-ref    HFA(simd16)  ""struct address for call/obj""
;  V03 tmp2         [V03,T01] (  2,  2   )  simd16  ->  d16         HFA(simd16)  ld-addr-op ""Inline ldloca(s) first use temp""
;
; Lcl frame size = 0

G_M19699_IG01:
        A9BF7BFD          stp     fp, lr, [sp,#-16]!
        910003FD          mov     fp, sp
						;; bbWeight=1    PerfScore 1.50
G_M19699_IG02:
        4E010FF0          dup     v16.16b, wzr
        53001C00          uxtb    w0, w0
        4E011C10          ins     v16.b[0], w0
        4EB01E00          mov     v0.16b, v16.16b
						;; bbWeight=1    PerfScore 4.00
G_M19699_IG03:
        A8C17BFD          ldp     fp, lr, [sp],#16
        D65F03C0          ret     lr
						;; bbWeight=1    PerfScore 2.00

; Total bytes of code 32, prolog size 8, PerfScore 10.70, (MethodHash=a381b30c) for method System.Runtime.Intrinsics.Vector128:CreateScalar(ubyte):System.Runtime.Intrinsics.Vector128`1[Byte]
; ============================================================
```

```asm
; Assembly listing for method System.Runtime.Intrinsics.Vector128:CreateScalar(double):System.Runtime.Intrinsics.Vector128`1[Double]
; Emitting BLENDED_CODE for generic ARM64 CPU - Windows
; optimized code
; fp based frame
; partially interruptible
; Final local variable assignments
;
;  V00 arg0         [V00,T00] (  3,  3   )  double  ->   d0        
;# V01 OutArgs      [V01    ] (  1,  1   )  lclBlk ( 0) [sp+0x00]   ""OutgoingArgSpace""
;* V02 tmp1         [V02    ] (  0,  0   )  simd16  ->  zero-ref    HFA(simd16)  ""struct address for call/obj""
;  V03 tmp2         [V03,T01] (  2,  2   )  simd16  ->  d16         HFA(simd16)  ld-addr-op ""Inline ldloca(s) first use temp""
;
; Lcl frame size = 0

G_M6886_IG01:
        A9BF7BFD          stp     fp, lr, [sp,#-16]!
        910003FD          mov     fp, sp
						;; bbWeight=1    PerfScore 1.50
G_M6886_IG02:
        4E080FF0          dup     v16.2d, xzr
        6E080410          ins     v16.d[0], v0.d[0]
        4EB01E00          mov     v0.16b, v16.16b
						;; bbWeight=1    PerfScore 3.50
G_M6886_IG03:
        A8C17BFD          ldp     fp, lr, [sp],#16
        D65F03C0          ret     lr
						;; bbWeight=1    PerfScore 2.00

; Total bytes of code 28, prolog size 8, PerfScore 9.80, (MethodHash=17a3e519) for method System.Runtime.Intrinsics.Vector128:CreateScalar(double):System.Runtime.Intrinsics.Vector128`1[Double]
; ============================================================
```

```asm
; Assembly listing for method System.Runtime.Intrinsics.Vector128:CreateScalar(short):System.Runtime.Intrinsics.Vector128`1[Int16]
; Emitting BLENDED_CODE for generic ARM64 CPU - Windows
; optimized code
; fp based frame
; partially interruptible
; Final local variable assignments
;
;  V00 arg0         [V00,T00] (  3,  3   )   short  ->   x0        
;# V01 OutArgs      [V01    ] (  1,  1   )  lclBlk ( 0) [sp+0x00]   ""OutgoingArgSpace""
;* V02 tmp1         [V02    ] (  0,  0   )  simd16  ->  zero-ref    HFA(simd16)  ""struct address for call/obj""
;  V03 tmp2         [V03,T01] (  2,  2   )  simd16  ->  d16         HFA(simd16)  ld-addr-op ""Inline ldloca(s) first use temp""
;
; Lcl frame size = 0

G_M37120_IG01:
        A9BF7BFD          stp     fp, lr, [sp,#-16]!
        910003FD          mov     fp, sp
						;; bbWeight=1    PerfScore 1.50
G_M37120_IG02:
        4E020FF0          dup     v16.8h, wzr
        13003C00          sxth    w0, w0
        4E021C10          ins     v16.h[0], w0
        4EB01E00          mov     v0.16b, v16.16b
						;; bbWeight=1    PerfScore 4.00
G_M37120_IG03:
        A8C17BFD          ldp     fp, lr, [sp],#16
        D65F03C0          ret     lr
						;; bbWeight=1    PerfScore 2.00

; Total bytes of code 32, prolog size 8, PerfScore 10.70, (MethodHash=87f66eff) for method System.Runtime.Intrinsics.Vector128:CreateScalar(short):System.Runtime.Intrinsics.Vector128`1[Int16]
; ============================================================
```

```asm
; Assembly listing for method System.Runtime.Intrinsics.Vector128:CreateScalar(int):System.Runtime.Intrinsics.Vector128`1[Int32]
; Emitting BLENDED_CODE for generic ARM64 CPU - Windows
; optimized code
; fp based frame
; partially interruptible
; Final local variable assignments
;
;  V00 arg0         [V00,T00] (  3,  3   )     int  ->   x0        
;# V01 OutArgs      [V01    ] (  1,  1   )  lclBlk ( 0) [sp+0x00]   ""OutgoingArgSpace""
;* V02 tmp1         [V02    ] (  0,  0   )  simd16  ->  zero-ref    HFA(simd16)  ""struct address for call/obj""
;  V03 tmp2         [V03,T01] (  2,  2   )  simd16  ->  d16         HFA(simd16)  ld-addr-op ""Inline ldloca(s) first use temp""
;
; Lcl frame size = 0

G_M42503_IG01:
        A9BF7BFD          stp     fp, lr, [sp,#-16]!
        910003FD          mov     fp, sp
						;; bbWeight=1    PerfScore 1.50
G_M42503_IG02:
        4E040FF0          dup     v16.4s, wzr
        4E041C10          ins     v16.s[0], w0
        4EB01E00          mov     v0.16b, v16.16b
						;; bbWeight=1    PerfScore 3.50
G_M42503_IG03:
        A8C17BFD          ldp     fp, lr, [sp],#16
        D65F03C0          ret     lr
						;; bbWeight=1    PerfScore 2.00

; Total bytes of code 28, prolog size 8, PerfScore 9.80, (MethodHash=56ed59f8) for method System.Runtime.Intrinsics.Vector128:CreateScalar(int):System.Runtime.Intrinsics.Vector128`1[Int32]
; ============================================================
```

```asm
; Assembly listing for method System.Runtime.Intrinsics.Vector128:CreateScalar(long):System.Runtime.Intrinsics.Vector128`1[Int64]
; Emitting BLENDED_CODE for generic ARM64 CPU - Windows
; optimized code
; fp based frame
; partially interruptible
; Final local variable assignments
;
;  V00 arg0         [V00,T00] (  3,  3   )    long  ->   x0        
;# V01 OutArgs      [V01    ] (  1,  1   )  lclBlk ( 0) [sp+0x00]   ""OutgoingArgSpace""
;* V02 tmp1         [V02    ] (  0,  0   )  simd16  ->  zero-ref    HFA(simd16)  ""struct address for call/obj""
;  V03 tmp2         [V03,T01] (  2,  2   )  simd16  ->  d16         HFA(simd16)  ld-addr-op ""Inline ldloca(s) first use temp""
;
; Lcl frame size = 0

G_M9853_IG01:
        A9BF7BFD          stp     fp, lr, [sp,#-16]!
        910003FD          mov     fp, sp
						;; bbWeight=1    PerfScore 1.50
G_M9853_IG02:
        4E080FF0          dup     v16.2d, xzr
        4E081C10          ins     v16.d[0], x0
        4EB01E00          mov     v0.16b, v16.16b
						;; bbWeight=1    PerfScore 3.50
G_M9853_IG03:
        A8C17BFD          ldp     fp, lr, [sp],#16
        D65F03C0          ret     lr
						;; bbWeight=1    PerfScore 2.00

; Total bytes of code 28, prolog size 8, PerfScore 9.80, (MethodHash=99c7d982) for method System.Runtime.Intrinsics.Vector128:CreateScalar(long):System.Runtime.Intrinsics.Vector128`1[Int64]
; ============================================================
```

```asm
; Assembly listing for method System.Runtime.Intrinsics.Vector128:CreateScalar(byte):System.Runtime.Intrinsics.Vector128`1[SByte]
; Emitting BLENDED_CODE for generic ARM64 CPU - Windows
; optimized code
; fp based frame
; partially interruptible
; Final local variable assignments
;
;  V00 arg0         [V00,T00] (  3,  3   )    byte  ->   x0        
;# V01 OutArgs      [V01    ] (  1,  1   )  lclBlk ( 0) [sp+0x00]   ""OutgoingArgSpace""
;* V02 tmp1         [V02    ] (  0,  0   )  simd16  ->  zero-ref    HFA(simd16)  ""struct address for call/obj""
;  V03 tmp2         [V03,T01] (  2,  2   )  simd16  ->  d16         HFA(simd16)  ld-addr-op ""Inline ldloca(s) first use temp""
;
; Lcl frame size = 0

G_M64149_IG01:
        A9BF7BFD          stp     fp, lr, [sp,#-16]!
        910003FD          mov     fp, sp
						;; bbWeight=1    PerfScore 1.50
G_M64149_IG02:
        4E010FF0          dup     v16.16b, wzr
        13001C00          sxtb    w0, w0
        4E011C10          ins     v16.b[0], w0
        4EB01E00          mov     v0.16b, v16.16b
						;; bbWeight=1    PerfScore 4.00
G_M64149_IG03:
        A8C17BFD          ldp     fp, lr, [sp],#16
        D65F03C0          ret     lr
						;; bbWeight=1    PerfScore 2.00

; Total bytes of code 32, prolog size 8, PerfScore 10.70, (MethodHash=3757056a) for method System.Runtime.Intrinsics.Vector128:CreateScalar(byte):System.Runtime.Intrinsics.Vector128`1[SByte]
; ============================================================
```

```asm
; Assembly listing for method System.Runtime.Intrinsics.Vector128:CreateScalar(float):System.Runtime.Intrinsics.Vector128`1[Single]
; Emitting BLENDED_CODE for generic ARM64 CPU - Windows
; optimized code
; fp based frame
; partially interruptible
; Final local variable assignments
;
;  V00 arg0         [V00,T00] (  3,  3   )   float  ->   d0        
;# V01 OutArgs      [V01    ] (  1,  1   )  lclBlk ( 0) [sp+0x00]   ""OutgoingArgSpace""
;* V02 tmp1         [V02    ] (  0,  0   )  simd16  ->  zero-ref    HFA(simd16)  ""struct address for call/obj""
;  V03 tmp2         [V03,T01] (  2,  2   )  simd16  ->  d16         HFA(simd16)  ld-addr-op ""Inline ldloca(s) first use temp""
;
; Lcl frame size = 0

G_M28940_IG01:
        A9BF7BFD          stp     fp, lr, [sp,#-16]!
        910003FD          mov     fp, sp
						;; bbWeight=1    PerfScore 1.50
G_M28940_IG02:
        4E040FF0          dup     v16.4s, wzr
        6E040410          ins     v16.s[0], v0.s[0]
        4EB01E00          mov     v0.16b, v16.16b
						;; bbWeight=1    PerfScore 3.50
G_M28940_IG03:
        A8C17BFD          ldp     fp, lr, [sp],#16
        D65F03C0          ret     lr
						;; bbWeight=1    PerfScore 2.00

; Total bytes of code 28, prolog size 8, PerfScore 9.80, (MethodHash=910d8ef3) for method System.Runtime.Intrinsics.Vector128:CreateScalar(float):System.Runtime.Intrinsics.Vector128`1[Single]
; ============================================================
```

```asm
; Assembly listing for method System.Runtime.Intrinsics.Vector128:CreateScalar(ushort):System.Runtime.Intrinsics.Vector128`1[UInt16]
; Emitting BLENDED_CODE for generic ARM64 CPU - Windows
; optimized code
; fp based frame
; partially interruptible
; Final local variable assignments
;
;  V00 arg0         [V00,T00] (  3,  3   )  ushort  ->   x0        
;# V01 OutArgs      [V01    ] (  1,  1   )  lclBlk ( 0) [sp+0x00]   ""OutgoingArgSpace""
;* V02 tmp1         [V02    ] (  0,  0   )  simd16  ->  zero-ref    HFA(simd16)  ""struct address for call/obj""
;  V03 tmp2         [V03,T01] (  2,  2   )  simd16  ->  d16         HFA(simd16)  ld-addr-op ""Inline ldloca(s) first use temp""
;
; Lcl frame size = 0

G_M480_IG01:
        A9BF7BFD          stp     fp, lr, [sp,#-16]!
        910003FD          mov     fp, sp
						;; bbWeight=1    PerfScore 1.50
G_M480_IG02:
        4E020FF0          dup     v16.8h, wzr
        53003C00          uxth    w0, w0
        4E021C10          ins     v16.h[0], w0
        4EB01E00          mov     v0.16b, v16.16b
						;; bbWeight=1    PerfScore 4.00
G_M480_IG03:
        A8C17BFD          ldp     fp, lr, [sp],#16
        D65F03C0          ret     lr
						;; bbWeight=1    PerfScore 2.00

; Total bytes of code 32, prolog size 8, PerfScore 10.70, (MethodHash=5a77fe1f) for method System.Runtime.Intrinsics.Vector128:CreateScalar(ushort):System.Runtime.Intrinsics.Vector128`1[UInt16]
; ============================================================
```

```asm
; Assembly listing for method System.Runtime.Intrinsics.Vector128:CreateScalar(int):System.Runtime.Intrinsics.Vector128`1[UInt32]
; Emitting BLENDED_CODE for generic ARM64 CPU - Windows
; optimized code
; fp based frame
; partially interruptible
; Final local variable assignments
;
;  V00 arg0         [V00,T00] (  3,  3   )     int  ->   x0        
;# V01 OutArgs      [V01    ] (  1,  1   )  lclBlk ( 0) [sp+0x00]   ""OutgoingArgSpace""
;* V02 tmp1         [V02    ] (  0,  0   )  simd16  ->  zero-ref    HFA(simd16)  ""struct address for call/obj""
;  V03 tmp2         [V03,T01] (  2,  2   )  simd16  ->  d16         HFA(simd16)  ld-addr-op ""Inline ldloca(s) first use temp""
;
; Lcl frame size = 0

G_M21746_IG01:
        A9BF7BFD          stp     fp, lr, [sp,#-16]!
        910003FD          mov     fp, sp
						;; bbWeight=1    PerfScore 1.50
G_M21746_IG02:
        4E040FF0          dup     v16.4s, wzr
        4E041C10          ins     v16.s[0], w0
        4EB01E00          mov     v0.16b, v16.16b
						;; bbWeight=1    PerfScore 3.50
G_M21746_IG03:
        A8C17BFD          ldp     fp, lr, [sp],#16
        D65F03C0          ret     lr
						;; bbWeight=1    PerfScore 2.00

; Total bytes of code 28, prolog size 8, PerfScore 9.80, (MethodHash=4a35ab0d) for method System.Runtime.Intrinsics.Vector128:CreateScalar(int):System.Runtime.Intrinsics.Vector128`1[UInt32]
; ============================================================
```

```asm
; Assembly listing for method System.Runtime.Intrinsics.Vector128:CreateScalar(long):System.Runtime.Intrinsics.Vector128`1[UInt64]
; Emitting BLENDED_CODE for generic ARM64 CPU - Windows
; optimized code
; fp based frame
; partially interruptible
; Final local variable assignments
;
;  V00 arg0         [V00,T00] (  3,  3   )    long  ->   x0        
;# V01 OutArgs      [V01    ] (  1,  1   )  lclBlk ( 0) [sp+0x00]   ""OutgoingArgSpace""
;* V02 tmp1         [V02    ] (  0,  0   )  simd16  ->  zero-ref    HFA(simd16)  ""struct address for call/obj""
;  V03 tmp2         [V03,T01] (  2,  2   )  simd16  ->  d16         HFA(simd16)  ld-addr-op ""Inline ldloca(s) first use temp""
;
; Lcl frame size = 0

G_M2664_IG01:
        A9BF7BFD          stp     fp, lr, [sp,#-16]!
        910003FD          mov     fp, sp
						;; bbWeight=1    PerfScore 1.50
G_M2664_IG02:
        4E080FF0          dup     v16.2d, xzr
        4E081C10          ins     v16.d[0], x0
        4EB01E00          mov     v0.16b, v16.16b
						;; bbWeight=1    PerfScore 3.50
G_M2664_IG03:
        A8C17BFD          ldp     fp, lr, [sp],#16
        D65F03C0          ret     lr
						;; bbWeight=1    PerfScore 2.00

; Total bytes of code 28, prolog size 8, PerfScore 9.80, (MethodHash=714bf597) for method System.Runtime.Intrinsics.Vector128:CreateScalar(long):System.Runtime.Intrinsics.Vector128`1[UInt64]
; ============================================================
```

```asm
; Assembly listing for method System.Runtime.Intrinsics.Vector64:CreateScalar(int):System.Runtime.Intrinsics.Vector64`1[Int32]
; Emitting BLENDED_CODE for generic ARM64 CPU - Windows
; optimized code
; fp based frame
; partially interruptible
; Final local variable assignments
;
;  V00 arg0         [V00,T00] (  3,  3   )     int  ->   x0        
;# V01 OutArgs      [V01    ] (  1,  1   )  lclBlk ( 0) [sp+0x00]   ""OutgoingArgSpace""
;  V02 tmp1         [V02,T01] (  2,  4   )   simd8  ->  [fp+0x18]   HFA(double)  do-not-enreg[SF] ""struct address for call/obj""
;  V03 tmp2         [V03,T02] (  2,  2   )   simd8  ->   d0         HFA(double)  ld-addr-op ""Inline ldloca(s) first use temp""
;
; Lcl frame size = 16

G_M25863_IG01:
        A9BE7BFD          stp     fp, lr, [sp,#-32]!
        910003FD          mov     fp, sp
						;; bbWeight=1    PerfScore 1.50
G_M25863_IG02:
        0E040FE0          dup     v0.2s, wzr
        FD000FA0          str     d0, [fp,#24]
        FD400FA0          ldr     d0, [fp,#24]
        4E041C00          ins     v0.s[0], w0
						;; bbWeight=1    PerfScore 6.00
G_M25863_IG03:
        A8C27BFD          ldp     fp, lr, [sp],#32
        D65F03C0          ret     lr
						;; bbWeight=1    PerfScore 2.00

; Total bytes of code 32, prolog size 8, PerfScore 12.70, (MethodHash=80a89af8) for method System.Runtime.Intrinsics.Vector64:CreateScalar(int):System.Runtime.Intrinsics.Vector64`1[Int32]
; ============================================================
```

```asm
; Assembly listing for method System.Runtime.Intrinsics.Vector64:CreateScalar(byte):System.Runtime.Intrinsics.Vector64`1[SByte]
; Emitting BLENDED_CODE for generic ARM64 CPU - Windows
; optimized code
; fp based frame
; partially interruptible
; Final local variable assignments
;
;  V00 arg0         [V00,T00] (  3,  3   )    byte  ->   x0        
;# V01 OutArgs      [V01    ] (  1,  1   )  lclBlk ( 0) [sp+0x00]   ""OutgoingArgSpace""
;  V02 tmp1         [V02,T01] (  2,  4   )   simd8  ->  [fp+0x18]   HFA(double)  do-not-enreg[SF] ""struct address for call/obj""
;  V03 tmp2         [V03,T02] (  2,  2   )   simd8  ->   d0         HFA(double)  ld-addr-op ""Inline ldloca(s) first use temp""
;
; Lcl frame size = 16

G_M12309_IG01:
        A9BE7BFD          stp     fp, lr, [sp,#-32]!
        910003FD          mov     fp, sp
						;; bbWeight=1    PerfScore 1.50
G_M12309_IG02:
        0E010FE0          dup     v0.8b, wzr
        FD000FA0          str     d0, [fp,#24]
        FD400FA0          ldr     d0, [fp,#24]
        13001C00          sxtb    w0, w0
        4E011C00          ins     v0.b[0], w0
						;; bbWeight=1    PerfScore 6.50
G_M12309_IG03:
        A8C27BFD          ldp     fp, lr, [sp],#32
        D65F03C0          ret     lr
						;; bbWeight=1    PerfScore 2.00

; Total bytes of code 36, prolog size 8, PerfScore 13.60, (MethodHash=1802cfea) for method System.Runtime.Intrinsics.Vector64:CreateScalar(byte):System.Runtime.Intrinsics.Vector64`1[SByte]
; ============================================================
```

```asm
; Assembly listing for method System.Runtime.Intrinsics.Vector64:CreateScalar(float):System.Runtime.Intrinsics.Vector64`1[Single]
; Emitting BLENDED_CODE for generic ARM64 CPU - Windows
; optimized code
; fp based frame
; partially interruptible
; Final local variable assignments
;
;  V00 arg0         [V00,T00] (  3,  3   )   float  ->   d0        
;# V01 OutArgs      [V01    ] (  1,  1   )  lclBlk ( 0) [sp+0x00]   ""OutgoingArgSpace""
;  V02 tmp1         [V02,T01] (  2,  4   )   simd8  ->  [fp+0x18]   HFA(double)  do-not-enreg[SF] ""struct address for call/obj""
;  V03 tmp2         [V03,T02] (  2,  2   )   simd8  ->  d16         HFA(double)  ld-addr-op ""Inline ldloca(s) first use temp""
;
; Lcl frame size = 16

G_M44268_IG01:
        A9BE7BFD          stp     fp, lr, [sp,#-32]!
        910003FD          mov     fp, sp
						;; bbWeight=1    PerfScore 1.50
G_M44268_IG02:
        0E040FF0          dup     v16.2s, wzr
        FD000FB0          str     d16, [fp,#24]
        FD400FB0          ldr     d16, [fp,#24]
        6E040410          ins     v16.s[0], v0.s[0]
        1E604200          fmov    d0, d16
						;; bbWeight=1    PerfScore 6.50
G_M44268_IG03:
        A8C27BFD          ldp     fp, lr, [sp],#32
        D65F03C0          ret     lr
						;; bbWeight=1    PerfScore 2.00

; Total bytes of code 36, prolog size 8, PerfScore 13.60, (MethodHash=b5c65313) for method System.Runtime.Intrinsics.Vector64:CreateScalar(float):System.Runtime.Intrinsics.Vector64`1[Single]
; ============================================================
```

```asm
; Assembly listing for method System.Runtime.Intrinsics.Vector64:CreateScalar(ushort):System.Runtime.Intrinsics.Vector64`1[UInt16]
; Emitting BLENDED_CODE for generic ARM64 CPU - Windows
; optimized code
; fp based frame
; partially interruptible
; Final local variable assignments
;
;  V00 arg0         [V00,T00] (  3,  3   )  ushort  ->   x0        
;# V01 OutArgs      [V01    ] (  1,  1   )  lclBlk ( 0) [sp+0x00]   ""OutgoingArgSpace""
;  V02 tmp1         [V02,T01] (  2,  4   )   simd8  ->  [fp+0x18]   HFA(double)  do-not-enreg[SF] ""struct address for call/obj""
;  V03 tmp2         [V03,T02] (  2,  2   )   simd8  ->   d0         HFA(double)  ld-addr-op ""Inline ldloca(s) first use temp""
;
; Lcl frame size = 16

G_M37504_IG01:
        A9BE7BFD          stp     fp, lr, [sp,#-32]!
        910003FD          mov     fp, sp
						;; bbWeight=1    PerfScore 1.50
G_M37504_IG02:
        0E020FE0          dup     v0.4h, wzr
        FD000FA0          str     d0, [fp,#24]
        FD400FA0          ldr     d0, [fp,#24]
        53003C00          uxth    w0, w0
        4E021C00          ins     v0.h[0], w0
						;; bbWeight=1    PerfScore 6.50
G_M37504_IG03:
        A8C27BFD          ldp     fp, lr, [sp],#32
        D65F03C0          ret     lr
						;; bbWeight=1    PerfScore 2.00

; Total bytes of code 36, prolog size 8, PerfScore 13.60, (MethodHash=68536d7f) for method System.Runtime.Intrinsics.Vector64:CreateScalar(ushort):System.Runtime.Intrinsics.Vector64`1[UInt16]
; ============================================================
```
Collected JIT disassemblies with the changes rebased on top of latest master
```asm
; Assembly listing for method System.Runtime.Intrinsics.Vector64:CreateScalar(int):System.Runtime.Intrinsics.Vector64`1[UInt32]
; Emitting BLENDED_CODE for generic ARM64 CPU - Windows
; optimized code
; fp based frame
; partially interruptible
; Final local variable assignments
;
;  V00 arg0         [V00,T00] (  3,  3   )     int  ->   x0        
;# V01 OutArgs      [V01    ] (  1,  1   )  lclBlk ( 0) [sp+0x00]   ""OutgoingArgSpace""
;  V02 tmp1         [V02,T01] (  2,  4   )   simd8  ->  [fp+0x18]   HFA(double)  do-not-enreg[SF] ""struct address for call/obj""
;  V03 tmp2         [V03,T02] (  2,  2   )   simd8  ->   d0         HFA(double)  ld-addr-op ""Inline ldloca(s) first use temp""
;
; Lcl frame size = 16

G_M62450_IG01:
        A9BE7BFD          stp     fp, lr, [sp,#-32]!
        910003FD          mov     fp, sp
						;; bbWeight=1    PerfScore 1.50
G_M62450_IG02:
        0E040FE0          dup     v0.2s, wzr
        FD000FA0          str     d0, [fp,#24]
        FD400FA0          ldr     d0, [fp,#24]
        4E041C00          ins     v0.s[0], w0
						;; bbWeight=1    PerfScore 6.00
G_M62450_IG03:
        A8C27BFD          ldp     fp, lr, [sp],#32
        D65F03C0          ret     lr
						;; bbWeight=1    PerfScore 2.00

; Total bytes of code 32, prolog size 8, PerfScore 12.70, (MethodHash=ab590c0d) for method System.Runtime.Intrinsics.Vector64:CreateScalar(int):System.Runtime.Intrinsics.Vector64`1[UInt32]
; ============================================================
```

```asm
; Assembly listing for method System.Runtime.Intrinsics.Vector64:CreateScalar(ubyte):System.Runtime.Intrinsics.Vector64`1[Byte]
; Emitting BLENDED_CODE for generic ARM64 CPU - Windows
; optimized code
; fp based frame
; partially interruptible
; Final local variable assignments
;
;  V00 arg0         [V00,T00] (  3,  3   )   ubyte  ->   x0        
;# V01 OutArgs      [V01    ] (  1,  1   )  lclBlk ( 0) [sp+0x00]   ""OutgoingArgSpace""
;  V02 tmp1         [V02,T01] (  2,  4   )   simd8  ->  [fp+0x18]   HFA(double)  do-not-enreg[SF] ""struct address for call/obj""
;  V03 tmp2         [V03,T02] (  2,  2   )   simd8  ->   d0         HFA(double)  ld-addr-op ""Inline ldloca(s) first use temp""
;
; Lcl frame size = 16

G_M20083_IG01:
        A9BE7BFD          stp     fp, lr, [sp,#-32]!
        910003FD          mov     fp, sp
						;; bbWeight=1    PerfScore 1.50
G_M20083_IG02:
        0E010FE0          dup     v0.8b, wzr
        FD000FA0          str     d0, [fp,#24]
        FD400FA0          ldr     d0, [fp,#24]
        53001C00          uxtb    w0, w0
        4E011C00          ins     v0.b[0], w0
						;; bbWeight=1    PerfScore 6.50
G_M20083_IG03:
        A8C27BFD          ldp     fp, lr, [sp],#32
        D65F03C0          ret     lr
						;; bbWeight=1    PerfScore 2.00

; Total bytes of code 36, prolog size 8, PerfScore 13.60, (MethodHash=cedeb18c) for method System.Runtime.Intrinsics.Vector64:CreateScalar(ubyte):System.Runtime.Intrinsics.Vector64`1[Byte]
; ============================================================
```

```asm
; Assembly listing for method System.Runtime.Intrinsics.Vector64:CreateScalar(short):System.Runtime.Intrinsics.Vector64`1[Int16]
; Emitting BLENDED_CODE for generic ARM64 CPU - Windows
; optimized code
; fp based frame
; partially interruptible
; Final local variable assignments
;
;  V00 arg0         [V00,T00] (  3,  3   )   short  ->   x0        
;# V01 OutArgs      [V01    ] (  1,  1   )  lclBlk ( 0) [sp+0x00]   ""OutgoingArgSpace""
;  V02 tmp1         [V02,T01] (  2,  4   )   simd8  ->  [fp+0x18]   HFA(double)  do-not-enreg[SF] ""struct address for call/obj""
;  V03 tmp2         [V03,T02] (  2,  2   )   simd8  ->   d0         HFA(double)  ld-addr-op ""Inline ldloca(s) first use temp""
;
; Lcl frame size = 16

G_M58336_IG01:
        A9BE7BFD          stp     fp, lr, [sp,#-32]!
        910003FD          mov     fp, sp
						;; bbWeight=1    PerfScore 1.50
G_M58336_IG02:
        0E020FE0          dup     v0.4h, wzr
        FD000FA0          str     d0, [fp,#24]
        FD400FA0          ldr     d0, [fp,#24]
        13003C00          sxth    w0, w0
        4E021C00          ins     v0.h[0], w0
						;; bbWeight=1    PerfScore 6.50
G_M58336_IG03:
        A8C27BFD          ldp     fp, lr, [sp],#32
        D65F03C0          ret     lr
						;; bbWeight=1    PerfScore 2.00

; Total bytes of code 36, prolog size 8, PerfScore 13.60, (MethodHash=e95c1c1f) for method System.Runtime.Intrinsics.Vector64:CreateScalar(short):System.Runtime.Intrinsics.Vector64`1[Int16]
; ============================================================
```

There are multiple issues here:

1) Redundant str/ldr-s with a SIMD register - this appears only in Vector64<T>.CreateScalar():
```asm
str     d0, [fp,#24]
ldr     d0, [fp,#24]
```

The code is the worst for Vector64\<float\>.CreateScalar()
```asm
        0E040FF0          dup     v16.2s, wzr
        FD000FB0          str     d16, [fp,#24]
        FD400FB0          ldr     d16, [fp,#24]
        6E040410          ins     v16.s[0], v0.s[0]
        1E604200          fmov    d0, d16
```

or Vector64\<ushort\>.CreateScalar()
```asm
        0E020FE0          dup     v0.4h, wzr
        FD000FA0          str     d0, [fp,#24]
        FD400FA0          ldr     d0, [fp,#24]
        53003C00          uxth    w0, w0
        4E021C00          ins     v0.h[0], w0
```

2) Unnecessary sign-/zero-extensions with byte,ubyte,short,ushort (the same as seen in #35590):
```asm
uxtb    w0, w0
uxth    w0, w0
sxtb   w0, w0
sxth    w0, w0
```

3)  `dup Vd.T, wzr` seems to be used for code generation of Vector64/128<T>.Zero which I thought was fixed with #33924 (cc @github). I will follow up on this

cc @github @github "
"> Yes, that is correct. But we do the computation now at the point where we are just about to invoke the refactoring provider to fix all instances in the document, and the provider is going to request for syntax nodes to compute the fixes, so we are not really saving anything.

This is not actually true.  Consider if we needed to do fix-all on 100s to thousands of files.  This will go through and parse them all (just to find the length).  However, these trees are not held onto by *anything*.  Internally, we just have a weak-reference (so that if one is alive, and another asks for it, they'll get the same tree).  So if a *single* GC happens (which is virtually guaranteed in a fix-all scenario), then all those trees will be garbage collected and we'll have to parse them all over again as we actually fix each file.

You're only parsing/loading-text just to say ""i want to do the whole file"".  there's got to be a cleaner way of representing that doesn't involve pre-parsing and potentially redoing all that work later :)"
"@github, when you benchmarked, were 95% of your inputs already normalized?

I realize numbers are the final arbiter in any perf debate, but I'm having trouble imagining how anything in JS-space could be faster than a native indexOf (which also, I would think, produces no garbage)."
"@github seemed to think not, so on his recommendation I have been testing by compiling tinymce instead. However, it has around 1000 unions, which may not be enough to detect a worst-case slowdown. Maybe I should find a gnarly DT package, read it, then print it out and see how long printing it takes."
"> I think what @github was saying was that even if he does this, the -ci version would still be there. One thing we could do here would be to have this test be a msbuild-based test that runs as part of the build as opposed to a unit test, and have that msbuild-based test only run on official builds which would quickly check if the assembly version is correct which you can all do with msbuild (or worst case scenario write a small c# task that makes this check)

That's what I was trying to express. Don't validate as part of a managed xunit that which we don't run as part of the official build but instead validate as part of a post-build target which would error out if the AssemblyVersion isn't the expected one. Condition that target so that it only runs when it should."
"Please remove all instances of code that generates sequence numbers programmatically. It's incredibly misleading. Users keep doing this too, perhaps because they copy examples. Sequence numbers should always be literal values otherwise they don't serve any purpose."
"For this PR I believe strongly Ordinal is correct.  Using ordinal there is never a case where you'd fail to search a directory that needs to be searched.  At worst you search the same directory twice and end up providing the compiler with the same editor config twice.  That is a case we should be able to handle. 

Using ignore case though it's possible you miss searching a directory which needs to be searched.  "
"The ""only doing it once bit"" I've taken from just the standard `IDisposable` [implementation pattern](https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/implementing-dispose#the-disposebool-method-overload), but I think your question is really getting at the fact that I'm not sure exactly _why_ this has started happening in RC2.

At first I thought it was because `DeferredHost` actually was disposing twice in the same call, but it turned out I'd just misread the code. After that, it just _seemed_ correct to have both these types (which were to only two I looked at in this repo) follow that pattern anyway.

You're right though that `Host` itself also doesn't follow this pattern either, which is why I don't think this change is _the_ fix as two different objects are disposing the same underlying `IHost` twice overall, so even with these guards it's still disposed of twice, just from two different places that don't know about each other.

https://github.com/dotnet/runtime/blob/8608dca513a9be1f1bfc6a31deb8b22639a33d9f/src/libraries/Microsoft.Extensions.Hosting/src/Internal/Host.cs#L162-L195

Maybe the fix is to instead (or also) make a similar change there too?
"
"Again,
> This isn't incredibly urgent. But, might be nice to get it into ~~3.1.15~~ _5.0.5_ if dependency flow_ hasn't reached this repo by the time the changes are approved and validated.
> /fyi @github 
> /btw this is `tell-mode` because it only affects samples and tests."
"do we have to be parallel?  This scares me *in proc*.  This can saturate cores, and cause lots of garbage to be created (causing gc pauses/hangs).  in-proc that can seriously degrade the editing experience.  We try to avoid this if possible."
"Nit: This should not be needed. You can set it to garbage value in debug-only. "
"Couldn't the `index <0 ` check extended to a range check of AttributeCount? I.e. `if(index < 0 || index >= AttributeCount)`? However since AttributeCount includes the same logic as here, this would run the same iteration code twice in the worst case.
Otherwise yes, the last line should throw as well."
"So we saw a crash here in `Running TestUnmanagedCallersOnlyValid_OnNewNativeThread...` which is a good test - it runs some managed code on a foreign native thread.  Which means that the runtime will still have a `MonoThreadInfo` with a TLS dtor on that foreign thread which will call `mono_thread_detach_internal` (via the `thread_detach` callback) which will come here.

But by the time we get here, the domain is already unset.  So the JIT will die.

```
=================================================================
      Native stacktrace:
      =================================================================
      0x10ee5e6d6 - /private/tmp/helix/working/AE8409AC/p/libcoreclr.dylib : mono_dump_native_crash_info
      0x10ee024ae - /private/tmp/helix/working/AE8409AC/p/libcoreclr.dylib : mono_handle_native_crash
      0x10ed5a515 - /private/tmp/helix/working/AE8409AC/p/libcoreclr.dylib : mono_sigsegv_signal_handler_debug
      0x7fff69677f5a - /usr/lib/system/libsystem_platform.dylib : _sigtramp
      0x0 - Unknown
      0x10ed59fb1 - /private/tmp/helix/working/AE8409AC/p/libcoreclr.dylib : lookup_method
      0x10ed58c92 - /private/tmp/helix/working/AE8409AC/p/libcoreclr.dylib : mono_jit_compile_method_with_opt
      0x10ed5d09f - /private/tmp/helix/working/AE8409AC/p/libcoreclr.dylib : mono_jit_runtime_invoke
      0x10ec6215f - /private/tmp/helix/working/AE8409AC/p/libcoreclr.dylib : mono_runtime_try_invoke
      0x10ec643fe - /private/tmp/helix/working/AE8409AC/p/libcoreclr.dylib : mono_runtime_try_invoke_handle
      0x10ec7825b - /private/tmp/helix/working/AE8409AC/p/libcoreclr.dylib : abandon_mutexes
      0x10ec7513c - /private/tmp/helix/working/AE8409AC/p/libcoreclr.dylib : mono_thread_detach_internal
      0x10ec77cdd - /private/tmp/helix/working/AE8409AC/p/libcoreclr.dylib : thread_detach
      0x10ece23c4 - /private/tmp/helix/working/AE8409AC/p/libcoreclr.dylib : unregister_thread
      0x10ece2c20 - /private/tmp/helix/working/AE8409AC/p/libcoreclr.dylib : thread_info_key_dtor
      0x7fff69683163 - /usr/lib/system/libsystem_pthread.dylib : _pthread_tsd_cleanup
      0x7fff69682ee9 - /usr/lib/system/libsystem_pthread.dylib : _pthread_exit
      0x7fff6968166c - /usr/lib/system/libsystem_pthread.dylib : pthread_sigmask
      0x7fff6968150d - /usr/lib/system/libsystem_pthread.dylib : _pthread_body
      0x7fff69680bf9 - /usr/lib/system/libsystem_pthread.dylib : thread_start
 ```

---

So the gist of the current approach is: ""call OnThreadExiting"" from the dying thread.

I think that's not a workable idea - we already started tearing down the thread and calling managed code will be fragile at this point.

Instead I think we should do this: create a finalizable managed object that is immediately garbage and give it the responsibility to call OnThreadExiting by giving it a reference to the managed thread object.

We have the managed thread object still, because (except in the case of shutdown) it's not destroyed when the native thread is destroyed - it goes away when the last reference to it goes away - but the `MonoThreadInfo` is still holding a handle to it.

The new object will be created and it won't have any references to it as soon as the native thread exits. So on the next GC it will be garbage.  When the finalizer runs on the finalizer thread, it should abandon the mutexes.  (I guess one complication is that we can't just put a managed reference to the thread into the object since it'll be null when the finalizer runs - so maybe put a GC Handle in there - that seems dangerous, but maybe it's ok?)
"
"It's a good warning, but this class broke the rule already so suppressing to stay consistent. It is incredibly frustrating to have all of these overloads (which I observed while writing tests). For EmbeddedText.FromXxx equivalents, the warning helped me to make separate FromBytes, FromStream, etc. factories. So I think the warning has value. Maybe we should just put a disable/restore pair aorund the full set of From overloads here and not suppress every time we add a new one. (Hopefully we won't add more, though).

---

In reply to: [83500492](https://github.com/dotnet/roslyn/pull/14531#discussion_r83500492) [](ancestors = 83500492)
"
"> It seems like the state we want to join into the local function's StartingState is one where all arguments have been visited, but postconditions from the call have not yet been applied.

Good point. Maybe we should call `VisitLocalFunctionUse` at the start of `VisitCall` rather than as we currently do (at the end of `VisitCall`.

That said, I'm not sure if that's Aleksey's question. 
The way I understand it: the reason why we `Join` is that we want to get the worst case state at the start of all local function invocations, then we'll analyze the local function with that state."
"Draft commit message: 

```
Currently, Xamarin.Android supports compression of managed assemblies
within the `.apk` if the app is built with
[`$(BundleAssemblies)`=True][0], with the compressed assembly data
stored within `libmonodroid_bundle_app.so` using gzip compression and
placed in an array inside the data section of the shared library.

There are two problems with this approach:

 1. `mkbundle` emits C code, which requires a C compiler which requires
    the full Android NDK, and thus requires Visual Studio Enterprise.

 2. Reliance on Mono's `mkbundle` results in possible issues around
    [filename globbing][1] such that
    `Xamarin.AndroidX.AppCompat.Resources.dll` is improperly treated
    as a [satellite assembly][2].

Because of (2), we are planning on [removing support][3] for
`$(BundleAssemblies)` in .NET 6 ([nÃ©e .NET 5][4]), which resulted in
[some pushback][5] because `.apk` size is very important for some
customers, and the startup overheads we believed to be inherent to
`$(BundleAssemblies)` turned out to be somewhat over-estimated.

To resolve the above issues, add an assembly compression mechanism
that doesn't rely on `mkbundle` and the NDK: separately compress the
assemblies and store the compressed data within the `.apk`.
Compression is performed using the [managed implementation][6] of the
excellent [LZ4][7] algorithm.  This gives us a decent compression ratio
and a much faster (de)compression speed than gzip/zlib offer.  Also,
assemblies are stored directly in the APK in their usual directory,
which allows us to [**mmap**(2)][8] them in the runtime directly from
the `.apk`.  The build process calculates the size required to store
the decompressed assemblies and adds a data section to
`libxamarin-app.so` which causes *Android* to allocate all the required
memory when the DSO is loaded, thus removing the need of dynamic memory
allocation and making the startup faster.

Compression is supported only in `Release` builds and is enabled by
default, but it can be turned off by setting the
`$(AndroidEnableAssemblyCompression)` MSBuild property to `False`.
Compression can be disabled for an individual assembly by setting the
`%(AndroidSkipCompression)` MSBuild item metadata to True for the
assembly in question, e.g. via:

	<AndroidCustomMetaDataForReferences Include=""MyAssembly.dll"">
	  <AndroidSkipCompression>true</AssemblySkipCompression>
	</AndroidCustomMetaDataForReferences>

The compressed assemblies still use their original name, e.g.
`Mono.Android.dll`, so that we don't have to perform any string
matching on the runtime in order to detect whether the assembly we are
asked to load is compressed or not.  Instead, the compression code
*prepends* a short header to each `.dll` file (in pseudo C code):

	struct CompressedAssemblyHeader {
	    uint32_t magic;                 // 0x5A4C4158; 'XALZ', little-endian
	    uint32_t descriptor_index;      // Index into an internal assembly descriptor table
	    uint32_t uncompressed_length;   // Size of assembly, uncompressed
	};

The decompression code looks at the `mmap`ed data and checks whether
the above header is present.  If yes, the assembly is decompressed,
otherwise it's loaded as-is.

It is important to remember that the assemblies are compressed at
build time using LZ4 block compression, which requires assembly data
to be entirely loaded into memory before compression; we do this
instead of using the LZ4 frame format to make decompression at runtime
faster.  The compression output also requires a separate buffer, thus
memory consumption at *build* time will be roughly 1.5x the size of the
largest assembly, which is reused across all assemblies.


~~ Application Size ~~

A Xamarin.Forms ""Hello World"" application `.apk` shrinks by 27% with
this approach for a single ABI:

|    Before (bytes) |   LZ4 (bytes) |     Î”     |
|------------------:|--------------:|:---------:|
|        23,305,194 |    16,813,034 |  -27.85%  |

Size comparison between this commit and `.apk`s created with
`$(BundleAssemblies)` =True depends on the number of enabled ABI
targets in the application.  For each ABI, `$(BundleAssemblies)`=True
creates a separate shared library, so the amount of space consumed
increases by the size of the bundle shared library.

The new compression scheme shares the compressed assemblies among all
the enabled ABIs, thus effectively creating smaller multi-ABI `.apk`s.

In the tables below, `mkbundle` refers to the APK created with
`$(BundleAssemblies)`=True, `lz4` refers to the `.apk` build with
the new compression scheme:

|                                  ABIs |  mkbundle (bytes) |   LZ4 (bytes) |    Î”    |
|--------------------------------------:|------------------:|--------------:|---------|
|   armeabi-v7a, arm64-v8a, x86, x86_64 |        27,130,240 |    16,813,034 | -38.03% |
|                             arm64-v8a |         7,783,449 |     8,746,878 | +11.01% |

The single API case is ~11% larger because gzip offers better
compression, at the cost of higher runtime startup overhead.


~~ Startup Performance ~~

When launching the Xamarin.Forms ""Hello World"" application on a
Pixel 3 XL, the use of LZ4-compressed assemblies has at worst a ~1.58%
increase in the Activity Displayed time (64-bit app w/ assembly
preload enabled), while slightly faster on 32-bit apps, but is *always*
faster than the mkbundle startup time for all configurations:

|                                   |           |               |           |  LZ4 vs  |   LZ4 vs   |
|                       Description | None (ms) | mkbundle (ms) |  LZ4 (ms) |  None Î”  | mkbundle Î” |
|----------------------------------:|----------:|--------------:|----------:|:--------:|:----------:|
|     preload enabled; 32-bit build |     795.8 |         855.6 |     783.8 | -0.25% âœ“ |  -7.22% âœ“  |
|    preload disabled; 32-bit build |     777.1 |         843.0 |     780.5 | +0.44% âœ— |  -7.41% âœ“  |
|     preload enabled; 64-bit build |     779.0 |         843.0 |     791.5 | +1.58% âœ— |  -6.82% âœ“  |
|    preload disabled; 64-bit build |     776.0 |         841.6 |     781.5 | +0.69% âœ— |  -7.15% âœ“  |


[0]: https://docs.microsoft.com/en-us/xamarin/android/deploy-test/release-prep/?tabs=windows#bundle-assemblies-into-native-code
[1]: https://github.com/xamarin/AndroidX/issues/64
[2]: https://github.com/mono/mono/blob/9b4736d4c271e9d4e04cafa258ddd58961f1a39f/mcs/tools/mkbundle/mkbundle.cs#L1315-L1317
[3]: https://github.com/xamarin/AndroidX/issues/64#issuecomment-609970584
[4]: https://devblogs.microsoft.com/dotnet/announcing-net-5-preview-4-and-our-journey-to-one-net/
[5]: https://github.com/xamarin/AndroidX/issues/64#issuecomment-610002467
[6]: https://www.nuget.org/packages/K4os.Compression.LZ4/
[7]: https://github.com/lz4/lz4
[8]: https://linux.die.net/man/2/mmap
```"
"```suggestion
.NET has a long-standing history of taking API usability extremely seriously. Thus, we generally review every single API that is added to the product. This page discusses how we conduct design reviews for the Roslyn components.
```"
"Thanks orta for looking into that.
I don't think, we should depend on the os to determine how to get canonical file name at all.. Considering `Ä°Ä±` as unique(that is not changing file casing for those) is right thing to do.. In worst cases we wont recognize that file with `Ä°` and `iÌ‡` are not same which would be really a small group. The `GetCanonicalFileName` is internal compiler thing that currently is based on if file system is case sensitive or not and we shouldn't change that fact. It is an ok trade off to do in my opinion

cc: @github  and @github to see if they agree."
"Right. Enumerating just the containing arrays is basically the same as enumerating handles and we do that a few lines later.

My concern was that if multiple threads will start fight for every static variable, there could be some issues with false sharing. If you have N threads and K statics, it seems it would result in N*K accesses to the same data and in a worst case nearly all will be cache misses."
"I agree. valueDeclaration is the worst offender in my opinion, but replacing just the public API isn't the right way to go. We would welcome a PR that changes the actual type and inserts `!` in the right places, but I'm going to close this PR."
"> Worst case I'll need to remove the DIM and manually declare the implementation for all 20 types.

I've done this with [baf69de](https://github.com/dotnet/runtime/pull/69756/commits/baf69de8576f8528125e6ddee518d3dd310e9e9b). You'll want to remember to revert this if trying to repro the failures above"
"My thought process was that I'm trying to optimize for the worst-case scenario. When the server is under maximum load, the entire buffer will be allocated anyways, so I would rather error early than during a traffic spike. "
"`out var` all the things! (seriously, there's a bunch here)"
"> Please place `HostAbortedException` in the .Sources package instead. Worst case, use `#if` to decide between an exception in the Abstractions library and something in the .Sources package (maybe an optionally-defined inner `class`).

That's not going to work because the type won't have the same identity as the one in Microsoft.Extensions.Hosting, isn't that the case?"
"expecting the behaviors to be detached t this point might be a valid assumption, except that you have to take into account the unpredictable behavior of the Garbage Collector, or collectors.

e.g., in the case of the Mono GC, the TestSecondPage is probably not collect yet at this point."
"> But the question is really if we need to allow WeakReference(null) in the first place.

The docs say it throws an exception if ""Garbage collection has already been performed on wo."" Are you suggesting that should be changed?"
"For a small fraction of the original repro

Before:
```
Duration: 141.75s, Total samples = 138099ms (97.43%)
Showing nodes accounting for 126840ms, 91.85% of 138099ms total
Dropped 2840 nodes (cum <= 690.50ms)
      flat  flat%   sum%        cum   cum%
   28741ms 20.81% 20.81%    28974ms 20.98%  (anonymous) e:\ts_gh\built\local\tsc.js:113326
   26903ms 19.48% 40.29%    97402ms 70.53%  forEachFileAndExportsOfFile e:\ts_gh\built\local\tsc.js:113307
   24543ms 17.77% 58.06%    24550ms 17.78%  (anonymous) e:\ts_gh\built\local\tsc.js:113334
   20966ms 15.18% 73.25%    20966ms 15.18%  (garbage collector)
   16251ms 11.77% 85.01%    95676ms 69.28%  (anonymous) e:\ts_gh\built\local\tsc.js:113318
```

(The anonymous functions are the lambdas in `forEachFileAndExportsOfFile`)

After
```
Duration: 21.43s, Total samples = 20772ms (96.91%)
Showing nodes accounting for 14223ms, 68.47% of 20772ms total
Dropped 2475 nodes (cum <= 103.86ms)
      flat  flat%   sum%        cum   cum%
    1998ms  9.62%  9.62%     1998ms  9.62%  mark perf_hooks.js:391
    1094ms  5.27% 14.89%     1094ms  5.27%  measure perf_hooks.js:397
     969ms  4.66% 19.55%      969ms  4.66%  (garbage collector)
     922ms  4.44% 23.99%      922ms  4.44%  stat
     642ms  3.09% 27.08%     2125ms 10.23%  createUnionOrIntersectionProperty e:\ts_gh\built\local\tsc.js:55535
```"
"Warning: This file is a bit of a garbage fire when it comes to nullability. Methods are BIG and reuse variables a lot.

Lots of `!`"
"> LOG_INFOF(L""get_hostfxr_path failed""); 

In general, for diagnosing issues with the failed call, it would be useful to log out the return value. The APIs will return different error codes based on what failed, so that can help narrow down what is going on.

>  garbage in the corehosttrace 

I took a look through how the parameters were being set in [commit 481e0e9](https://github.com/dotnet/aspnetcore/commit/481e0e995e68293460dc1696de7004a2f6d40216). I suspect the issue is with the use of `parent_path()`:
https://github.com/dotnet/aspnetcore/blob/481e0e995e68293460dc1696de7004a2f6d40216/src/Servers/IIS/AspNetCoreModuleV2/CommonLib/HostFxrResolver.cpp#L81-L84

I'm not familiar with the `std::filesystem::path` APIs, but based on the [parent_path doc](https://en.cppreference.com/w/cpp/filesystem/path/parent_path), I'd guess what is happening is that the `path` returned by `parent_path()` has gone out of scope when `get_hostfxr_path` is called, so `params.dotnet_root` just ends up pointing to whatever happens to be at that address now (which would explain why it sometimes has what is expected and sometimes does not).
"
"Since we aren't making any text buffer edits ourselves, I don't think we need any special handling for inactive regions. The worst that will happen is typing <kbd>;</kbd> will move the caret without changing any text. Perhaps a little weird, but not horrible."
"In what situation would _size != 0 but _items == null?  I'm wondering why the second check is needed (and worst case, if there's some corner case that could result in that, it would fall out of the subsequent type checks, both of which would be false)."
"in the worst case, it could be a Method or a Property and that happens **only if** we encounter an expression bodied member syntax **and not** for those with block body.
"
"You're right, but I also don't see any clear way of improving on this. We don't have a way to know for sure whether `_scopeFactory` was already disposed, or even if we did we couldn't lock it to prevent it being disposed in between that check and the usage of it or any services it supplies.

Like you've pointed out, the worst case is that it will log the exception, but the behavior will still be correct. Given this cycle only occurs every 30 mins by default, this should be extremely rare, if app developers ever see it at all.

If this becomes an issue in the future, I suspect we might need to introduce some event like ""circuit will shut down soon"" that things like this can listen to and proactively stop themselves."
"(more seriously: I could give a ""something broke"" message but not sure that actually helps anyone...)"
"it seriously worries me that:

1. Enter is being sent through as typing '\n'.
2. Commit is being sent through as typing \0
 #Pending"
Can we have this merged? It's incredibly frustrating
"Similarly, if `androidRuntime` is null, something is *seriously* FUBAR. I'd rather have the NRE from something ""impossible"" happening."
"Changing this to draft for now because I just discovered a very strange bug:

If the textbox has some text in it and then you check the ""use parent process directory"" checkbox, it works fine (text gets erased, textbox is disabled)

However, if the textbox is focused and has _no_ text in it and then you check the ""use parent process directory"" checkbox, the checkbox immediately unchecks itself, which is incredibly perplexing"
"The way the issue got resolved is very much not ideal (it placed a similar filter into mono's AOT step, so this filter is also needed).

Correct fix should have been to make it so that the problematic DLL doesn't get into CORE_ROOT in the first place. The fact we still have the file in CORE_ROOT means we're sending a garbage file to all test machines."
"We have two unused bits left in `TransformFlags`; `27` and `28`, so we can do it. But even as is the next new emit feature that seriously needs transform flags (like, say, proper lexical `arguments`) is already in for a bad time what with there not being enough flags. But I suppose that's a future problem.
  "
"FWIW I think @github has the right idea here. Create a buffer large enough to hold the worst-case scenario, then encode straight to that buffer. I assume the typical case is only a few dozen chars? You can use `Encoding.UTF8.GetMaxByteCount(input)`, which will tell you that the worst case scenario is ~3x expansion, then figure out what the worst-case base64 expansion is.

Rent a `byte[]` to hold the UTF-8 conversion and a `char[]` to hold the base64 conversion. Then call `string.Create`, passing in your `char[]` as the state object. This should all be allocation-free."
"I've looked at the source generators, and with my little understanding, it is not that easy:

1) The source generator for Razor converts component syntax to C#. At the time of code generation, the inner classes do not exist yet (as this is generated by the C# compiler), so no way to find out which closure type corresponds to which lambda.
2) At this moment the lambda is just rewritten as `EventCallback.Factory.Create(this, <labda>)>` and handed off to the C# compiler. If you'd want to generate any map of free variables you would have to start parsing the lambda. Although probably doable, it requires two passes to get a more or less complete image of all free variables (as code blocks can come after it) to not generate any garbage. It is certainly more complex than the runtime solution.
3) The `EventCallback.Factory.CreateBinder` case can probably be done with a combination of the source generator and a change of the method signature, but this would introduce a breaking change in the public API for all `EventCallback<ChangeEventArgs> CreateBinder(` overloads. I'm not sure whether that is desired. It would also mean that anybody using these methods directly and using a closure would still create delegates that trigger the DiffBuilder to change EventHandler ids. 

If anything, I would say that the closure types generated by Roslyn would benefit from a generated comparison method that would roughly correspond to this logic. That is a change on another level though.

General points regarding execution time concerns:

- Just like in the old situation, two delegates are compared with a normal `Equals()` first. If the EventCallbacks/delegates point to the same method on the same instance, nothing changes in execution speed when diffing an attribute that is bound to an EventCallback / delegate.
- With caching the comparison function per type, even in the case of interpreted WebAssembly, the reflection is only executed once. The generated IL for retrieving a field is as optimal as it can get, (well, maybe with refs to value types it can be optimized a little more), it won't trigger any of the normal safety checks that is inside normal reflection in the hot path subsequently.
- Comparing closures like this is still infinitely faster than actually generating a diff and sending that over the wire to be handled inside TypeScript.

There is still another way to solve this issue that does not require any reflection or closure introspection at all: Just always replace the event callback inside .NET, that way you only have to send an event handler id to the browser if the attribute is first added with an event callback attached. As long as the attribute stays in the diff at the same location and there is an event callback attached, it's event handler Id would remain the same. This changes the dynamic between the browser and Blazor though, where you could have the situation that an event handler is already ""updated"" before the accompanying render batch is sent and/or processed by the browser. I think the original design intention was to prevent this situation, which is why I haven't pursued this.
"
"The idls are the contract, once they're published we shouldn't change them. Just changing the name and not the value isn't the worst case (existing code would run fine) but people would have compiler errors when they went to rebuild.

I think it would be better to leave it as is and add a comment saying it refers to the native com handle since winrt handles don't exist anymore."
"\b doesn't work since it doesn't consider `$` and `_`  as `word` character. Powershell, php would break seriously. Javascript allows $ and _ in variable names so they would have quirks as well. 

"
"How large can `size` be here, in the worst case, and how come we can get away with using GC-backed array? `JsonDocument` APIs are meant to be amortized non-allocating for payloads under 2 MB."
"> I don't see the value here -- why have every caller have to do this when the function itself can just do it once?

It's really easy to make mistakes. Either strategy could be supported by analyzers, but placing in the callee requires the analyzer be full solution analysis _and_ cross-assembly (worst case performance scenario). Placing the token in the caller is ideal for implementation, code review, runtime performance, and automated analysis."
"> Updated CalculateEffectiveProtocols to make sure the low bit is never set.

Well, sort of.  It clears the bit.  If the TLS1.4/2.0/whatever identifier had the bit set then the routine would just decide it was irrelevant.

If unknown values are caught before CalculateEffectiveProtocols, then it should instead assert that the bit was clear, instead of clearing it.  If CalculateEffectiveProtocols can be fed garbage then we should add a test that does something like

```C#
[Fact]
public static void SslProtocolsLowBitAvailable()
{
    foreach (SslProtocols value in Enum.GetValues<SslProtocols>())
    {
        Assert.True((value & (SslProtocols)1) == 0, $""SslProtocols.{value} does not use the least significant bit"");
    }
}
```"
"```suggestion
Each OSR method may comprise most or all of the original method. So in the worst case we can have a lot of OSR codegen, but it is currently thought such cases
```"
"`garbage` might create an issue with polycheck. #Resolved
"
"We can, and should, just be using `StoreUnsafe` on both.

For modern CPUs on x86/x64, the instructions all support unaligned loads and so `StoreAligned` has limitations on when it can be folded (e.g. `T1` only, etc). `StoreUnaligned` does not have these limitations and doesn't imply other checks.

It's better to process the data ""as if it was pinned"" and optimistically operate on aligned chunks. In the normal case, the GC won't interrupt and move the data and everything is ""ok"". In the worst case, the GC interrupts, moves the data, and it's no longer aligned but since you're doing unaligned loads/stores, the algorithm keeps working anyways and you just pay the minor cost.

This works well with actual pinned data and unpinned data where you just have the `byref`. The latter ends up playing better with the GC as well, so we really should prefer it for most of our code."
"> We might need to make some changes to VB though - since we can't link GeneratedNameParser.vb to C# code. Perhaps the best would be if both of these parsers were moved to Microsoft.CodeAnalysis and written in C#.

Yea. Worst case we end up exposing something with MEF and link appropriately to each. It seems doable though, and I'm glad we have `GeneratedNameParser` to handle this "
"> It seems like the HTTP callback is not ready to called multiple times.

This is specific to HttpClientHandler (you're not hitting this when using SocketsHttpHandler directly, right?).  The callback signature from HttpClientHandler expects the HttpRequestMessage to be passed in, so we need to capture the instance into the closure used for the underlying RemoteCertificateValidationCallback so that it can be in turn passed to the handler's ServerCertificateCustomValidationCallback.  But as a result of capturing it in that way, the HttpRequestMessage would end up being referenced for the lifetime of the callback, thus keeping it alive and from being garbage collected.  The fix for this issue (https://github.com/dotnet/runtime/issues/36979) was to null out the field after the callback was invoked (https://github.com/dotnet/runtime/pull/37021), with the assumption that it wouldn't be invoked again.  If it is invoked again, then it triggers the assert put in place to validate it wasn't being invoked again.

So, we have a choice to make, either:
1. Ensure the callback is never invoked multiple times.  But it sounds like that's not actually viable, that it's expected to be invoked multiple times in the face of renegotation if the certificate changes.
2. Allow null to be passed instead of the original message on renegotation.  This is in fact what's happening today, and the nullable annotation on the parameter is wrong.  But if we want to accept that, then a) we need to remove the assert, and b) we need to change the first argument of the callback from `HttpRequestMessage` to `HttpRequestMessage?` to signify that it can actually be null, so that callbacks guard against it if they happen to be using the request message.
3. Hold on to the message for the lifetime of the connection in case it may be needed for a subsequent callback invocation, essentially reverting #37021.
4. Expose something on HttpClientHandler to allow this choice to be made by the dev, e.g. exposing AllowRenegotation.  But as it would default to true as is currently the case, this choice alone is likely insufficient.

None of these options seem great, but (2) is probably the best.  @github? (@github, FYI on nullable annotations)"
"What methods specifically do you need optimized? The only one that is likely to regress is `Equals(StringSegment)`, because it's now bouncing through an indirection. But I can hand-tune that if needed.

Reiterating my earlier comment: if this is _really_ in a hot path, seriously consider allowing some behavioral changes here. For instance, the fact that there's special-casing all over the place to distinguish between `default(StringSegment)` and `StringSegment.Empty` leads to unnecessary complexity + slower perf. Allowing the two to be treated as equivalent would optimize things.

Allowing this change in behavior would make existing methods dumb wrappers around the already highly-optimized span versions. For example:

```cs
// Pretty much the whole call stack gets inlined into the caller at this point.
public bool Equals(StringSegment other) => this.AsSpan().SequenceEquals(other.AsSpan());
```"
Seems like this regressed our worst cast TLS scenario that closes the connection every request. Looking into it.
"> @github, what do you think about adding {M}IBC to glossary. To my understanding:
> 
> ```diff
> --- a/docs/project/glossary.md
> +++ b/docs/project/glossary.md
> @@ -24,12 +24,14 @@ terminology.
>  | DAC | Data Access Component. An abstraction layer over the internal structures in the runtime. |
>  | EE | [Execution Engine](https://docs.microsoft.com/dotnet/standard/managed-execution-process#running_code). |
>  | GC | [Garbage Collector](https://github.com/dotnet/runtime/blob/main/docs/design/coreclr/botr/garbage-collection.md). |
> +| IBC | Instrumented Block Counts - used as extension (`*.ibc`) for old PGO files. |
>  | IPC | Inter-Process Communication. |
>  | IL | Intermediate Language. Equivalent to CIL, also equivalent to [MSIL](https://docs.microsoft.com/dotnet/standard/managed-execution-process#compiling-to-msil). |
>  | JIT | [Just-in-Time](https://github.com/dotnet/runtime/blob/main/docs/design/coreclr/jit/ryujit-overview.md) compiler. RyuJIT is the code name for the next generation Just-in-Time(aka ""JIT"") for the .NET runtime. |
>  | LCG | Lightweight Code Generation. An early name for [dynamic methods](https://github.com/dotnet/runtime/blob/main/src/coreclr/System.Private.CoreLib/src/System/Reflection/Emit/DynamicMethod.cs). |
>  | MD | MetaData. |
>  | MDA | Managed Debugging Assistant - see [details](https://docs.microsoft.com/dotnet/framework/debug-trace-profile/diagnosing-errors-with-managed-debugging-assistants) (Note: Not in .NET Core, equivalent diagnostic functionality is made available on a case-by-case basis, e.g. [#9418](https://github.com/dotnet/runtime/issues/9418)) |
> +| MIBC | Modern Instrumented Block Counts - used as extension (`*.mibc`) for modern PGO files. |
>  | MSIL | [Microsoft Intermediate Language](https://docs.microsoft.com/dotnet/standard/managed-execution-process#compiling-to-msil).Common Intermediate Language. Equivalent to IL, also equivalent to CIL. |
>  | NGen | Native Image Generator. |
>  | NYI | Not Yet Implemented. |
> ```

good idea, feel free to file a PR ðŸ™‚ "
"We could potentially do that, but I'm going to leave this as is. I think making `SharedWalkerState` a class would probably introduce more garbage than it would save.

---
In reply to: [289925758](https://github.com/dotnet/roslyn/pull/35850#discussion_r289925758) [](ancestors = 289925758)"
"```suggestion
                if (((DefType)fieldType).IsUnsafeValueType)
```

I'm a garbage programmer without IntelliSense. This line needs more LISP."
"The problem is that we only call `ExecutableMemoryAllocator::Initialize` when PAL initialization is passed a specific flag and we now query the reserved range, so potentially that could read garbage (though it won't since it's a static variable).
Do the compilers not elide constructor calls for simple zero-initing constructors of static variables?

I can change it to unconditionally initialize it, but only reserve the range when the flag is passed. Does that sound better?"
"This is invalid grammar, but the input is invalid grammar as well. I assume we can safely treat it as garbage in, garbage out?"
"I wouldn't.  The test was largely codifying an API (mis)usage and maintaining some corner-case compat.  As @github said while I was writing an equivalent statement: anyone who encounters this can do their own ""don't add null to this collection"".

The nullability annotations for the type say that `null` isn't supported in context, anyways (so we're really dealing with garbage-in, garbage-out; but we're now limiting how smelly the garbage is)."
"Just to comment on the quote below (and not the PR, which itself seems fine; someone else on the team may be better to review it than me):

> IMO the TypeScript team should seriously consider just using chokidar

FWIW it's unfortunately more complicated than that; `tsc`/`tsserver` make use of a number of different watching methods for different types of files and directories (which makes it difficult to consider switching to something else with different tradeoffs), and adding a third party watcher would introduce a dependency to the typescript package (currently, it has none).

Also of interest may be #43790 (and its linked issues), and that VS Code is actually removing all of its watchers in favor of `@parcel/watcher`."
@github draft of the struct layout idea that we talked about.  I'm not sure why `System.Threading.Thread` is being preserved.  Even if I add garbage fields they are kept for some reason.  (`Mono.MonoDomain` is different - garbage fields there got trimmed)
"I'm looking at the impl of `GetSegmentCount` and maybe I'm missing something but I don't understand how this can work for regions -

```
    static int GetSegmentCount(T seg_start)
    {
        int count = 0;
        while (seg_start)
        {
            // If we find this many segments, something is seriously wrong.
            if (count++ > 4096)
                break;

            seg_start = seg_start->next;
        }

        return count;
    }
```

for regions you can no longer enumerate all SOH segments by starting from gen2's start seg - you'd need to go into gen0/1's segments separately. also for regions it's not inconceivable at all that the count could be more than 4096. "
"This seems incredibly specific, what's the reasoning behind it?"
"Maybe.... this isn't incredibly common to do. All of our guidance for static files is to put it ahead of routing/mvc. This would only come up in a very specialized use case like this. "
Nit: But just have it be a regular (get / set) property on the base type. This pattern isn't incredibly useful and I've been changing it in other places I've come across it.
">it is context and scope.context.

That is exactly what i said.  If i saw: 

```c#
Task.Run(() => Foo(scope.Context.UserCancellationToken)).Wait(context.UserCancellationToken);
```

It would be equally confusing, and i would insist it be changed.  I'm nto sure why this is *at all* contentious Heejae.  We've seen how difficult it can be to get properly behaving Task/Async/Cancellation code over the lifetime of Roslyn.  Writing code that uses different paths like this to get at cancellation tokens and which mixes/matches that in a single-statement is just *confusing*.

At best, there are no problems with it.  At worst, tehre are problems with it, but people are very nervous to change it because it's unclear what the implications would be.

We should strive to make code as clear and sensible as possible.  it's good for PRs.  It's good for avoiding bugs.  And it's extremely good for maintaining things.  "
"I'm not sure how to structure this code otherwise. We can't return a `string_view`, since in the case where we're reading the file from disk, the view will be garbage when the stack gets popped. Right?"
"To raise another argument for these changes -- I believe that the `AndroidSupportedHostJitAbis` prop is also used to determine which `libzip.dll` files to build for Windows, which are required for these packages to be functional from a ""system installation"" standpoint.

Build time differences have been negligible in the results that I have seen locally, and also in a handful of pipeline PR builds while iterating on this. At best, our `build` step in the `xamarin-android-pr-pipeline` Jenkins job takes around 12 minutes and at worst it takes around 30. Here's a table of recent Jenkins PR build times: 

| Build Time | #2733 - Release | Others - Release | #2733 - Debug | Other - Debug |
| -------------|-----------------------|------------------|-------------------------|--------------------|
| Time 1     |    [build - (12 min in block)](https://jenkins.mono-project.com/view/Xamarin.Android/job/xamarin-android-pr-pipeline-release/100/flowGraphTable/)  |          [build - (10 min in block)](https://jenkins.mono-project.com/view/Xamarin.Android/job/xamarin-android-pr-pipeline-release/104/flowGraphTable/) |                   [build - (34 min in block)](https://jenkins.mono-project.com/view/Xamarin.Android/job/xamarin-android-pr-pipeline-debug/23/flowGraphTable/) |                [build - (28 min in block)](https://jenkins.mono-project.com/view/Xamarin.Android/job/xamarin-android-pr-pipeline-debug/19/flowGraphTable/)|
| Time 2  |       [build - (11 min in block)](https://jenkins.mono-project.com/view/Xamarin.Android/job/xamarin-android-pr-pipeline-release/103/flowGraphTable/) |              [build - (11 min in block)](https://jenkins.mono-project.com/view/Xamarin.Android/job/xamarin-android-pr-pipeline-release/108/flowGraphTable/) |                      |                |

Recent debug pipeline build results have been a bit harder to compare so far, and I am not sure if this is a result of differences in the pipeline as it is coming online, differences in hardware, or the fact that we're building more in debug by default? It seems that more recent [Debug pipeline PR builds](https://jenkins.mono-project.com/view/Xamarin.Android/job/xamarin-android-pr-pipeline-debug/) are again hovering around 10-12 minutes, so I am not entirely sure what caused the spike in the debug numbers above. In our ""best case build timings"" as shown above, the changes in this PR seem relatively negligible timing wise to me, especially compared to the time cost of our tests. 

I think further investment in breaking up the 2 hour test step so that various pieces can run on multiple machines simultaniously would offset any build time increases introduced by this PR. To that end, we will also be able to execute _more_ tests against PR build artifacts with these changes."
"Are you intentionally being incredibly condescending? It sure seems like it.

We get it. You and the fifty other people who asked for MRU without being quite as arrogant or prickish (save one; there was one other) have made your voices heard.

This will be factored into future planning discussions."
"@github, @github. Rust behavior is here: [full link to rust.godbolt](https://rust.godbolt.org/#z:OYLghAFBqd5QCxAYwPYBMCmBRdBLAF1QCcAaPECAMzwBtMA7AQwFtMQByARg9KtQYEAysib0QXACx8BBAKoBnTAAUAHpwAMvAFYTStJg1DEArgoKkl9ZATwDKjdAGFUtEywYgATAFZSjgBk8BkwAOXcAI0xiEAA2LlIAB1QFQjsGFzcPbz9k1NsBIJDwliiY%2BMtMawKGIQImYgJM909fSur0uoaCIrDI6LiE827m7Lbhxt6SssGASktUE2Jkdg5EkwiAaioGTbwAZi8AfSIjqkOIBncQPcPZzYBaAFJ97G3DzaeAdgAhJ40AIKbYGbK4sTZMBTvLz/AHfAAisNh6y2O1ux1OVFikku13R92er222M%2Bv1hINB7ghUKxkiRX0RgORG22uzw2JOqDOFzBN3ZkgJLze5y8pL%2BgIpYOp0PpjLhgJRrL2HMx2NxLD52MFRNpYvJIKlkOJdKZDKRCpZaJFnKOBy86puIu1bzteolBqpRrtsvNAMVVsONv5DuhzuVkjdQI94K92J9TItqN21tOJh5eKdjyFmzTou%2B4qjwMNUNz8flfstycDqbVvNDWaJJhJ%2Bf1Rc9JbjprlzKTxqD6Y1xrDrpb7rbMah3q7vv9yZVXODddpw%2BbZLHlIn4bLPaVtJtuZDy4bb1zkcl7Zzh23id386OTZxS61x5zq4L583D%2BvFd7uZtIpDXMwxFM9o2lEVv1nS8MS5WlALuF9dVHQsN3Azs4TNBMfyVB9/wHG4H2Aj5kI/cCr2nLCoNw1VHzxQjELfVtUKNWlZQ4eZaE4HxeE8DgtFIVBOAAJTMAhNgURZlkwT4vH2HhSAITR2PmABrEB9n2AA6DSdN0vTYn0ThJB4pSBM4XgFBADQFKU%2BY4FgJA0BYRI6GichKCclz6BiYguB8WJrJoWgCGiSyIAiUyImCBoAE9OHkpy2EEAB5BhaDivjeCwFhDGAcRMtIfBiEwGw8AAN0wSyCswVQSpMEL4t4YIQs4graDwCJiFilwsFMghiDwDVuHYvgDGABQADU8EwAB3ZLEkYRqZEEEQxHYKRlvkJQ1FM3QEgMIwQFMcx9A6yzIHmVBEhqKqHmSx4nE2epiGATACAeKhMCYAglkwF5ES8H4mDK1QvAsqoSpqBwGGcVwWhyfwYamfoYgATmkPI0gEUZPC4L4khSLGGGR0oBnR9pIc6Z6mjhsY/CsSmBC6SZgj6Um0ekCYaayXH8a5kmZnR%2BYJKWFY9H6zBVh4DiuJMgrBI4Y6CGQTYuE0/zNI0TYIBEk7NlwQgSBkuTSE2FxnNc4hja4WZeEUzLZlU9StL012dIM1rjNIXj%2BIViyrJsh3SHsxAQEWAh1gsCgIE8y29EwfAiBiBJ%2BBW0RxA21OtpUdQColqXeBmrrEkamWOG473TIV5L6sjzZUCoTYlZVtWNa1iBza86JrdtwOtEd0g1I07S3ddwyOC9n3eD9ywA/t/uy7Byv5fMvvlNICriFSexJCAA%3D%3D%3D)

Discussion of the overall issue + the benchmarks they did are here: <https://github.com/rust-lang/rust/issues/10184>

For simple cases like `f32_to_i32` we'd go, on Skylake, from ~7 cycles to between ~19 and ~24 cycles (a theoretical ""worst case"" taking 3.42x more time).

The measured impact was lower but still significant in some of the areas I called out. For example, there was a measured ""x0.82"" regression.

Notably, I'm not convinced what `rust` is doing here is the ""most efficient"" and looking through some of the issues/comments they believe so as well but are somewhat limited by what LLVM can trivially represent today.

We on the other hand, are not, and I believe the right thing here is to do replace the cast with the following in morph:
```csharp
var result = platform_f32_to_i32(value); // this is ""inline"", so just `vcvttss2si`

if (result == sentinel) // 0x80000000
{
    result = saturating_f32_to_i32(value); // this is a helper call to `CORINFO_HELPER_FLT2INT`
}
```

This should, given branch prediction, help avoid some of the slowdown that occurs in the normal case.

The rust code itself is always dealing with the predictor anyways due to `cmovnp` inserted to handle `nan` (possibly more for a couple of the generated patterns) and so it shouldn't be any slower than the rust code and will be on average smaller."
"Once R2R started working reliably on OSX, this scenario started failing on OSX/ARM64. I beleive OSX/ARM64 has special API for varargs, but emitted code does not adjust for that and `JIT_NewMDArr` reads garbage.

In JIT scenario we use a different helper. There is a comment somewhare that R2R should switch to that eventually to avoid variadic calls, which may have portability issues.
Perhaps it is time for that."
"It is not supported to get objects from a destroyed handle ðŸ˜„. When you destroy the handle it is returned to the pool of available handles and you would be potentially getting some random other object, or garbage.

If you wanted to verify that the object is indeed collected you could use a second weak handle and verify that it returns NULL. I don't think it's necessary to add that though."
"I don't get the worry here that multiple frameworks doesn't support multi item changes. This PR has nothing to do with this. `INotifyCollectionChanged` is the interface used for multi item changes and it already supports it and has since it was first designed. Any framework that listens to the event this interface declares could get multi item changes - including WPF - and WPF has never supported it but that doesn't mean the interface wasn't shipped. This PR doesn't create a new limitation in WPF.

Sure this PR makes it easier to raise multi item changes on a commonly used collection, but it doesn't in any way expose an issue that wasnâ€™t already there. It merely implements the existing interface to its fullest. Holding up this PR because WPF has a problem that it always had and weâ€™ve complained about for years, when this change can create huge performance gains for other UI frameworks is frankly incredibly disappointing. I guess weâ€™ll just have to continue with using our own observable collection implementations."
"This have made me think that this code might not be 100% correct, since I'm not 100% sure I should mock with the `_spaProcess` instance when running in the context of the finalizer. That said, I don't think that matters since this object is managed by the DI container and will always be disposed, and the worst case scenario is an that an exception is thrown and caught within the finalizer, which I think it's fine.

I'm not going to change it since that would require me to retest all the templates across OSs again."
This may be the most useful comment ever (seriously).
"> The 3.1 [submodule update](https://github.com/dotnet/aspnetcore/pull/39903) already got merged, will that be an issue?

Probably not. Worst case is the change will result in an official build that flows old versions into dotnet/sdk. Even that doesn't seem like a big deal. Agreed @githubâ”"
"note: in the external process we do like spinning them all off at once.  It's good core utilization.  Whereas in the VS process we don't like that as the amount of garbage it can produce can impede things.  In other words, the VS process is optimized for latency, the server process for throughput.  GC's really impact the former by causing spikes. "
Actually the right thing to do here is to just remove the whole `teardown` section and simply never set `container` to `null` and let the garbage collector do its thing.
"My figuring was that the read around the Sleep would not need any barriers or volatility because the Sleep would have to imply a barrier if necessary, and for the write the only reason for volatile is to make sure it happens after the clock read, which I don't think is actually necessary, but doesn't hurt at all in this case, and the write is immediately followed by a wait that would imply a barrier where necessary. Even in the worst-case situation, it seems like the order of operations demanded by the code would be ensured by cache consistency at least. Is there a theoretical case where the intended behavior would not work on ARM?"
"> Its a tad bit weird to use ""visual"" navigation when all you see is one pane, but presumably the person knows what their layout under the zoomed pane looks like.

Yeah, I'd agree with this assertion. Worst case, somebody can flail around for a couple seconds until they find the pane they wanted anyway -- the harm in choosing the wrong one is pretty small."
"expanded otu the linq version so we can preallocate the right sized arrays without garbage.  Also, because the tree info is on the tree itself, we avoid two full rounds of hashing + using the CWT from before."
"Look a little closer, @github.
This code path is hit only for non-ASCII chars, and it's hit only once, the first time we encounter such a char (there's a 'break' after the WriteBytes) to handle the rest of the string.

In fact, worst case (when the first char is non-ASCII), this code reduces to the original code because the original ""GetBytes(string s)"" call is implemented in the same way: A call to GetByteCount to allocate a byte[], then a call to GetBytes to fill the array. 
"
"I thought I left a comment, but looks like it was deleted or I forgot to submit it...
Yeah, in the worst case, `getTypeAtLocation` will return an `errorType` with no call signatures and the code action will fail to be reported (and won't delete code or anything of that nature)."
the new array+member approach feels like it creates unnecessary garbage in a path that may happen a lot.
"Worst case scenario using `CopyTo` for a small number of bytes can result in a 20-30% regression. I'm sure `CopyTo` already does unrolling of its own, but I would guess that both `Slice` and `CopyTo` waste a few CPU cycles in bounds checking etc. that makes it relatively inefficient for short byte sequences."
"> Would you be able to share the actual telemetry data and/or the source of this data?

We have data about where Azure cloud services spend time inside .NET. This telemetry is enabled only for code that is owned by Microsoft. We take privacy and confidentiality very seriously. Collecting this data for customer workloads would not be acceptable.

It is aggregate over thousands of individual cloud services written by hundreds of teams running on many thousands of machines. You can think about it as attaching profiler to every machine for random 5 minutes each day. I am not able to share the raw data, but I can share answers to questions like what is relative time spent in one API vs. other API.

Many of the performance improvements are motivated or influenced by insights we get from this data. For example, the recent batch of major regular expression improvements was largely motivated and influenced by this data.

> if the blocker is framework operations that use Sort internally I wouldn't be so lucky.

If you find these without a good workaround, we would like to know."
"> If we're going to do anything here (and I don't think it's a particularly high yield scenario) we should add additional logic to `getIntersectionType` that reduces intersections of array types and tuple types

This was my first thought as well, but I noticed that dealing with unreduced intersections of arrays and tuples works totally as expected, from an assignability perspective. Itâ€™s only during the normalization into another tuple (specifically this last condition where itâ€™s assumed that anything that reaches this branch can just become a rest element of the index type) that things go awryâ€”so while eagerly reducing more intersections would prevent that, it seemed like unnecessary work to me. And while I agree that this is a strange edge case, there are examples that are weird and wrong enough that I thought fixing it would be worthwhile if it didnâ€™t come at the cost of performance (which I need to check). But I guess my test cases didnâ€™t show the worst manifestations of the bug:

```ts
type Intersection = number[] & [""a"", 2, 3];
type Normalized = [...Intersection]; // (3 | 2)[]
```

We can reason that `Intersection` ought to be `never`, and although it does not immediately reduce, the only types that are assignable to it are `never` and 3-length tuples where at least the zeroth element is `never`, which are arguably as good as `never`. And of course, `Intersection` is assignable both to `number[]` and to `[""a"", 2, 3]`. I have no problem with `Intersection` in its non-reduced form.

But after spreading into another tuple, it becomes `(3 | 2)[]`. This breaks the identity that I _think_ should hold, that `[...T]` is equivalent to `T` for any array-like `T`, because `(3 | 2)[]` is not assignable to `[""a"", 2, 3]`. So Iâ€™m not sure I see why eager intersection reductions would be necessary when no assignability problems are observable until tuple normalization happens."
"> it now tightly couples the perf on one area to an implementation detail of another value and how it relates inheritance-wise to others

But perf *always* depends on implementation details of any external code and libraries you're using, because those implementation details affect the perf. In the next version of a library, the function you're using might be 2x slower (although realistically, that would usually be treated as a regression in corefx and it's unlikely to happen in the first place because they take perf of every area really seriously and have automated tests to detect any regressions). But still, when you're comparing 2 ways of doing something and you find that 1 of them is faster, in the next version, it could be the other one. So with perf, you're *always* relying on implementation details. That's why I tested this with multiple .NET versions.

Most importantly though, I'm not relying on implementation details with respect to correctness of behavior, only with respect to desirable perf outcomes.

> The cost is on maintainability. Anyone who has to maintain this now needs to hold all this complexity in their head and get things right on framework/core for any changes here in the future. That added complexity is more chance to get things wrong and to introduce subtle issues (like potential corruption).

I really don't see where the complexity or maintainability cost is. You're talking about this piece of code:
```c#
var @delegate = (TextWriterWriteReadOnlySpanDelegate)textWriter.Write;
if (@delegate.Method.DeclaringType != typeof(TextWriter))
{
    // Use the Write(ReadOnlySpan<char>) overload only if the method is overriden, because if it's not, the base
    // TextWriter implementation needs to delegate to the array overload and could potentially allocate a really
    // large array, which would be slower than the buffering base implementation.
    textWriter.Write(this.Source.AsSpan(span.Start, span.Length));
}
else
{
    base.Write(textWriter, span, cancellationToken);
}
```
That's literally just 1 condition - to check whether the `TextWriter` *supports* writing directly from a span, and 2 branches based on that, each do the same thing and both 1 line (or rather the second one calls an existing method). I really don't see where the cost is on maintainability. Both branches have the same behavior so if someone edits this someday and gets the condition wrong, it's not like the code will start behaving incorrectly or like there would be any corruption, as you're saying. It would just lose the perf benefit - that's literally it. The behavior would still be correct (and the tests also cover the behavior in both branches). And on top of that, this isn't the kind of method that has to evolve for new language features etc. This just copies a piece of data. This is the kind of code that is written once and usually stays the same because it doesn't need to evolve or change.

I'm kinda surprised by this. In dotnet/aspnetcore and especially dotnet/runtime, they routinely add all sorts of convoluted and hard to read code with many branches and different implementations to achieve perf benefits in any public API. This is really *nothing* compared to that. This is just one condition and 2 equally simple branches. I just don't see the complexity. I guess the recent perf enthusiasm in .NET hasn't reached Roslyn as much ðŸ˜”

> Specifically, these types are already complex. To the point that it makes implementing language features harder

StringText.cs is currently 103 lines (without this PR). And as I mentioned, this isn't really the kind of code that has to evolve for new language features. This just copies a string.

>  I don't believe we do this elsewhere.

That doesn't mean it's complex though. ðŸ˜³"
There is an almost 10% difference between the best and worst check times in compiler-with-unions. I'm going to run the perf tests again to see if that persists.
"> > We might need to make some changes to VB though - since we can't link GeneratedNameParser.vb to C# code. Perhaps the best would be if both of these parsers were moved to Microsoft.CodeAnalysis and written in C#.
> 
> Yea. Worst case we end up exposing something with MEF and link appropriately to each. It seems doable though, and I'm glad we have `GeneratedNameParser` to handle this

This would be my preferred approach. expose a service and export it for both languages."
"> There is one additional 'invisible' source of overhead that I'm not too sure how to remove: currently for tailcalls through indirection we end up creating two different delay load helper stubs in the R2R image on x64, one that gets it from caller instructions and one that gets it through register (when we realize later that we will do a fast tailcall). The problem is that we create the entry point very eagerly in crossgen2 and we don't really have a way of backing out. Do you have any ideas for this? Maybe some kind of ref-count mechanism in CG2?

`crossgen2` has very sophisticated dependency tracking that should garbage collect the unused delay-load helpers. @github Is there a reason why it is not kicking in?"
"> Just to make sure I understand, what are the expectations regarding the timeline of this PR getting merged?

I understand the ambition behind this proof of concept, but I don't think now is the right time. Things have been incredibly busy getting releases out the door this month as-is, and large changes have introduced issues (such as the issue fixed in #66176) that have been time-consuming to investigate and address while trying to hit release dates. We're not in a position to take in large changes like this unless there's a pressing need that justifies it and the acceptance criteria of the change are very clear.

@github / @github is there a pressing need here, or could this wait for reconsideration after Preview 4?"
@github please look into the incredibly high number of warnings in your internal build. What gives?
"seriously minor nit: consider ordering from ""widest to narrowest scope"".  so CompilationUnit, NamespaceDecl, TyepDecl. "
"This is the rejection sampling loop which produces a uniform distributiion in the interval [0, maxValue) with no bias whatsoever. Whereas the previous approach relied on floating point multiplication, and therefore I think probably exhibited some bias; certainly I would say that use of floating point arithmetic makes it harder to reason about whether it is biased or not.

In the worst case scenario maxValue is 1 above a power of 2 (2^n +1), and thus each loop has an approx. 50% chance of generating a value that can be returned. E.g. maxValue = 129, and bits = 8 giving 2^8=256.  

If you were to call this method a lot, say 100s of millions or billions of times (which is not unlikely/unreasonable) with such a worst case maxValue, then you can expect the following behaviour on average:

1 loop 50% of calls
2 loops 25% of calls
3 loops 12.5% of calls,
etc.

So roughly speaking: 
  ` percentage of calls = 1/(2^loops)`

Re-arranging to give:
   `loops = log2(1/p)`

So in a billion calls (for example) we can expect one of those calls (on average) to run for this many loops:
  `loops = log2(10^9) = 29`

Let's just sanity check that number. In a billion calls we can expect a 1 in a billion probability event to occur (on average, if we did lots of experiments, doing 1 billion calls in each experiment). One such '1 in a billion' event would be executing the loop 30 times (or thereabouts), with a 50% chance of succeeding on each loop!!

So 30 loops is a tiny fraction of overall executions, but not an uncommon occurrence (it could perhaps occur once every few seconds on a fast CPU calling this method heavily).

However, let's just say we put a hard limit of 100 loops (I pulled that number out of the air, but let's just run with that for now) after which we exit to prevent any chance of a bad PRNG causing an infinite loop (or lots more loops on average than we'd like). We probably shouldn't throw an exception, because it might occur on rare occasions - OK maybe at  a limit of 100 loops the chance is somewhat theoretical, but it doesn't feel like a good pattern/precedent I would say.

So the other option is to just pick a 'close enough' value in that very rare (maybe once in lifetime of the universe!) scenario. Well, we can take the sample we have that is too high, and divide by two (shift right), or we can zero out the high bit to give a sample within the required interval. These special case samples are not properly distributed, but they are so rare that the overall bias of Next(int) is vanishingly small.

Hence my point is that there is a way of putting in a limit to the number of loops as a safety check, and that doesn't break the overall contract of that method in any meaningful way. What would be a sane limit? well, greater than 30, as we have seen that that is a common occurrence. If we said a billion times less likely than the 30 loop case we get to 60 loops:

`log2(10^18) = 59.79 `to be precise.

I'm not suggesting this is the way forward, but thought some might find it a useful thought exercise :)





"
"Another _project maintenance note:_ We've been largely focused on fixes that we can service back to 1.12, for syncing fixes with the OS build of the Terminal. This seems like it would firmly fit outside the realm of servicable fixes, so things that build off this may have a much harder time getting serviced to 1.12. 

Looking through the 1.14 list, there's like, two categories where this might matter:
* Alt buffer stuff - I don't think we're planning on bringing that back to 1.12 anymore, so ezpz
* the z-order / foreground / showhide stuff. This is... complicated
  * Plumbing `FocusIn`/`FocusOut` from the Terminal to conpty via InteractDispatch: before this PR, it'll require a new congetset method. After this PR, the body of that congetset method could certainly just get moved into InteractDispatch directly. So not the worst to have separate versions of that.
  * Same with the Show/Hide stuff, for `ConhostInternalGetSet::ShowWindow`. 


Okay, not that complicated. Definitely doable, annoying, but doable if we want the FG stuff in 1.12. @github as fyi"
"Nit: Since `ISpanFormattable` is now public, this could expose uninitialized memory to the caller, as `StringBuilder` uses `GC.AllocateUninitializedArray` under the covers. In the worst case, this could allow me to write a buggy `ISpanFormattable` implementation as follows.

```cs
public class MyClass : ISpanFormattable
{
    public bool TryFormat(Span<char> destination, out int charsWritten, ...)
    {
        charsWritten = destination.Length;
        return true;
    }
}
```

If I then pass an instance of that type into the new `StringBuilder.Append` overloads, the resulting `StringBuilder` will contain data populated from uninitialized memory, even though the caller (me) never wrote unsafe code or used any unsafe-equivalent API.

Now, I don't think this is a problem in practice, as nobody would ever write an `ISpanFormattable` like that. But it's an example of where we're starting to play a little fast and loose with type \& memory safety, and we may need to start considering how this affects more common APIs like `StringBuilder` (and potentially `MemoryStream`, if we decide to make these same changes there). Are these patterns deserving of mention on MSDN? Is it even worth spending any time on this at all? Maybe in the extreme it's not much different than what we already have in the `Stream.Read[Async]` base implementations, which use the array pool, and I'm being overly paranoid."
"> if someone disables default framework references in modern SDK projects or removes the default references in classic framework projects, they already have to do extra work to be able to interact with the NuGet ecosystem.

Fair enough.  Worst case here someone does an upgrade and sees a build error.  It will be pretty clear what they need to do to get things working again.

> Of course we can explicitly annotate our P2Ps with PrivateAssets=""compile"" but how would someone on the team know when they have to do that?

We had previously talked about making this an optional package validation rule, but it's pretty hard to get it right.

> Does anything speak against bringing these back at a later point when we have more mature validation?

Bringing it back would be breaking, we probably wouldn't do that unless we found a serious problem with exposing implementation dependencies from our packages.  I didn't see anything in the diffs that was especially concerning here.  Our thinking has changed a lot since we first did this.  Initially we were trying to avoid exposing platform specific dependencies, or dependencies which had jagged support (Registry, Ref.Emit, OpenSSL, etc).  We were trying to manage the support/not-support by what developers could see in intellisense.  I think we have recently traded that approach for design time tooling via analyzers.

The only place I can see this being valuable is in hiding Microsoft.* OOB packages.  By hiding those from compile we can safely remove them from dependencies in compatible frameworks where they aren't needed.  
"
"> Total bytes on the wire reduced

This will be the biggest improvement. You can see the significant decrease in HEADERS frame size some of the unit tests where multiple calls are made. I'll get a byte break down from the benchmark.

I think the benchmarks so far mostly shows that the impact HPack encoding has on the server CPU usage is minimum in either direction. Best case it improves (and IMO this will probably be the typical case), and worst case where every header is different it is only slightly slower."
"Our convention is that `Update` is nondestructive too.  This would work either destructively or nondestructively.  It was originally nondestructive but then I noticed that the old node was always garbage after this was called (and that property is local to this class), so I made it destructive.

---
In reply to: [400510417](https://github.com/dotnet/roslyn/pull/42313#discussion_r400510417) [](ancestors = 400510417)"
"I am seeing this very reasonable for the worst case. "
"While I believe I see a CG1 vs. CG2 diff in NgenDump I tracked down to this difference, I have rolled back the 2nd commit based on your clarification that the error I've been attempting to fix is unrelated to my change. Amusingly this is about the 2nd or 3rd time the SPC CG2 switchover revealed a bug that ended up being completely unrelated to the switchover as such. Let's hope it just means that we do take CG2 testing seriously ;-)."
"Commit message:

```markdown
Bump to xamarin/java.interop/main@c942ab68 (#7157)

Fixes: https://github.com/xamarin/java.interop/issues/335
Fixes: https://github.com/xamarin/java.interop/issues/954
Fixes: https://github.com/xamarin/java.interop/issues/976
Fixes: https://github.com/xamarin/java.interop/issues/981
Fixes: https://github.com/xamarin/java.interop/issues/992

Changes: https://github.com/xamarin/java.interop/compare/7716ae53da79068e079b17dd4961d0c821bb2c34...c942ab683c12e88e0527ed584a13b411e005ea57

  * xamarin/java.interop@c942ab68: [java-source-utils] Build one `$(TargetFramework)` (#1007)
  * xamarin/java.interop@b7caa788: [Byecode] Hide `internal` Kotlin fields marked with `@JvmField` (#1004)
  * xamarin/java.interop@22d5687b: [generator] Add `@managedOverride` values `none` and `reabstract`. (#1000)
  * xamarin/java.interop@7f1d2d7a: [generator] Fix <remove-attr/> metadata (#999)
  * xamarin/java.interop@265ad762: [Java.Interop.Tools.JavaSource] Fix tag parsing errors (#997)
  * xamarin/java.interop@99c68a86: [Java.Interop] JniTypeSignature & CultureInfo, empty strings (#1002)
  * xamarin/java.interop@920ea648: [Java.Interop] optimize JniTypeManager.AssertSimpleReference() (#1001)
  * xamarin/java.interop@4b4fedd3: [generator] Process Javadoc for nested types (#996)

Ever since commit 2197a459, CI builds for the `main` branch have been
incredibly flakey: 918613d0 through 27647d2a all failed to complete
the **Mac** stage -- which *must* pass in order for most unit tests
to run -- then dbadf130 built, then f149c25c failed.

Nine commits in the past week have failed to adequately build.
(PR builds, meanwhile, were largely fine.)

Most of these failures are from `make all-tests`:

	_ExtractJavadocsFromJavaSourceJars:
	  /Users/runner/android-toolchain/jdk-11/bin/java -jar /Users/runner/work/1/s/xamarin-android/bin/Release/lib/xamarin.android/xbuild/Xamarin/Android/java-source-utils.jar @/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/tmp1cf0947e.tmp 
	JAVASOURCEUTILS : warning : Invalid or corrupt jarfile /Users/runner/work/1/s/xamarin-android/bin/Release/lib/xamarin.android/xbuild/Xamarin/Android/java-source-utils.jar [/Users/runner/work/1/s/xamarin-android/tests/CodeGen-Binding/Xamarin.Android.McwGen-Tests/Xamarin.Android.McwGen-Tests.csproj]
	â€¦
	BINDINGSGENERATOR : warning BG8600: Invalid XML file '/Users/runner/work/1/s/xamarin-android/tests/CodeGen-Binding/Xamarin.Android.McwGen-Tests/obj/Release/javadoc-xamarin-test-sources-F6F5170BF7A8BC71.xml': Could not find file ""/Users/runner/work/1/s/xamarin-android/tests/CodeGen-Binding/Xamarin.Android.McwGen-Tests/obj/Release/javadoc-xamarin-test-sources-F6F5170BF7A8BC71.xml"".  For details, see verbose output. [/Users/runner/work/1/s/xamarin-android/tests/CodeGen-Binding/Xamarin.Android.McwGen-Tests/Xamarin.Android.McwGen-Tests.csproj]
	  System.IO.FileNotFoundException: Could not find file ""/Users/runner/work/1/s/xamarin-android/tests/CodeGen-Binding/Xamarin.Android.McwGen-Tests/obj/Release/javadoc-xamarin-test-sources-F6F5170BF7A8BC71.xml""
	â€¦
	""/Users/runner/work/1/s/xamarin-android/Xamarin.Android-Tests.sln"" (default target) (1:2) ->
	""/Users/runner/work/1/s/xamarin-android/tests/CodeGen-Binding/Xamarin.Android.JcwGen-Tests/Xamarin.Android.JcwGen-Tests.csproj"" (default target) (12:6) ->
	(CoreCompile target) -> 
	  /Users/runner/work/1/s/xamarin-android/tests/CodeGen-Binding/Xamarin.Android.JcwGen-Tests/BindingTests.cs(78,37): error CS1061: 'DataEventArgs' does not contain a definition for 'FromNode' and no accessible extension method 'FromNode' accepting a first argument of type 'DataEventArgs' could be found (are you missing a using directive or an assembly reference?) [/Users/runner/work/1/s/xamarin-android/tests/CodeGen-Binding/Xamarin.Android.JcwGen-Tests/Xamarin.Android.JcwGen-Tests.csproj]
	  /Users/runner/work/1/s/xamarin-android/tests/CodeGen-Binding/Xamarin.Android.JcwGen-Tests/BindingTests.cs(79,40): error CS1061: 'DataEventArgs' does not contain a definition for 'FromChannel' and no accessible extension method 'FromChannel' accepting a first argument of type 'DataEventArgs' could be found (are you missing a using directive or an assembly reference?) [/Users/runner/work/1/s/xamarin-android/tests/CodeGen-Binding/Xamarin.Android.JcwGen-Tests/Xamarin.Android.JcwGen-Tests.csproj]
	  /Users/runner/work/1/s/xamarin-android/tests/CodeGen-Binding/Xamarin.Android.JcwGen-Tests/BindingTests.cs(80,40): error CS1061: 'DataEventArgs' does not contain a definition for 'PayloadType' and no accessible extension method 'PayloadType' accepting a first argument of type 'DataEventArgs' could be found (are you missing a using directive or an assembly reference?) [/Users/runner/work/1/s/xamarin-android/tests/CodeGen-Binding/Xamarin.Android.JcwGen-Tests/Xamarin.Android.JcwGen-Tests.csproj]
	  /Users/runner/work/1/s/xamarin-android/tests/CodeGen-Binding/Xamarin.Android.JcwGen-Tests/BindingTests.cs(81,28): error CS1061: 'DataEventArgs' does not contain a definition for 'Payload' and no accessible extension method 'Payload' accepting a first argument of type 'DataEventArgs' could be found (are you missing a using directive or an assembly reference?) [/Users/runner/work/1/s/xamarin-android/tests/CodeGen-Binding/Xamarin.Android.JcwGen-Tests/Xamarin.Android.JcwGen-Tests.csproj]
	  /Users/runner/work/1/s/xamarin-android/tests/CodeGen-Binding/Xamarin.Android.JcwGen-Tests/BindingTests.cs(82,29): error CS1061: 'DataEventArgs' does not contain a definition for 'Payload' and no accessible extension method 'Payload' accepting a first argument of type 'DataEventArgs' could be found (are you missing a using directive or an assembly reference?) [/Users/runner/work/1/s/xamarin-android/tests/CodeGen-Binding/Xamarin.Android.JcwGen-Tests/Xamarin.Android.JcwGen-Tests.csproj]
	  /Users/runner/work/1/s/xamarin-android/tests/CodeGen-Binding/Xamarin.Android.JcwGen-Tests/BindingTests.cs(84,51): error CS1061: 'DataEventArgs' does not contain a definition for 'Payload' and no accessible extension method 'Payload' accepting a first argument of type 'DataEventArgs' could be found (are you missing a using directive or an assembly reference?) [/Users/runner/work/1/s/xamarin-android/tests/CodeGen-Binding/Xamarin.Android.JcwGen-Tests/Xamarin.Android.JcwGen-Tests.csproj]
	  /Users/runner/work/1/s/xamarin-android/tests/CodeGen-Binding/Xamarin.Android.JcwGen-Tests/BindingTests.cs(85,10): error CS1061: 'DataEventArgs' does not contain a definition for 'Payload' and no accessible extension method 'Payload' accepting a first argument of type 'DataEventArgs' could be found (are you missing a using directive or an assembly reference?) [/Users/runner/work/1/s/xamarin-android/tests/CodeGen-Binding/Xamarin.Android.JcwGen-Tests/Xamarin.Android.JcwGen-Tests.csproj]

`java-source-utils.jar` is (apparently) corrupt, so
`javadoc-xamarin-test-sources-F6F5170BF7A8BC71.xml` is never created,
which means no parameter name information is present, which means the
generated `DataEventArgs` type doesn't have the appropriate property
names, as the property names come from parameter names, and since we
don't have parameter names we instead have property names like `P0`,
`P1`, `P2`, etc.

Why, then, is `java-source-utils.jar` corrupt?  The current guess is
that it is being built *concurrently*; from a
`msbuild-20220706T175333-leeroy-all.binlog` build log file from one
of the offending builds, we see:

	17:53:44.526 59:11>Target ""_BuildJava: (TargetId:490)"" in file ""/Users/runner/work/1/s/xamarin-android/external/Java.Interop/tools/java-source-utils/java-source-utils.targets"" from project ""/Users/runner/work/1/s/xamarin-android/external/Java.Interop/tools/java-source-utils/java-source-utils.csproj"" (entry point):
	17:53:44.526 59:11>Building target ""_BuildJava"" completely.
	  Output file ""/Users/runner/work/1/s/xamarin-android/bin/Release/lib/packs/Microsoft.Android.Sdk.Darwin/33.0.0/tools/java-source-utils.jar"" does not exist.
	â€¦
	17:54:19.564 59:12>Target ""DispatchToInnerBuilds: (TargetId:1637)"" in file ""/Users/runner/work/1/s/xamarin-android/bin/Release/dotnet/sdk/7.0.100-preview.7.22354.2/Microsoft.Common.CrossTargeting.targets"" from project ""/Users/runner/work/1/s/xamarin-android/external/Java.Interop/tools/java-source-utils/java-source-utils.csproj"" (target ""Build"" depends on it):
	  Task ""MSBuild"" (TaskId:1099)
	    Task Parameter:BuildInParallel=True (TaskId:1099)
	    Task Parameter:Targets=Build (TaskId:1099)
	    Task Parameter:
	        Projects=
	            java-source-utils.csproj
	                    AdditionalProperties=TargetFramework=net7.0 (TaskId:1099)
	    Additional Properties for project ""java-source-utils.csproj"": (TaskId:1099)
	      TargetFramework=net7.0 (TaskId:1099)
	â€¦
	17:54:19.592 59:12>Target ""_BuildJava: (TargetId:1640)"" in file ""/Users/runner/work/1/s/xamarin-android/external/Java.Interop/tools/java-source-utils/java-source-utils.targets"" from project ""/Users/runner/work/1/s/xamarin-android/external/Java.Interop/tools/java-source-utils/java-source-utils.csproj"" (entry point):
	  Building target ""_BuildJava"" completely.
	  Output file ""/Users/runner/work/1/s/xamarin-android/external/Java.Interop/../../bin/Release/lib/xamarin.android/xbuild/Xamarin/Android/java-source-utils.jar"" does not exist.
	â€¦
	17:54:41.034 59:11>Done building target ""_BuildJava"" in project ""java-source-utils.csproj"".: (TargetId:490)
 
What appears to be happening -- and there is lots of conjecture here,
as the build log is enormous and it's difficult to piece everything
together -- is that `java-source-utils.csproj` is built *twice*:
Once via `Xamarin.Android.sln`, and once via `apksigner.csproj`, as
it appears that setting `%(PackageReference.AdditionalProperties)`
triggers a *nested build*; note the `DispatchToInnerBuilds` target
which invokes the `<MSBuild/>` Task with the specified
`AdditionalProperties`.

However, that's not the only potential source of concurrent builds;
`java-source-utils.csproj` used `$(TargetFrameworks)` (plural), which
can *also* result in concurrent builds between the ""outer"" and ""inner""
builds used as part of `$(TargetFrameworks)`.

Fix this problem by bumping to xamarin/java.interop@c942ab68, which
updates `java-source-utils.csproj` to use the singular
`$(TargetFramework)` property -- avoiding ""outer"" and ""inner"" build
problems -- and ""for good measure"" update `apksigner.csproj` to
remove `%(ProjectReference.AdditionalProperties)` on
`java-source-utils.csproj`.  This should ensure that
`java-source-utils.csproj` is only built *once*, which in turn should
ensure that `java-source-utils.jar` isn't corrupted, and our CI
builds become more reliable.
```"
"seriously, i tried so hard not to p off the spell bot"
Do we care that hash won't be disposed in the (rare) case of CryptoPool.Rent throwing?  Presumably the worst case is it just gets finalized?
"Good day! I hope you all and your families are safe? The Covid-19 situation is really getting worst all over the globe, we all pray to almighty god to strength us, so that we can stay alive to testify, I brought good news that everyone can benefit from and also help the privilege, if you are a good kindest person, that also want to make the world a better place to live, now the freemasonry society are selecting the kindest people that are interested to join the society, and the good part is that freemasonry society they don't involved their self into evil things, they are not like the illuminati society, freemasonry society believe in god, they are grouped of people that come together to evaluate people and others original condition, once you become a member of the society they have sitting amount of benefit that we be given to you as the new member, and you should use 10% of the benefit to assist the privilege, and summit the credential to the lordship, I urge you if you have interest, find the official contact address below, you can be one of the lucky random, everyone has inside them a piece of good news, the good news is you donâ€™t know how great you can be! How much you can love! What you can accomplish! And what your potential is, stay strong, stand up, have a voice.

WhatsApp; +31 687329133

Mail Address; info.masonic.oregon@gmail.com

Website Address; https://masonicoregon.wordpress.com"
"Looking through the history, I made a significant change here https://github.com/dotnet/runtime/commit/75206ed0853b39501ae13219be6f55bfa7096c49#diff-1ea18ff65faa2ae6fed570b83747086d0317f5e4bc325064f6c14319a9c4ff67R142 and @github made a change here: https://github.com/dotnet/runtime/commit/0416c3469b7bccff087b89db3d9967dfd36070a3#diff-1ea18ff65faa2ae6fed570b83747086d0317f5e4bc325064f6c14319a9c4ff67R143.

I think we're OK bumping this.  MSBuild doesn't change the assembly version in their packages, so by updating our references the worst we do is expose MSBuild API to consuming projects that may not be present in a version of MSBuild they wish to run in.

In the future it might make sense to add a DARC subscription from MSBuild -> runtime for test purposes that we use in test projects for tasks."
"I would ask for your apologies for the rant youâ€™re about to read, but after 6 years and all the patience I can muster, I believe Iâ€™ve earned it.

The original issue and PR for this bug is about to celebrate itâ€™s SEVENTH BIRTHDAY. Over that time Skye and I have expended **so much energy** trying to get this over the finish line. It is incredibly disappointing that there are so many at Microsoft *continuing* to get in the way of getting this done.

.NET Core was sold to us developers as a way to break from the past, and major releases were going to be places where breaking changes could be made. Yet for 6 years Iâ€™ve continued to hear excuse after excuse. To quote Yoda, â€œalways with you it cannot be done.â€

Had this been fixed with .NET Core 2 when I submitted the first PR, MAUI and the other platforms would have dealt with it when they were being built, and WPF would have been fixed when it was being migrated. But it was punted to .NET Core 3.

Then, after the PR was already accepted and integrated, I was told WPF was already late and had too many bugs to be able to get the work done in time for the .NET Core 3 release. Our work was held up because the WPF team over-promised and couldnâ€™t get its act together. Victory was ripped away from us at the last possible second.

In the time since, instead of guiding people building new frameworks like MAUI towards making decisions that would allow this work to get done more easily, Microsoft finds time to invest in things like â€œMinimal APIsâ€ that only marginally impact developers.

So youâ€™ll have to forgive me if, after missing it in .NET Core 2, 3, 5, 6, AND 7, that maybe I donâ€™t believe weâ€™ll see it in .NET 8.

Itâ€™s cool though. Maybe weâ€™ll see this get fixed by the time .NET 12 rolls around. In the meantime, Iâ€™ll just continue to wonder how anyone EVER let WPF get past a code review with exceptions being thrown in event handlers, and why it seems like the same people are being allowed to hold back amazing frameworks like Blazor from taking advantage of these features.

@github thank you so much for your efforts. I can only imagine the emotional toll that having your status as â€œ.NET contributorâ€ ripped away year after year after effing year has had on you. It sucks having so many people stand in the way of something so obviously broken."
"If I read this correctly, this would copy the KeyID from parent certificate, right? 
But that does not seems right as the format seems to be slightly different. 
(openssl x509 -text shows garbage with the simple copy) "
"<insert appropriate meme here for @github asking crazy questions>

More seriously, when generators are ran the IDE needs to compare results from one run to the next. I naively used == to compare the instances, so..."
thios feels incredibly confusing.  why would we need to know things like isGeneratedRazorDocument.  Having Razor leak into the diagnostic engine *in any way* feels... really really bad.  @github ?
"But srsly, why have a garbage path? 
"
"> Are there scenarios where somebody might subclass these types and set MetadataToken to a garbage value? 

For these, no, since since they are sealed:
```
AssemblyBuilder
ConstructorBuilder
FieldBuilder
GenericTypeParameterBuilder
MethodBuilder
ModuleBuilder
TypeBuilder
```

For abstract `MemberInfo` and related non-sealed types that have the virtual `MetadataToken` property yes a garbage value could be returned. But that was also possible previously."
"A logger of errors from unit tests needs to be able to successfully log total garbage, or at least recover gracefully ðŸ˜„ 

In a quick look through I don't see any protection in the HtmlLogger against garbage, and no tests verifying garbage is accepted by its event handlers:
https://github.com/microsoft/vstest/blob/ad32654bc155a059fda263f0ec99b445058d84ea/test/Microsoft.TestPlatform.Extensions.HtmlLogger.UnitTests/HtmlLoggerTests.cs"
"> I'm still trying to understand all the possible states the header table can be in. A quick writeup here would probably be useful.

The base collection is the abstract `HttpHeaders` type. It contains an `object _headerStore` field.
The `_headerStore` can be either:
- `null` for an empty header collection before any headers are added.
- A `HeaderEntry[]` where `HeaderEntry` is essentially a `KeyValuePair<HeaderDescriptor, object>`[^1].
- A `Dictionary<HeaderDescriptor, object>` once too many entries are added to the array (currently `ArrayThreshold = 64`)[^2].

---

A `HeaderDescriptor` is the name of the header, wrapping either a `KnownHeader` or a `string`, where `KnownHeader` allows us to cache information for common headers. After #62847, `HeaderDescriptor` stores the `KnownHeader`/`string` in a single `object` field.

The `object` we store the header value in is either:
- A `string` when a single raw value is added via `TryAddWithoutValidation`.
- A `HeaderStoreItemInfo` when multiple values are added for the same header name, or when the value is parsed.

---

`HeaderStoreItemInfo` contains 3 fields for `Raw`, `Parsed` and `Invalid` values.
Each of these fields is an `object` that can be either:
- A single `T` storing a single value.
- A `List<T>` storing multiple values.

where `T` is `string` for `Raw` and `Invalid`, and `object` for `Parsed`. Parsed values can be various types that are boxed in an `object` when stored.

---

Some operations around headers are lazy:
- If a value is added via `TryAddWithoutValidation`, we will defer parsing until the value is accessed in a validating manner (strongly typed properties, `TryGetValues`, enumeration). At that point, we will promote the value from a `string` to a `HeaderStoreItemInfo` and parse any values contained in its `RawValue` field.

This has observable side effects where the header representation on the wire (or during `NonValidated` enumeration) may change depending on whether and how the collection was accessed/enumerated.

[^1]: We use a dedicated `HeaderEntry` struct over a `KeyValuePair<HeaderDescriptor, object` because we need `ref` access to the `Value` field for efficiency. Plus `KeyValuePair` is annoying to work with :)
We use an array instead of `List<HeaderEntry>` to avoid the allocation for the `List` object, limitations around accessing values by `ref`, and overhead for the indirection during enumeration/adds.

[^2]: We use the hybrid array/dictionary approach to get the low memory usage for the common case while capping the worst-case O(n^2) performance of operations over the array (see worst-case performance data in https://github.com/dotnet/runtime/pull/62981#issuecomment-997234416)."
"It must be noted that this change introduces a slight performance regression, specifically when it comes to serializing small root-level `object` values:

### [System.Text.Json.Serialization.Tests.WriteJson&lt;Int32&gt;.SerializeObjectProperty](https://github.com/dotnet/performance/blob/945eb2a8ae39f3692885e1029d0cda9d3d249954/src/benchmarks/micro/libraries/System.Text.Json/Serializer/WriteJson.cs#L69)

|                  Method |        Job |                                                                                                         Toolchain |     Mean |   Error |   StdDev |   Median |      Min |      Max | Ratio | MannWhitney(3%) | RatioSD |  Gen 0 | Allocated | Alloc Ratio |
|------------------------ |----------- |------------------------------------------------------------------------------------------------------------------ |---------:|--------:|---------:|---------:|---------:|---------:|------:|---------------- |--------:|-------:|----------:|------------:|
| SerializeObjectProperty | Job-LSKJVH |        main | 235.7 ns | 5.44 ns |  5.82 ns | 236.1 ns | 226.0 ns | 246.8 ns |  1.00 |            Base |    0.00 | 0.0254 |     264 B |        1.00 |
| SerializeObjectProperty | Job-KLBYHL | PR | 249.3 ns | 8.82 ns | 10.16 ns | 249.8 ns | 231.8 ns | 270.1 ns |  1.06 |          Slower |    0.04 | 0.0260 |     264 B |        1.00 |

Fundamentally, it is caused by this change removing the hardcoded polymorphic serialization for root-level `object` values. Instead of directly resolving the metadata for the runtime type, the metadata for `object` is consulted _before_ proceeding with polymorphic serialization. As such, in the worst-case scenario, two lookups need to be made against the metadata cache instead of just one.

I've been able to recover some of the performance losses by [employing a secondary LRU cache](https://github.com/dotnet/runtime/pull/72789/commits/80a1fa9bd484bc2276d131e49a5b1ea239e2df54#diff-e9ac53ab9e93a974203023f17f11923b1a4c4e8cf38c28ae0bbee95a030a0620R25) specifically used for root-level polymorphic types. I think we should merge this PR and accept the performance regression, as it is essential for getting https://github.com/dotnet/runtime/issues/72187 and https://github.com/dotnet/runtime/issues/72681 fixed. The performance regression only impacts small, root-level polymorphic values and does not affect the statically typed APIs or the source generator which are more performance-minded.

cc @github @github @github"
"Hmm... yes, the api is the worst. (but it does ... work... blech.) It represents the hierarchy of parsing the signature, and it actually allows for substitution inside of the signature without breaking stuff (which I must say is somewhat miraculous).

However, the goal was to require the modopt for these to be at the very beginning of the modifier stream before any non mod opts. Fortunately, that will result in the index following a consistent pattern. I need to do some research to understand the actual pattern you need to actually work with. I hope I can get back to you today."
This should be stronger than an Assert. Since we're guarding against Windows giving us garbage we should guard against this being invalid as well. We should take min of numBytes and buffer.Length below.
"```suggestion
        _fontFaceMap.emplace(_ToMapKey(weight, style, stretch), std::move(fontFace));
```

hmm.. these functions are INCREDIBLY SIMILAR."
"A good test would be calling GetTypeInfo on an expression on the RHS that is not-null in the when-not-null case and null in the worst case, and making sure that the type's state is null."
"In ecma - yes, publishing a reference can move ahead of field assignments.

I think it is technically allowed to publish complete garbage, as long as it is followed by patching it with real data - from a single threaded point of view noone would see a difference anyways."
"> Worst case we end up exposing something with MEF and link appropriately to each

Good idea, I think that would work fine."
@github I'm ok with making any experimentation if you can see large perf potential benefit. Make sure to write some reasonable benchmarks covering all common scenarios. Technically such TextWriter doesn't really need any changes to the current code as you can technically use it in your own code so perhaps even if this finds useful for very narrow scenarios it might be more of a blog post rather than change and people mind find it useful. I'd suggest start with opening an issue explaining more the idea and if you got some time for experimentation go ahead and share your code and benchmarks used (ideally open an issue once you got some results). Worst case scenario we will not find any benefit and not change anything and close the issue.
"An update: I tested this change with other benchmarks in the lab (plaintext, json, fortunes) and, as in the websocket case, observed no throughput or latency impact when running them.

To summarize where we are, we've seen some allocation reduction with this change (based on the data above), but no signs of any other impact--positive or negative--on the benchmarks that have been run. We'll get more coverage once this is merged and gets picked up by all the automated benchmarking runs. If we see any regressions at that point, we could back out the change. How does that sound?


## Raw Data

### Plaintext 
_Note: P90 latency of this one is highly variable from run to run (I've seen 22 ms in a ""before"" run...), so the delta here shouldn't be taken seriously._

Before:
```
| application           |                            |
| --------------------- | -------------------------- |
| CPU Usage (%)         | 94                         |
| Cores usage (%)       | 2,642                      |
| Working Set (MB)      | 391                        |
| Private Memory (MB)   | 808                        |
| Build Time (ms)       | 3,000                      |
| Start Time (ms)       | 337                        |
| Published Size (KB)   | 109,320                    |
| .NET Core SDK Version | 6.0.100-preview.6.21276.12 |
| ASP.NET Core Version  | 6.0.0-rc.1.21413.5+fad9658 |
| .NET Runtime Version  | 6.0.0-rc.1.21413.9+60d9b98 |


| load                   |            |
| ---------------------- | ---------- |
| CPU Usage (%)          | 68         |
| Cores usage (%)        | 1,893      |
| Working Set (MB)       | 48         |
| Private Memory (MB)    | 376        |
| Start Time (ms)        | 0          |
| First Request (ms)     | 233        |
| Requests/sec           | 2,663,433  |
| Requests               | 40,215,905 |
| Mean latency (ms)      | 1.68       |
| Max latency (ms)       | 166.39     |
| Bad responses          | 0          |
| Socket errors          | 0          |
| Read throughput (MB/s) | 335.29     |
| Latency 50th (ms)      | 0.88       |
| Latency 75th (ms)      | 1.28       |
| Latency 90th (ms)      | 1.76       |
| Latency 99th (ms)      | 0.00       |

```

After:
```
| application           |                            |
| --------------------- | -------------------------- |
| CPU Usage (%)         | 94                         |
| Cores usage (%)       | 2,641                      |
| Working Set (MB)      | 387                        |
| Private Memory (MB)   | 804                        |
| Build Time (ms)       | 3,022                      |
| Start Time (ms)       | 353                        |
| Published Size (KB)   | 109,320                    |
| .NET Core SDK Version | 6.0.100-preview.6.21276.12 |
| ASP.NET Core Version  | 6.0.0-rc.1.21413.5+fad9658 |
| .NET Runtime Version  | 6.0.0-rc.1.21413.9+60d9b98 |


| load                   |            |
| ---------------------- | ---------- |
| CPU Usage (%)          | 67         |
| Cores usage (%)        | 1,882      |
| Working Set (MB)       | 49         |
| Private Memory (MB)    | 376        |
| Start Time (ms)        | 0          |
| First Request (ms)     | 266        |
| Requests/sec           | 2,709,348  |
| Requests               | 40,904,982 |
| Mean latency (ms)      | 1.73       |
| Max latency (ms)       | 214.63     |
| Bad responses          | 0          |
| Socket errors          | 0          |
| Read throughput (MB/s) | 341.07     |
| Latency 50th (ms)      | 0.89       |
| Latency 75th (ms)      | 1.29       |
| Latency 90th (ms)      | 2.64       |
| Latency 99th (ms)      | 0.00       |
```

### JSON
Before:
```
| application           |                            |
| --------------------- | -------------------------- |
| CPU Usage (%)         | 82                         |
| Cores usage (%)       | 2,296                      |
| Working Set (MB)      | 361                        |
| Private Memory (MB)   | 443                        |
| Build Time (ms)       | 5,835                      |
| Start Time (ms)       | 296                        |
| Published Size (KB)   | 109,320                    |
| .NET Core SDK Version | 6.0.100-preview.6.21276.12 |
| ASP.NET Core Version  | 6.0.0-rc.1.21413.5+fad9658 |
| .NET Runtime Version  | 6.0.0-rc.1.21413.9+60d9b98 |


| load                   |            |
| ---------------------- | ---------- |
| CPU Usage (%)          | 52         |
| Cores usage (%)        | 1,458      |
| Working Set (MB)       | 37         |
| Private Memory (MB)    | 358        |
| Start Time (ms)        | 0          |
| First Request (ms)     | 113        |
| Requests/sec           | 748,260    |
| Requests               | 11,298,306 |
| Mean latency (ms)      | 0.67       |
| Max latency (ms)       | 192.01     |
| Bad responses          | 0          |
| Socket errors          | 0          |
| Read throughput (MB/s) | 108.47     |
| Latency 50th (ms)      | 0.23       |
| Latency 75th (ms)      | 0.48       |
| Latency 90th (ms)      | 1.15       |
| Latency 99th (ms)      | 6.61       |
```

After:
```
| application           |                            |
| --------------------- | -------------------------- |
| CPU Usage (%)         | 79                         |
| Cores usage (%)       | 2,225                      |
| Working Set (MB)      | 362                        |
| Private Memory (MB)   | 782                        |
| Build Time (ms)       | 9,493                      |
| Start Time (ms)       | 308                        |
| Published Size (KB)   | 109,321                    |
| .NET Core SDK Version | 6.0.100-preview.6.21276.12 |
| ASP.NET Core Version  | 6.0.0-rc.1.21413.9+4076bad |
| .NET Runtime Version  | 6.0.0-rc.1.21413.9+60d9b98 |


| load                   |            |
| ---------------------- | ---------- |
| CPU Usage (%)          | 52         |
| Cores usage (%)        | 1,454      |
| Working Set (MB)       | 37         |
| Private Memory (MB)    | 358        |
| Start Time (ms)        | 0          |
| First Request (ms)     | 141        |
| Requests/sec           | 740,802    |
| Requests               | 11,185,649 |
| Mean latency (ms)      | 0.66       |
| Max latency (ms)       | 112.99     |
| Bad responses          | 0          |
| Socket errors          | 0          |
| Read throughput (MB/s) | 107.39     |
| Latency 50th (ms)      | 0.23       |
| Latency 75th (ms)      | 0.48       |
| Latency 90th (ms)      | 1.00       |
| Latency 99th (ms)      | 7.14       |
```


### Fortunes
Before:
```
| db                  |         |
| ------------------- | ------- |
| CPU Usage (%)       | 25      |
| Cores usage (%)     | 711     |
| Working Set (MB)    | 47      |
| Build Time (ms)     | 1,943   |
| Start Time (ms)     | 474     |
| Published Size (KB) | 914,909 |


| application           |                            |
| --------------------- | -------------------------- |
| CPU Usage (%)         | 75                         |
| Cores usage (%)       | 2,114                      |
| Working Set (MB)      | 409                        |
| Private Memory (MB)   | 824                        |
| Build Time (ms)       | 2,876                      |
| Start Time (ms)       | 1,850                      |
| Published Size (KB)   | 93,209                     |
| .NET Core SDK Version | 6.0.100-preview.6.21276.12 |
| ASP.NET Core Version  | 6.0.0-rc.1.21416.2+98ee247 |
| .NET Runtime Version  | 6.0.0-rc.1.21415.6+fde6b37 |


| load                   |           |
| ---------------------- | --------- |
| CPU Usage (%)          | 31        |
| Cores usage (%)        | 861       |
| Working Set (MB)       | 37        |
| Private Memory (MB)    | 363       |
| Start Time (ms)        | 0         |
| First Request (ms)     | 100       |
| Requests/sec           | 327,682   |
| Requests               | 4,943,368 |
| Mean latency (ms)      | 2.04      |
| Max latency (ms)       | 102.00    |
| Bad responses          | 0         |
| Socket errors          | 0         |
| Read throughput (MB/s) | 425.31    |
| Latency 50th (ms)      | 1.37      |
| Latency 75th (ms)      | 1.92      |
| Latency 90th (ms)      | 3.50      |
| Latency 99th (ms)      | 13.52     |
```

After:
```
| db                  |         |
| ------------------- | ------- |
| CPU Usage (%)       | 25      |
| Cores usage (%)     | 713     |
| Working Set (MB)    | 47      |
| Build Time (ms)     | 2,020   |
| Start Time (ms)     | 475     |
| Published Size (KB) | 914,909 |


| application           |                            |
| --------------------- | -------------------------- |
| CPU Usage (%)         | 74                         |
| Cores usage (%)       | 2,074                      |
| Working Set (MB)      | 409                        |
| Private Memory (MB)   | 824                        |
| Build Time (ms)       | 2,676                      |
| Start Time (ms)       | 1,903                      |
| Published Size (KB)   | 93,209                     |
| .NET Core SDK Version | 6.0.100-preview.6.21276.12 |
| ASP.NET Core Version  | 6.0.0-rc.1.21416.2+98ee247 |
| .NET Runtime Version  | 6.0.0-rc.1.21415.6+fde6b37 |


| load                   |           |
| ---------------------- | --------- |
| CPU Usage (%)          | 31        |
| Cores usage (%)        | 863       |
| Working Set (MB)       | 38        |
| Private Memory (MB)    | 363       |
| Start Time (ms)        | 0         |
| First Request (ms)     | 99        |
| Requests/sec           | 328,971   |
| Requests               | 4,967,324 |
| Mean latency (ms)      | 2.07      |
| Max latency (ms)       | 95.46     |
| Bad responses          | 0         |
| Socket errors          | 0         |
| Read throughput (MB/s) | 426.99    |
| Latency 50th (ms)      | 1.36      |
| Latency 75th (ms)      | 1.90      |
| Latency 90th (ms)      | 3.50      |
| Latency 99th (ms)      | 13.70     |
```"
wait... do we seriously have no VB tests for thsi feature?  @github ??
"> It passed the tests, but due to my experiment on HasTopLevelNullabilityImplicitConversion, I now think this alternative is likely incorrect. More on that in next comment.

Yes, the proposed alternative fails to distinguish T? from T. When T is replaced with a non-nullable type, we are dealing with types of different nullability. The ValueCanBeNull helper assumes the ""worst"" case and ignores the fact that T could actually be not nullable. These functions should consider both scenarios.   


---
In reply to: [231760271](https://github.com/dotnet/roslyn/pull/30913#discussion_r231760271) [](ancestors = 231760271)"
"Couple questions/comments:

1. Can we add the source property to the notification:actionExecuted event?  Would be useful in determining click-through/cancel rates for notifications by extension.
2. I would prefer the source always be filled, with some indicator that a notification is coming from VS Code Core.  ""vscode"" or ""core"" both make sense to me.  I believe this increases long-term readability.
3. Would really like to capture if a user removes a notification using Esc - is this possible with the current setup?
4. I like notification events separate from workbenchActionExecuted - the latter already covers an incredibly wide range of functionality within the product."
"There's some documentation about the log environment variables here https://www.mono-project.com/docs/advanced/runtime/logging-runtime-events/, it's not full however. It would be good to mention the relevant bits here. Mono manpage has more information:

```
You can use the MONO_LOG_LEVEL and MONO_LOG_MASK environment variables to get verbose debugging output about the execution of your application within Mono.                                                                                                           

       The  MONO_LOG_LEVEL  environment  variable  if  set, the logging level is changed to the set value. Possible values are ""error"", ""critical"", ""warning"", ""message"", ""info"", ""debug"". The default value is ""error"". Messages with a logging level greater then or
       equal to the log level will be printed to stdout/stderr.                                                                                                                                                                                                              

       Use ""info"" to track the dynamic loading of assemblies.
                                                                                                                                                                                                                                                                             
       Use the MONO_LOG_MASK environment variable to limit the extent of the messages you get: If set, the log mask is changed to the set value. Possible values are ""asm"" (assembly loader), ""type"", ""dll"" (native library loader), ""gc"" (garbage  collector),  ""cfg""
       (config  file  loader),  ""aot"" (precompiler), ""security"" (e.g. Moonlight CoreCLR support), ""threadpool"" (thread pool generic), ""io-selector"" (async socket operations), ""io-layer"" (I/O layer - processes, files, sockets, events, semaphores, mutexes and hanâ€
       dles), ""io-layer-process"", ""io-layer-file"", ""io-layer-socket"", ""io-layer-event"", ""io-layer-semaphore"", ""io-layer-mutex"", ""io-layer-handle"" and ""all"".  The default value is ""all"". Changing the mask value allows you to display only messages  for  a  certain
       component. You can use multiple masks by comma separating them. For example to see config file messages and assembly loader messages set you mask to ""asm,cfg"".
```"
"> that somehow makes no difference

Interesting.  Does it just end up returning some garbage register value or something?"
"This should be changed to slice the char span to the exact used length,  and then not also pass in a count.  Otherwise,  you might be looking at garbage in the helper call,  e.g. with the LastIndexOf."
I'd rather inline this function than generate garbage for each call.
"> We should remove (or at least seriously consider removing) the special cases for Razor as a prerequisite pull request

@github This work is out of scope of this PR, and I don't believe it complicates this change - we just pass an additional flag to method that tries to fetch cached document diagnostics."
"ok, new idea. We add a [`ObjectHandleOnStack`](https://github.com/dotnet/runtime/blob/128d3ecf42549f0913e93a0dc2c90d73c651fab4/src/libraries/System.Private.CoreLib/src/System/Runtime/CompilerServices/QCallHandles.cs#L24)-style argument
```csharp
namespace Mono
{
    internal unsafe ref struct ICallArg<T> where T : class?
    {
        private void *_ptr;
        internal ICallArg (ref T o) {
            _ptr = Internal.Runtime.CompilerServices.Unsafe.AsPointer (ref o);
        }
    }
}
```

And then pass `new Mono.ICallArg<Thread>(ref t)` wherever we used to pass a plain `Thread` argument.  Turns out we don't have to change anythign on the native side since the `HANDLES` icall wrappers are already effectively a `MonoInternalThread **`.

This is enough to convince the linker that it doesn't need to preserve everything in `Thread`, and it retains a modicum of type safety so we're not just passing around IntPtr everywhere.

If I do this to every icall, then I can add a `private IntPtr garbage` field to `Thread` and the linker will remove it.

"
"if 3rd party converters are stageful, it's garbage in, garbage out. works for me"
TL;DR: **No observable change** in perf for the identified worst case with this change.
called once and cached?  or called multple times?  we may want to consider hte former to prevent excess garbage.   #Resolved
"I think this comment can sound very misleading - we definitely want to take large pause time very seriously ðŸ˜„ I think you meant to say the amount of decommit we want to do is kind of proportional to the elapsed time inbetween GCs, as in, if there's only a little time elapsed we wouldn't want to spend a lot of paused doing decommit. perhaps say that instead."
"We are seriously hitting the point of saying, ""wow, that's a lot of parameters!""

I assume that parameter passing is faster than reading a static field from a class, but...*is it*?

Is there some point where we should reconsider this signature?"
"I don't believe that there's anything we need to be concerned about with `int`. Reads and writes are atomic, and since the calculation is stable, the worst that could happen is potentially multiple writes of the same value."
"####                     match.Text = input.ToString(); // Only box in case a match Object is going to be returned to the user.

---

We should not do this. This will make it incredibly expensive.  Effectively, the existing public APIs that work in terms of Match simply aren't compatible with span inputs. #Resolved"
"`delete this.text` would be the worst you could do. V8 will deoptimize the whole object and your optimization becomes a deoptimization.

If you really need to optimize here (I doubt that), you can use `Object.definePropery` to override the getter with the computed value"
I would have had to duplicate the infrastructure in the other server project. These tests weren't supposed to be here in the first place. There's a workitem to clean them up. And @github will be incredibly happy about it.
"Again,
> This isn't incredibly urgent. But, might be nice to get it into ~~2.1.28 if ProdCon~~ _3.1.15 if dependency flow_ hasn't reached this repo by the time the changes are approved and validated.
> /fyi @github 
> /btw this is `tell-mode` because it only affects samples and tests."
"i switched to a pooled approach so we can avoid some intermediary garbage.  note, we still need the StreamWriter, but that's a tiny object (compared the byte[]s we save with pooling)."
"It turns out that these dictionaries are typically very small (usually zero or one item). I don't know which would overall be faster, but a builder is certainly an option. Using a builder requires creation of the builder, and then a dictionary, and then an immutable dictionary, so for the typical case it might create more garbage. I'm going to leave it this way in this PR though.

---
In reply to: [486774228](https://github.com/dotnet/roslyn/pull/47567#discussion_r486774228) [](ancestors = 486774228)"
"One early option would disabling this telemetry if `IsFullyLoadedAsync` is true:

https://github.com/dotnet/roslyn/blob/28b4508467cdc2ad2af7cb2f52c74cc73515a7e8/src/Workspaces/Core/Portable/Workspace/Host/Status/IWorkspaceStatusService.cs#L46

This minimizes the impact of telemetry collection to a worst-case scenario of slower solution opening, while allowing us to expand the collection in the future if we both 1) demonstrate that it is not expensive and 2) demonstrate that analysis is in place such that the additional data is likely to directly result in a better future product."
yay you used my fun template garbage
"I converted the test but...

> Another advantage is that the test would be executed on all our platforms without any other work.

Unfortunately the test won't work on other platforms for two different reasons:
- Mono has conservative garbage collection and it's currently not possible to precisely trigger collection of an object. I could move the object creation into different thread which would partly avoid problem with conservative stack scanning but some other stack could accidentally have the same numerical value and the test would still fail.
- The test requires pumping the message loop which uses different APIs on UIKit and AppKit. Since it would not work anyway I left only the AppKit version.

I may have missed some guard for the test that would prevent it from running on legacy Xamarin.Mac+Mono."
"> But then you go on to say it does have to with ordering:

Sure, I could have been more precise in my wording: It has nothing to do with *cross-thread* ordering.

> Volatile really was the semantically correct and cheap ""atomic"", for x86, until ARM came along (and PowerPC, Sparc, etc...and worst of all, Alpha).

It's really not, though. MSVC gives `volatile` acquire/release semantics on x86 and x64, but that's *bizarre* to say the least. No other compiler in widespread use that I'm aware of does this; they all follow the GCC/Clang definition which is much closer to the intent of the C standard.

For portability and sanity's sake, you really should be compiling with `/volatile:iso` if you're using MSVC so you don't end up relying on this historical quirk on x86/x64."
"If we truncated the identifier by character count, what's the worst that could happen? Is there a language with characters much wider than ""W""? We could just truncate to length and accept that the dashboard might expand slightly (since the alternative would be visually clipping something).
"
"To me I'm reading it as meeting the previous criteria of ""if the text is already loaded and the tree is already parsed.""

But seriously, what do you want it to say? I don't say that to be flippant, but I'm not psychic and can't fix problems I don't see. :smile:
"
"@github The fix you want actually almost happened in #33050, but was removed because the performance cost was too high:

> The specific change made by this PR is that type arguments are permitted to make circular references in aliased types of the following kinds:
>
> - Instantiations of generic class and interface types (for example `Array<Foo>`).
> - Array types (for example `Foo[]`).
> - Tuple types (for example `[string, Foo?]`).

then [later](https://github.com/microsoft/TypeScript/pull/33050#issuecomment-525471967)...

> Performance tests above show a worst case 5% check time increase and 10% memory consumption increase. That's a bit too expensive. With the latest commit we only defer type argument resolution for aliased array and tuple types. That should improve the numbers.

This is actually observable in your example in 3.7 by [changing the `SampleMixin1` and `SampleMixin2` to be tuple or array types](https://www.typescriptlang.org/play/#code/LAKApgHgDg9gTgFwAQGMYDsDOyDKBDAWygBswBZASwgvQEYkBeJAHgBUlIEx0ATTJAILoAngGEM2OAFcUCeMxgAjAFZhZAPnUAKRXkxgAXElYBKRutApie-viKlK1Ohwhde-XfqQBvUEn9IeOgwCAAWYHAAhEZ2JORUNABMoAC+oJCwiEgIwlBgSLEOCc5MANqONMw5eTAAZgWEcRV06gC6oOnQ8MhoWLiNRU6JjCzsnNx8giLifdKy8kqqGtqe+Uam5pbWmLYD8UMubpOrPn4BQSHhUTF7zbSpnZnI1fmF+0kj5cVVuWB1DfZ3uhEm0OiBQAB6CFITBSKBPGgAcyQPDU1jgeAQFAkSFq8CQBGKSCgmK4cHQj262V+U2EADEpOhZNj0MwBCMgsJ1AEAkwtAA6QU0KBSBBGTmlVpmBjcgSUrIvWkzSQyORwNkckTc-xMdBgADuSAFQvQIrFgREkulsvlzxpzTYhwm-CE9MZzIw3KYAEk+kEUGBWL9mAAlMAIKTkoN5NiaIA).

I know this is one of these so-close-yet-so-far thingsâ€”Iâ€™m not suggesting you can get by with making all your types tuples. Iâ€™m pointing this out because it will help you ask for the right thing. This has nothing to do with mixins; this is all about circular type references. You want circular type references to work for instantiations of generic types."
"Looks like you're spamming a bunch of repos with this, so I will lock this issue; but just to take it seriously, we don't have any issue with our license at this time, and likely won't change the license to something more restrictive like the GPL."
"There's still a lot of code that can be shared. For example, this PR passed the macOS tests with new APIs introduced in macOS 10.12 that exist on iOS too. I'm not necessarily saying the same approach will work everywhere but at least part of the code seems sharable. 

Locally the same change for key generation works for ECC too. I expect the key imports to be somewhat more problematic but let's see. Worst case I will have to split the implementations."
"In particular, this was incredibly difficult to track down, and I'm wondering if a blanket approach of favoring build isolation for individual assets is the right choice. Seems like it would at worst be slower and more clunky, but it would avoid very tricky bugs."
"> 2\. ""Patch"" the initial [callcounting] stub to look at tier0 instead of r2r and reset the call-counting-cell

It sounds better to me, as we never delete the call counting stubs, so this would leave less garbage. "
"Sorry for the fussiness, but would it be possible to consider doing this test without the mocking and the use of an artificial testability seam? (i.e., the `internal virtual` method)

Whenever a test uses mocking over an internal nonpublic API, each such test duplicates assumptions about internal implementation details. Over time this makes the production code harder to alter internally because refactoring (that doesn't touch public APIs) knocks over lots of tests. And then, reverse-engineering what the developer meant 18 months ago when chaining a set of Moq API calls is the worst!

Additionally in this case we have missed out on coverage of what `ThrowExceptionInRenderer` really does.

In this case I suspect it will be very straightforward to eliminate the ""mock router"" concept and just directly use `new Router()`. Then something like:

```cs
var testRenderer = new TestRenderer();
testRenderer.AssignRootComponentId(router);
```

And then after you do the thing to make it report an exception, you can read it directly from `testRenderer.HandledExceptions` and assert you got the desired exception. And no more need for the `internal virtual` ðŸ˜‰ "
"That's fair, the only thing that I worry about a bit, is that we don't give the developer any hook to avoid terminating the app, and I worry a bit that you can run into this situation by mistake quite often (like for example when binding the query string to a textbox or similar).

If you've considered those things and are ok, then it's fine. Worst case scenario I suspect you can use the Error boundaries component to prevent the circuit from completely crashing. We can wait until we receive feedback to determine if this is a problem or not, and offer some sort of switch/knob/error handler to allow you to deal with errors in whatever way you find best."
"def not liking this. 
1. symbolkey shoudn't be throwing.
2. not happy about all this sync blocking.  can we not be async here?

Worst case, when someone navigates, we can do a TWD and make it cancellable if something goes off the rails."
"This is what I had in mind:

```ts
// Fire off async update, do not await
this._updateAvailableProfiles();
// Return cached immediately
eturn this._availableProfiles?.map(s => ({ profileName: s.profileName, path: s.path, args: s.args } as ITerminalProfile));
```

Then when the profile result comes back from exthost, if they differ we would fire an event that indicates the cached profiles have changed, the dropdown would listen to this event and update. So in the ""worst case"" on start up it would go:

- TerminalService.ctor fires _updateAvailableProfiles (not await)
- Dropdown is initialized with []
- TerminalService fires an event that it's cached profiles have changed
- Dropdown listens and updates the list

We would also call `this._updateAvailableProfiles()` as soon as the primary ext host is ready."
"Okay, that's not the worst idea. But it's the last thing I'm gonna come back around to on this PR"
"Not that I can think of.  The Encoding.UTF8 shared object is in the ""map gibberish to the unknown character"" mode.  Consistent garbage in produces consistent garbage out."
"@github, IMO yes.

This is already behind the FEATURE_HW_INTRINSICS flag and so it's not something a nre platform had to consider in the first place.


Once a new platform is adding SIMD support, however, it's nicer (IMO) for all the places they should be touching up and handling to get flagged so they can iteratively work through them and ensure the quality is consistent with what everything else supports.

Worst case, they change this to an explicit `// TODO-TheirArch` so they can explicitly track it to be handled later. Without the check it's something that may easily get missed or forgotten about since"
Just following the [standard pattern](https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/implementing-disposeasync) on this.
"It's not clear to me why `64-bit tzcnt` is limited to 64-bit platforms.

On 32-bit this will ""at worst"" be a single compare against zero and a `bsf` which I'd expect should be faster than two `AllBytesInUInt32AreAscii` checks?"
"> You could crack open the assembly in a target after it is built and read the AssemblyVersion

I think what @github was saying was that even if he does this, the -ci version would still be there. One thing we could do here would be to have this test be a msbuild-based test that runs as part of the build as opposed to a unit test, and have that msbuild-based test only run on official builds which would quickly check if the assembly version is correct which you can all do with msbuild (or worst case scenario write a small c# task that makes this check)"
"> The source of Unix simulated functions is set by PAL_Initialize, which is never called in coreclr.

When did this change?

> Any thoughts about the best way to test behaviors when create_delegate invocation path is used?

The COM approach would work, but seems odd because this isn't really related to COM. This test would need some native assets though, the runner, so it really does belong under `src/tests`. Using `corerun` isn't too bad of an idea but I'd prefer to leave that as simple as possible. If I were doing this work, I'd take some inspiration from https://github.com/dotnet/runtime/tree/main/src/tests/Interop/COM/NativeClients and create a simple runner that calls directly into an updated [`CoreShim`](https://github.com/dotnet/runtime/tree/main/src/coreclr/hosts/coreshim). I wrote `CoreShim` for COM specifically, but it could be made cross-platform without too much effort. I was planning on rewriting `CoreShim` in the same style as `corerun`, but haven't gotten there yet. @github If you want to hack on `CoreShim` enough to make it work, I can handle rewriting it to align with `corerun`.

The runner in this case should be incredibly simple. It would load library on `CoreShim`, find an export, and then call it."
"Itâ€™s important because caching the wrong version of a file is bad - you canâ€™t easily recover from that without manually clearing caches, which is impossible to explain to most end users. The hash check protects users from this situation, so at worst they can reload the page to recover. 

Consider visiting the site at the same moment that a new version is being deployed. Half the assemblies are the old versions, then the second half comes from the new versions. This likely means a broken app, or at least undefined behaviour. Without the hash check this combination of files would get cached and that user would be stuck with the errors. With the hash check, the initial load fails which is good as it prevents the undefined behaviour, then on the next visit the user gets and caches a working state. 

Same general situation if you have multiple front end servers and no client affinity. If youâ€™re incrementally rolling out updates, we want to detect the case where you load an inconsistent set of resources because the requests were handled by an incompatible collection of servers. "
The settings in the release/5.0 branch include all target branches. We're probably fine there because doc-only updates during servicing are incredibly rare.
"this causes GC pressure as Builder  becomes garbage that the GC has to reclaim.  "
"@github so I see 3 options to solve this

1. Maybe `SKIndex` should have implemented `INativeObject` and not a subclass of `NSObject`. However

2. `ClearHandle` could stay (for `SKIndex` today) but not be used inside `NativeObject`. That way the linker can eliminate the method in all applications (except those using `SKIndex`).

2. Throwing an exception in `Retain` and `Release` is not needed (nor helpful). Why ? Both are `protected` so only a subclass of `SKIndex` (I seriously doubt it's common) or the Xamarin runtime can invoke them. Having this behaviour actually hurts the runtime and `ClearHandle` was a _fix_ for this. Any code that accept an `NativeObject` subclass now has to deal with potential exceptions on those call. Overriding the methods as ""no-op"" is a simpler fix.

Considering it's too late to change (at least for legacy) for option 1 then it's either 2 or 3. I much prefer the last one (3)."
"> From what I understand LiveShare takes care of the text synchronization between the guest<->host (or Nexus client<->server) and since our LSP server is in-proc what's in the real workspace should be just fine.

You're right, but the issue is one of timing. LSP clients ensure that an LSP request, for example for classifications, won't be sent until after the text changes have been sent to the server, so that classification request can operate on the right version of the text. LiveShare does indeed sync the text as well, but that sync is unordered with respect to LSP requests, so its possible that a classification request could come _before_ the workspace has been updated with the text that it's supposed to be operating on.

> Would there be a reason for an in-proc LSP server to turn on LSP text sync?

In the real world, I don't know. In theory though, yes, as LSP text sync solves the above problem by giving each handler, via the `RequestContext` object, a snapshot of the solution as the LSP client sees it, ignoring any potentially stale (or even future) text data in the workspace.

There are a bunch of rough edges and asterisks with all of this, and we're still trying to rationalize how it all works, particularly in relation to diagnostics.

The choices are essentially:

1. If a server opts in to LSP text sync, all handlers will operate on the LSP view of the world
2. If a server opts out of LSP text sync entirely (this PR), all handlers will operate on the workspace view of the world
3. If a server has the defaults (before this PR), all handlers will operate on a stale LSP view of the world

Obviously 3 is the worst case, which is why I created this PR."
"> so calling this in the worst case will do nothing.

I don't think so. Like I said

>  the connection could theoretically be restarted and stopped by the time this is run causing this to resolve a subsequent call to stop() incorrectly.


"
"This is producing about 100 KB of garbage opening Roslyn. Every bit counts, don't buy the debugging argument that's what results view is for."
"This is the required additions to the host file to support SSO for the Blazor WebAssembly template.
# Description

Update to the Blazor WebAssembly template to support the Azure Active Directory option from within Visual Studio.
# Customer Impact

Customers won't be able to  create Blazor WebAssembly applications with Azure Active Directory authentication from Visual Studio.

# Regression?

No

# Risk

Very Low. These are changes to templates only. This is lighting up a new experience on Visual Studio and the changes were verified by the tooling folks, worst case scenario non existing scenario regresses as part of this change."
"Thanks for reviewing. 
Yes, the config is left in storage service as garbage.
Correct, the previous fix should have no negative effects."
"Once again this user who wants to destroy terminal? :smile: 
So the restore is managed the TerminalPage level so we won't need to remember who hid it. 
If I don't miss anything In the worst case we will get mouse restored.."
"ha ha spreading a RegExpExecArray will surely never go wrong.

seriously, this is annoying if You Know What You're Doing, but I would feel comfortable making people write a cast for this."
"@github at worst that's probably much faster than trying to resolve the 100 different overloads. Though, I'm going to try to avoid speculating. I wasn't seeing a real perf hit in our benchmarks even with the initial change.
"
"While the api is in fact... strictly the worst I've ever had the displeasure of operating on, that's actually the best reason around to write a unit test, as its not really plausible that anyone will ever actually understand the darn thing intuitively, and for each case where we use the api really needs a unit test to ensure that particular use remains functional. For that particular marshalling concern, I think that the api is actually useable for that, as I think it will produce indices in a particular easily testable pattern. 

It will probably be something like...

`$""0.1.{parameterIndex + 2}.1.1""`

Since the test you're looking at has some extra BYREF/PTR scenarios, its a bit more complicated than that, but it should actually be workable."
"@github actually, I've looked at CG1 compiled S.P.C. and I've found CG1 emits these fixups too, only seemingly less often than CG2. But still, it seems like a completely unnecessary garbage that we can get rid of."
"Oh, seems like a lot of the garbage is Mono's garbage. Seems like after #2151 we embed garbage CoreCLR strings into Mono builds and garbage Mono strings into CoreCLR build.

The proper fix for that is still tracked in #7643. I'll bump that issue to .NET 6 and mark as linkable framework."
"> Note that you can still do that, even without settable Target and the API to get both Target and Dependent atomically.

Do you mean by taking a lock for readers as well, and having the remove method dispose, and then check that? I mean I can see how that would work but I was hoping to keep the performance difference to a minimum, considering that the type is pretty niche anyway. The public API are already taking a small performance hit due to the additional allocation check that the internal versions don't have ðŸ˜Ÿ

Also I'm not going to be the only one using the type and I would assume other consumers would be happy to have all those APIs available as well, especially the ones that are currently hacking `DependentHandle` through private reflection today and using them. This is just my 2 cents here, but what I'm trying to say is that I'm not convinced that limiting the public API surface would be the best decision here, considering how incredibly niche and perf oriented the type itself was meant to be from the beginning, and the fact that those APIs are already there, they're still pretty simple and don't constitute much more code to maintain at all (especially given that we'd need to keep them anyway as they're used internally), and they've been proven valuable already through `ConditionalWeakTable`. Personally I would really suggest to expose them publicly as well, though of course with whatever signature/name the team prefers ðŸ™‚"
"> It's unhappy about the INumberBase.CreateChecked. It's a default interface method. I didn't realize we have static default interface methods (I thought that's what the WIP pull request https://github.com/dotnet/runtime/pull/66887 is about?).

During the last status sync it was indicated that things should be generally working with just a few remaining edge cases. Apparently those edges are a bit more visible than expected

> I'm looking into it. Does this need to go in Preview 5?

The plan was for this to be in P5 yes. Worst case I'll need to remove the DIM and manually declare the implementation for all 20 types."
Nit: Test names like this are the worst - `TestMissingForObjectCreation` would be much better.
"> What I'd prefer is for a revert of dotnet/runtime#69404 or a follow-up change that includes HostFactoryResolver.cs in the .Sources package.

My bad. Should have said ""HostAbortedException.cs"".

> The new exception should have gone in the Mirosoft.Extensions.Hosting.Abstractions library.

I'm not sure that would help in the GetDocument.Insider project. That intentionally has very old and very few dependencies and targets `netcoreapp2.1` and `net462`. The point of using Microsoft.Extensions.HostFactoryResolver.Sources is to keep those dependencies lean and leave the ""inside man"" lib compatible w/ a very broad swath of ASP.NET Core web sites.

Please place `HostAbortedException` in the .Sources package instead. Worst case, use `#if` to decide between an exception in the Abstractions library and something in the .Sources package (maybe an optionally-defined inner `class`)."
Yeah.  let me see if i can find a better way to do this that produces less garbage.
"That would only be possible if a user callback takes a vastly inordinate amount of time to run. `TickCount` overflow would not impact the check done here, the check done here is on the difference between current tick count and start tick count, that signed value would take a lot more to overflow, enough so that IMO it would already be incredibly unreasonable for user code to cause such a thing to happen for it to be worth considering here."
"If we lose link  on ```ContentPage43941``` then Garbage collector can invoke finalizer of ```ContentPage43941``` at any time. Therefore test behaviour can be different each run. Sometimes counter can be zero after test ```for``` loop.

P.S. This changes are out of issue scope. I just want to remove comment ```// At this point, the counter can be any value, but it's most likely not zero.```"
"I recalled to perform these tests but didn't find the code anymore.
However, I wrote it new and that are the results.  
..._worstCase uses the worst case length (current implementation)  
..._determineLength calls the API function twice, to get the required length and then perform the conversion
```~~~
test ""CompNaturalLang_Chunks - en.txt""
 u8u16_worstCase length       96000000 elapsed 3.5621
 u16u8_worstCase length       96000000 elapsed 3.44074
 u8u16_determineLength length 96000000 elapsed 4.63183
 u16u8_determineLength length 96000000 elapsed 4.4248

~~~
test ""CompNaturalLang_Chunks - fr.txt""
 u8u16_worstCase length       96000000 elapsed 3.78793
 u16u8_worstCase length       100500000 elapsed 3.56667
 u8u16_determineLength length 96000000 elapsed 5.04372
 u16u8_determineLength length 100500000 elapsed 4.70454

~~~
test ""CompNaturalLang_Chunks - ru.txt""
 u8u16_worstCase length       96000000 elapsed 5.06262
 u16u8_worstCase length       167400000 elapsed 4.4118
 u8u16_determineLength length 96000000 elapsed 7.37263
 u16u8_determineLength length 167400000 elapsed 6.00668

~~~
test ""CompNaturalLang_Chunks - zh.txt""
 u8u16_worstCase length       96000000 elapsed 5.52145
 u16u8_worstCase length       247800000 elapsed 4.66172
 u8u16_determineLength length 96000000 elapsed 9.05221
 u16u8_determineLength length 247800000 elapsed 6.22064
```"
"@github here are two usecases where the current behavior is very frustrating:

## Usecase 1
1. I'm in a Node.js project looking at a folder with 6 different files
2. I Cmd+Click to explore one of my dependencies in node_modules
3. VS Code proceeds to open that node module in explorer, opening a thousand different hierachies in the `node_modules` folder
4. Because of the autoReveal scroll, I now have to scroll through hundreds if not **thousands** of folders before I can close the `node_modules` folder. Until I do that, I can't see the rest of my project hierarchy (both above and below `node_modules`)

*That* is incredibly annoying as a user. It takes you completely out of flow.

However, I still want to see the active file highlighted (just not scrolled to) because _that_ behavior is very useful.


## Usecase 2
Another usecase would be exploring down the sidebar and wanting to switch between viewing files you're exploring in the sidebar, **as well as** already-open tabs -- without VS Code forcing you to lose your place because the files aren't in the same part of the folder hierarchy.

Ref https://github.com/microsoft/vscode/issues/23902#issuecomment-596820200"
"> Mono has conservative garbage collection and it's currently not possible to precisely trigger collection of an object. I could move the object creation into different thread which would partly avoid problem with conservative stack scanning but some other stack could accidentally have the same numerical value and the test would still fail (or rather pass and not test for the broken scenario).

This means that the test may succeed sometimes when it shouldn't - but there's still value in it because it should never fail when it shouldn't (in other words: a failing test with Mono is always a real failure, which is valuable even if the a passing test doesn't necessarily mean nothing is broken)."
">                     new TopLevel(_containingModule, _namespaceName, name, arity, mangleName, isNativeInt: asNativeInt, _lazyErrorInfo, _lazyContainingNamespace, _lazyTypeId, TupleData);

> Unless we have a concrete scenario, I don't think it's worth the effort.

It looks like retargeting is going back and forth between the symbols, it is going to produce a bunch of ""garbage"" in the process and will fail to unify symbols 

---
In reply to: [599874814](https://github.com/dotnet/roslyn/pull/42419#issuecomment-599874814) [](ancestors = 599874814,599780574,599777725,599753462)

---
Refers to: src/Compilers/CSharp/Portable/Symbols/MissingMetadataTypeSymbol.cs:337 in c6a17a8. [](commit_id = c6a17a88d0b19d0900860c554dd99effada00ad6, deletion_comment = False)"
"> The worst case perf for rep-stos is probably for 1-byte aligned cases, not for 16-byte aligned 

This PR was triggered from a different PR https://github.com/dotnet/runtime/pull/32371#issuecomment-586674601 where I found it faster to compare 2 sets of 4096 bytes for equality with vectors that it was to pass the parameters; where the main difference was if it went via `rep stosd` at the start of the method https://github.com/dotnet/runtime/issues/32396

So its quite bad whatever it is..."
"I used LKG to benchmark this with the asserts and without the asserts (and going back to using casts) and there didn't seem to be any regression in perf. As I mentioned in the OP, we only had 36 cases where the assert had any impact on the type (allowing us to remove a cast), and none of those seemed to be likely to be a degenerate case involving a large union.

<table border=""0"" cellpadding=""0"" cellspacing=""0"" >
<thead>
<tr>
<th align=left><sub>Metric</sub></th>
<th align=right><sub>no assert conditions</sub></th>
<th align=right><sub>assert conditions</sub></th>
<th align=right><sub>Delta</sub></th>
<th align=right><sub>Best</sub></th>
<th align=right><sub>Worst</sub></th>
</tr>
</thead>
<tr class=""group""><th align=left colspan=""6"">Compiler - node (v13.0.1, x64)</th></tr>
<tbody>
<tr class=""measurement memory scenarioundefined host0"">
<td align=left><sub>Memory used</sub></td>
<td align=right><sub>268,971k (Â± 0.12%)</sub></td>
<td align=right><sub>269,418k (Â± 0.01%)</sub></td>
<td align=right><sub>+447k (+ 0.17%)</sub></td>
<td align=right><sub>269,371k</sub></td>
<td align=right><sub>269,462k</sub></td>
</tr>
<tr class=""measurement parse-time scenarioundefined host0"">
<td align=left><sub>Parse Time</sub></td>
<td align=right><sub>1.98s (Â± 2.58%)</sub></td>
<td align=right><sub>1.97s (Â± 1.45%)</sub></td>
<td align=right><sub>-0.01s (- 0.51%)</sub></td>
<td align=right><sub>1.95s</sub></td>
<td align=right><sub>2.01s</sub></td>
</tr>
<tr class=""measurement bind-time scenarioundefined host0"">
<td align=left><sub>Bind Time</sub></td>
<td align=right><sub>1.41s (Â± 0.45%)</sub></td>
<td align=right><sub>1.44s (Â± 2.42%)</sub></td>
<td align=right><sub>+0.03s (+ 1.98%)</sub></td>
<td align=right><sub>1.42s</sub></td>
<td align=right><sub>1.48s</sub></td>
</tr>
<tr class=""measurement check-time scenarioundefined host0"">
<td align=left><sub>Check Time</sub></td>
<td align=right><sub>10.12s (Â± 2.68%)</sub></td>
<td align=right><sub>9.89s (Â± 0.32%)</sub></td>
<td align=right><sub>-0.23s (- 2.23%)</sub></td>
<td align=right><sub>9.86s</sub></td>
<td align=right><sub>9.93s</sub></td>
</tr>
<tr class=""measurement emit-time scenarioundefined host0"">
<td align=left><sub>Emit Time</sub></td>
<td align=right><sub>7.40s (Â± 2.68%)</sub></td>
<td align=right><sub>7.39s (Â± 1.51%)</sub></td>
<td align=right><sub>-0.02s (- 0.24%)</sub></td>
<td align=right><sub>7.30s</sub></td>
<td align=right><sub>7.54s</sub></td>
</tr>
<tr class=""measurement total scenarioundefined host0"">
<td align=left><sub>Total Time</sub></td>
<td align=right><sub>20.91s (Â± 1.75%)</sub></td>
<td align=right><sub>20.68s (Â± 0.66%)</sub></td>
<td align=right><sub>-0.23s (- 1.11%)</sub></td>
<td align=right><sub>20.57s</sub></td>
<td align=right><sub>20.84s</sub></td>
</tr>
</tbody>
</table>
"
@github thanks for testing with w32tm tool. I think at least we have confirmed the expected behavior and what would be the worst-case scenario which still be acceptable considering these are rare scenarios. We should be good to go.
Did I seriously forget a semicolon?
"Coming back to this, I think the pattern should be something like this, based on:
* [implement-both-dispose-and-async-dispose-patterns](https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/implementing-disposeasync#implement-both-dispose-and-async-dispose-patterns)
* [Utf8JsonWriter](https://github.com/dotnet/runtime/blob/035b729d829368c2790d825bd02db14f0c0fd2ea/src/libraries/System.Text.Json/src/System/Text/Json/Writer/Utf8JsonWriter.cs#L297-L345)

The main points are:
* Dispose(bool) always disposes things that implement IDisposable when disposing = true and just releases unmanaged resources otherwise.
* Dispose(bool) disposes async resources and blocks if `Dispose()` was invoked instead of `DisposeAsync()`
  * This can likely be pushed to `Dispose()` however derived classes should call base.Dispose(disposing)
* DisposeAsync invokes `DisposeAsyncCore` to dispose IAsyncDisposable values, sets a flag for Dispose(disposing) to avoid invoking `DisposeAsyncCore` and blocking, calls Dispose(disposing: true) and then `GC.SupressFinalize(this).
* The finalizer only releases unmanaged resources for which there are no asynchronous versions.

@github thoughts?

```csharp
public class MyClass : IDisposable, IAsyncDisposable
{
    private bool _disposed = false;
    private bool _disposeAsync = false;

    public void Dispose()
    {
        if(_disposed)
        {
            return;
        }

        Dispose(disposing: true);
        GC.SupressFinalize(this);
    }

    protected virtual void Dispose(bool disposing)
    {
        if(disposing)
        {
            if(!_disposeAsync)
            {
                DisposeAsyncCore()
                    .ConfigureAwait(false)
                    .GetAwaiter()
                    .GetResult();
            }
        }

        // Release unmanaged resources
    }

    public virtual ValueTask DisposeAsyncCore() => return default;

    public ValueTask DisposeAsync()
    {
        if(_disposed)
        {
            return;
        }
        if(_disposeAsync)
        {
            return;
        }
        _disposed = true;

        await DisposeAsyncCore().ConfigureAwait(false);
        _disposeAsync = true;
        
        Dispose(disposing: true);
        GC.SupressFinalize(this);
    }

    ~MyClass
    {
        Dispose(disposing: false);
    }
}
```"
"Oh, this should probably take an ISet instead of a HashSet; if you have a different set under the covers we still don't want to spent garbage converting it."
"`PEReader` constructor takes a pointer here and doesn't make a copy of the data, so what is returned by `Open()` will likely return garbage after the allocated `byte[] Image` has been collected by the GC."
"Not for this PR, but subsequently I think we should delete the try/finally.  We never expect exceptions to come out of the execution, and if they do, the worst that happens is we lose the runner instance."
"> I agree... if I'm reading it correctly (and looking at the correct set of numbers, which I may not be), some of those regressions are upwards of 2x

@github, the first half are slower with the worst case being 1.28x slower, the second half (which includes the one that is 2.73 diff) is faster
| Slower                                                                           | diff/base | Base Median (ns) | Diff Median (ns) | Modality|
| -------------------------------------------------------------------------------- | ---------:| ----------------:| ----------------:| -------- |
| System.Tests.Perf_Int32.ToStringHex(value: -2147483648)                          |      1.28 |            21.96 |            28.20 |         |
| System.Tests.Perf_Int32.ToStringHex(value: 2147483647)                           |      1.26 |            22.38 |            28.24 |         |
| System.Tests.Perf_UInt64.TryFormat(value: 12345)                                 |      1.22 |            11.32 |            13.84 |         |
| System.Tests.Perf_UInt64.TryFormat(value: 0)                                     |      1.22 |             6.62 |             8.09 |         |
| System.Tests.Perf_Int64.TryFormat(value: -9223372036854775808)                   |      1.18 |            42.43 |            50.16 |         |

Notably, some of these tests are quite noisy and result in very different numbers across multiple independent runs."
"Here's the stack trace (from analyzing it in Windbg):

<summary>Stack Trace</summary>
<details>

```
 # Child-SP          RetAddr               Call Site
00 (Inline Function) --------`--------     libcoreclr!MethodTable::GetFlag [/__w/1/s\src/coreclr/vm/methodtable.h @ 3554] 
01 (Inline Function) --------`--------     libcoreclr!MethodTable::HasComponentSize
02 (Inline Function) --------`--------     libcoreclr!my_get_size+0x7
03 00007f7b`ea897d70 00007fbc`fb9a054a     libcoreclr!WKS::gc_heap::mark_object_simple+0x75
04 (Inline Function) --------`--------     libcoreclr!WKS::gc_heap::mark_through_cards_helper+0x4f [/__w/1/s\src/coreclr/gc/gc.cpp @ 36088] 
05 00007f7b`ea897dd0 00007fbc`fb990a5d     libcoreclr!WKS::gc_heap::mark_through_cards_for_uoh_objects+0xcda
06 00007f7b`ea897f00 00007fbc`fb98cc94     libcoreclr!WKS::gc_heap::mark_phase+0xcfd [/__w/1/s\src/coreclr/gc/gc.cpp @ 25122] 
07 00007f7b`ea897fc0 00007fbc`fb998d5b     libcoreclr!WKS::gc_heap::gc1+0x264
08 00007f7b`ea898010 00007fbc`fb9880ca     libcoreclr!WKS::gc_heap::garbage_collect+0x88b [/__w/1/s\src/coreclr/gc/gc.cpp @ 22032] 
09 00007f7b`ea898080 00007fbc`fb98a4f0     libcoreclr!WKS::GCHeap::GarbageCollectGeneration+0x3fa [/__w/1/s\src/coreclr/gc/gc.cpp @ 45098] 
0a 00007f7b`ea8980d0 00007fbc`fb9b1430     libcoreclr!WKS::gc_heap::try_allocate_more_space+0x380 [/__w/1/s\src/coreclr/gc/gc.cpp @ 17229] 
0b (Inline Function) --------`--------     libcoreclr!WKS::gc_heap::allocate_more_space+0x11 [/__w/1/s\src/coreclr/gc/gc.cpp @ 17695] 
0c (Inline Function) --------`--------     libcoreclr!WKS::gc_heap::allocate+0x2c
0d 00007f7b`ea898120 00007fbc`fb8487cb     libcoreclr!WKS::GCHeap::Alloc+0x80
0e (Inline Function) --------`--------     libcoreclr!Alloc+0x7e [/__w/1/s\src/coreclr/vm/gchelpers.cpp @ 237] 
0f 00007f7b`ea898160 00007fbc`fb865bf2     libcoreclr!AllocateSzArray+0x13b
10 00007f7b`ea8981c0 00007fbc`83d8422e     libcoreclr!JIT_NewArr1+0xc2
11 00007f7b`ea898340 00007fbc`83d80a50     System_Reflection_Metadata!System.Reflection.Metadata.MetadataReader.ReadStreamHeaders+0x8e
12 00007f7b`ea898410 00007fbc`83d7ebe7     System_Reflection_Metadata!System.Reflection.Metadata.MetadataReader..ctor+0x290
13 00007f7b`ea898590 00007fbc`853b1230     System_Reflection_Metadata!System.Reflection.Metadata.PEReaderExtensions.GetMetadataReader+0x107
14 00007f7b`ea898610 00007fbc`8539f77a     Microsoft_CodeAnalysis!Microsoft.CodeAnalysis.PEModule.InitializeMetadataReader+0x90
15 00007f7b`ea898650 00007fbc`83d75f3c     Microsoft_CodeAnalysis!Microsoft.CodeAnalysis.PEModule.get_MetadataReader+0x1a
16 00007f7b`ea898670 00007fbc`83d74c5d     Microsoft_CodeAnalysis!Microsoft.CodeAnalysis.PEModule.GetMetadataModuleNamesOrThrow+0x5c
17 00007f7b`ea8986d0 00007fbc`853b04c8     Microsoft_CodeAnalysis!Microsoft.CodeAnalysis.AssemblyMetadata.GetOrCreateData+0x7d
18 00007f7b`ea898750 00007fbc`83d73b18     Microsoft_CodeAnalysis!Microsoft.CodeAnalysis.AssemblyMetadata.GetModules+0x38
19 00007f7b`ea898780 00007fbc`853b0dde     Microsoft_CodeAnalysis!Microsoft.CodeAnalysis.AssemblyMetadata.IsValidAssembly+0x18
1a 00007f7b`ea8987b0 00007fbc`83d5b34e     Microsoft_CodeAnalysis!Microsoft.CodeAnalysis.CommonReferenceManager<Microsoft.CodeAnalysis.CSharp.CSharpCompilation,Microsoft.CodeAnalysis.CSharp.Symbols.AssemblySymbol>.GetMetadata+0x11e
1b 00007f7b`ea898870 00007fbc`83d55177     Microsoft_CodeAnalysis!Microsoft.CodeAnalysis.CommonReferenceManager<Microsoft.CodeAnalysis.CSharp.CSharpCompilation,Microsoft.CodeAnalysis.CSharp.Symbols.AssemblySymbol>.ResolveMetadataReferences+0x92e
1c 00007f7b`ea898b80 00007fbc`85503aa4     Microsoft_CodeAnalysis_CSharp!Microsoft.CodeAnalysis.CSharp.CSharpCompilation.ReferenceManager.CreateAndSetSourceAssemblyFullBind+0xf7
1d 00007f7b`ea898d70 00007fbc`83868a49     Microsoft_CodeAnalysis_CSharp!Microsoft.CodeAnalysis.CSharp.CSharpCompilation.ReferenceManager.CreateSourceAssemblyForCompilation+0x24
1e 00007f7b`ea898da0 00007fbc`8551338b     Microsoft_CodeAnalysis_CSharp!Microsoft.CodeAnalysis.CSharp.CSharpCompilation.GetDiagnosticsWithoutFiltering+0x709
1f 00007f7b`ea898ee0 00007fbc`83866429     Microsoft_CodeAnalysis_CSharp!Microsoft.CodeAnalysis.CSharp.CSharpCompilation.GetDiagnostics(System.Threading.CancellationToken)+0x1cace8b
20 00007f7b`ea898f20 00007fbc`85c3db4f     DllImportGenerator_Unit_Tests!DllImportGenerator.UnitTests.TestUtils.AssertPreSourceGeneratorCompilation+0x129
21 00007f7b`ea898f90 00007fbc`8548a157     DllImportGenerator_Unit_Tests!DllImportGenerator.UnitTests.Compiles.<ValidateSnippets>d__1.MoveNext+0x1af
22 00007f7b`ea899050 00007fbc`85c3d919     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncMethodBuilderCore.Start<System.__Canon>+0x67
23 00007f7b`ea899090 00007fbc`fb9b87a3     DllImportGenerator_Unit_Tests!DllImportGenerator.UnitTests.Compiles.ValidateSnippets+0xc9
24 00007f7b`ea8990e0 00007fbc`fb7fcb68     libcoreclr!CallDescrWorkerInternal+0x7c [/__w/1/s/src/coreclr/pal/inc\unixasmmacrosamd64.inc @ 849] 
25 00007f7b`ea899100 00007fbc`fb8b901f     libcoreclr!CallDescrWorkerWithHandler+0x78
26 (Inline Function) --------`--------     libcoreclr!<unnamed-class>::operator()+0x7 [/__w/1/s\src/coreclr/pal/inc/pal.h @ 4715] 
27 (Inline Function) --------`--------     libcoreclr!CallDescrWorkerReflectionWrapper+0xa8
28 00007f7b`ea899140 00007fbc`83736615     libcoreclr!RuntimeMethodHandle::InvokeMethod+0xb8f
29 00007f7b`ea8995a0 00007fbc`85718961     System_Private_CoreLib!System.Reflection.RuntimeMethodInfo.Invoke+0x175
2a 00007f7b`ea899650 00007fbc`8571863b     xunit_execution_dotnet!Xunit.Sdk.TestInvoker<Xunit.Sdk.IXunitTestCase>.<>c__DisplayClass48_0.<<InvokeTestMethodAsync>b__1>d.MoveNext+0x211
2b 00007f7b`ea8996c0 00007fbc`8571851c     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncMethodBuilderCore.Start[[NuGet.Protocol.HttpSource+<>c__DisplayClass14_0`1+<<GetAsync>b__0>d[[System.__Canon, System.Private.CoreLib]], NuGet.Protocol]](<<GetAsync>b__0>d<System.__Canon> ByRef)+0xe1315b
2c 00007f7b`ea899720 00007fbc`8571814e     xunit_execution_dotnet!Xunit.Sdk.TestInvoker<Xunit.Sdk.IXunitTestCase>.<>c__DisplayClass48_0.<InvokeTestMethodAsync>b__1+0x6c
2d 00007f7b`ea899770 00007fbc`85717fb7     xunit_execution_dotnet!Xunit.Sdk.ExecutionTimer+<AggregateAsync>d__4.MoveNext()+0x1f150de
2e 00007f7b`ea8997e0 00007fbc`85717f0c     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncMethodBuilderCore.Start[[NuGet.Protocol.HttpSource+<>c__DisplayClass14_0`1+<<GetAsync>b__0>d[[System.__Canon, System.Private.CoreLib]], NuGet.Protocol]](<<GetAsync>b__0>d<System.__Canon> ByRef)+0xe12ad7
2f 00007f7b`ea899820 00007fbc`85717b78     xunit_execution_dotnet!Xunit.Sdk.ExecutionTimer.AggregateAsync(System.Func`1<System.Threading.Tasks.Task>)+0x1f1505c
30 00007f7b`ea899860 00007fbc`85717a27     xunit_core!Xunit.Sdk.ExceptionAggregator.<RunAsync>d__9.MoveNext+0x38
31 00007f7b`ea8998c0 00007fbc`8571797d     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncMethodBuilderCore.Start[[NuGet.Protocol.HttpSource+<>c__DisplayClass14_0`1+<<GetAsync>b__0>d[[System.__Canon, System.Private.CoreLib]], NuGet.Protocol]](<<GetAsync>b__0>d<System.__Canon> ByRef)+0xe12547
32 00007f7b`ea899900 00007fbc`85717170     xunit_core!Xunit.Sdk.ExceptionAggregator.RunAsync(System.Func`1<System.Threading.Tasks.Task>)+0x1f1536d
33 00007f7b`ea899940 00007fbc`85716d6b     xunit_execution_dotnet!Xunit.Sdk.TestInvoker<Xunit.Sdk.IXunitTestCase>.<InvokeTestMethodAsync>d__48.MoveNext+0x2f0
34 00007f7b`ea899a00 00007fbc`85716c3f     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncMethodBuilderCore.Start[[NuGet.Protocol.HttpSource+<>c__DisplayClass14_0`1+<<GetAsync>b__0>d[[System.__Canon, System.Private.CoreLib]], NuGet.Protocol]](<<GetAsync>b__0>d<System.__Canon> ByRef)+0xe1188b
35 00007f7b`ea899a60 00007fbc`85715b03     xunit_execution_dotnet!Xunit.Sdk.TestInvoker<Xunit.Sdk.IXunitTestCase>.InvokeTestMethodAsync+0x6f
36 00007f7b`ea899ab0 00007fbc`857156fb     xunit_execution_dotnet!Xunit.Sdk.TestInvoker<Xunit.Sdk.IXunitTestCase>.<<RunAsync>b__47_0>d.MoveNext+0x2f3
37 00007f7b`ea899b50 00007fbc`857155d1     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncMethodBuilderCore.Start[[NuGet.Protocol.HttpSource+<>c__DisplayClass14_0`1+<<GetAsync>b__0>d[[System.__Canon, System.Private.CoreLib]], NuGet.Protocol]](<<GetAsync>b__0>d<System.__Canon> ByRef)+0xe1021b
38 00007f7b`ea899bb0 00007fbc`8571521a     xunit_execution_dotnet!Xunit.Sdk.TestInvoker<Xunit.Sdk.IXunitTestCase>.<RunAsync>b__47_0+0x71
39 00007f7b`ea899c10 00007fbc`857150c7     xunit_core!Xunit.Sdk.ExceptionAggregator.<RunAsync>d__10<System.Decimal>.MoveNext+0x3a
3a 00007f7b`ea899c90 00007fbc`8571501d     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncMethodBuilderCore.Start[[NuGet.Protocol.HttpSource+<>c__DisplayClass14_0`1+<<GetAsync>b__0>d[[System.__Canon, System.Private.CoreLib]], NuGet.Protocol]](<<GetAsync>b__0>d<System.__Canon> ByRef)+0xe0fbe7
3b 00007f7b`ea899cd0 00007fbc`837fe6f2     xunit_core!Xunit.Sdk.ExceptionAggregator.RunAsync<System.Decimal>+0x3d
3c 00007f7b`ea899d10 00007fbc`85714907     xunit_execution_dotnet!Xunit.Sdk.XunitTestRunner.<InvokeTestAsync>d__4.MoveNext+0xd2
3d 00007f7b`ea899d80 00007fbc`85714852     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncMethodBuilderCore.Start[[NuGet.Protocol.HttpSource+<>c__DisplayClass14_0`1+<<GetAsync>b__0>d[[System.__Canon, System.Private.CoreLib]], NuGet.Protocol]](<<GetAsync>b__0>d<System.__Canon> ByRef)+0xe0f427
3e 00007f7b`ea899dc0 00007fbc`85714582     xunit_execution_dotnet!Xunit.Sdk.XunitTestRunner.InvokeTestAsync(Xunit.Sdk.ExceptionAggregator)+0x1f16422
3f 00007f7b`ea899e10 00007fbc`8571442b     xunit_core!Xunit.Sdk.ExceptionAggregator+<RunAsync>d__10`1[[System.__Canon, System.Private.CoreLib]].MoveNext()+0x1f169b2
40 00007f7b`ea899e70 00007fbc`857142ef     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncMethodBuilderCore.Start[[NuGet.Protocol.HttpSource+<>c__DisplayClass14_0`1+<<GetAsync>b__0>d[[System.__Canon, System.Private.CoreLib]], NuGet.Protocol]](<<GetAsync>b__0>d<System.__Canon> ByRef)+0xe0ef4b
41 00007f7b`ea899ed0 00007fbc`85713962     xunit_core!Xunit.Sdk.ExceptionAggregator.RunAsync[[System.Decimal, System.Private.CoreLib]](System.Func`1<System.Threading.Tasks.Task`1<System.Decimal>>)+0x1f150cf
42 00007f7b`ea899f20 00007fbc`857134bb     xunit_execution_dotnet!Xunit.Sdk.TestRunner<Xunit.Sdk.IXunitTestCase>.<RunAsync>d__43.MoveNext+0x392
43 00007f7b`ea89a080 00007fbc`85713391     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncMethodBuilderCore.Start[[NuGet.Protocol.HttpSource+<>c__DisplayClass14_0`1+<<GetAsync>b__0>d[[System.__Canon, System.Private.CoreLib]], NuGet.Protocol]](<<GetAsync>b__0>d<System.__Canon> ByRef)+0xe0dfdb
44 00007f7b`ea89a0e0 00007fbc`8481d0a6     xunit_execution_dotnet!Xunit.Sdk.TestRunner<Xunit.Sdk.IXunitTestCase>.RunAsync+0x71
45 00007f7b`ea89a140 00007fbc`8481c399     xunit_execution_dotnet!Xunit.Sdk.XunitTheoryTestCaseRunner.<RunTestAsync>d__11.MoveNext+0xe6
46 00007f7b`ea89a200 00007fbc`8481c30f     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncMethodBuilderCore.Start[[NuGet.Protocol.HttpSource+<>c__DisplayClass14_0`1+<<GetAsync>b__0>d[[System.__Canon, System.Private.CoreLib]], NuGet.Protocol]](<<GetAsync>b__0>d<System.__Canon> ByRef)+0xffffffff`fff16eb9
47 00007f7b`ea89a240 00007fbc`8481c2ba     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder<System.__Canon>.Start<<RunTestAsync>d__11>+0x1f
48 00007f7b`ea89a270 00007fbc`837fa31d     xunit_execution_dotnet!Xunit.Sdk.XunitTheoryTestCaseRunner.RunTestAsync+0x6a
49 00007f7b`ea89a2d0 00007fbc`837f9c12     xunit_execution_dotnet!Xunit.Sdk.TestCaseRunner<Xunit.Sdk.IXunitTestCase>.<RunAsync>d__19.MoveNext+0x26d
4a 00007f7b`ea89a3f0 00007fbc`837f9b3f     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncMethodBuilderCore.Start[[NuGet.Protocol.HttpSource+<>c__DisplayClass14_0`1+<<GetAsync>b__0>d[[System.__Canon, System.Private.CoreLib]], NuGet.Protocol]](<<GetAsync>b__0>d<System.__Canon> ByRef)+0xffffffff`feef4732
4b 00007f7b`ea89a450 00007fbc`837f9aa6     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1[[System.Int32, System.Private.CoreLib]].Start[[NuGet.Protocol.TimeoutUtility+<StartWithTimeout>d__0`1[[System.Int32, System.Private.CoreLib]], NuGet.Protocol]](<StartWithTimeout>d__0`1<Int32> ByRef)+0xffffffff`fe01e8bf
4c 00007f7b`ea89a490 00007fbc`84813421     xunit_execution_dotnet!Xunit.Sdk.TestCaseRunner<Xunit.Sdk.IXunitTestCase>.RunAsync+0xa6
4d 00007f7b`ea89a4f0 00007fbc`84813280     xunit_execution_dotnet!Xunit.Sdk.XunitTheoryTestCase.RunAsync+0xa1
4e 00007f7b`ea89a560 00007fbc`84812ebb     Microsoft_DotNet_XUnitExtensions!Microsoft.DotNet.XUnitExtensions.SkippedTheoryTestCase.<>n__0+0x40
4f 00007f7b`ea89a5a0 00007fbc`84812d69     Microsoft_DotNet_XUnitExtensions!Microsoft.DotNet.XUnitExtensions.SkippedTheoryTestCase.<RunAsync>d__2.MoveNext+0xbb
50 00007f7b`ea89a620 00007fbc`84812cdf     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncMethodBuilderCore.Start[[NuGet.Protocol.HttpSource+<>c__DisplayClass14_0`1+<<GetAsync>b__0>d[[System.__Canon, System.Private.CoreLib]], NuGet.Protocol]](<<GetAsync>b__0>d<System.__Canon> ByRef)+0xffffffff`fff0d889
51 00007f7b`ea89a660 00007fbc`84812c7b     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1[[System.Int32, System.Private.CoreLib]].Start[[NuGet.Protocol.TimeoutUtility+<StartWithTimeout>d__0`1[[System.Int32, System.Private.CoreLib]], NuGet.Protocol]](<StartWithTimeout>d__0`1<Int32> ByRef)+0xffffffff`ff037a5f
52 00007f7b`ea89a690 00007fbc`837f8faa     Microsoft_DotNet_XUnitExtensions!Microsoft.DotNet.XUnitExtensions.SkippedTheoryTestCase.RunAsync+0xab
53 00007f7b`ea89a720 00007fbc`837f8bbd     xunit_execution_dotnet!Xunit.Sdk.XunitTestMethodRunner.RunTestCaseAsync+0xaa
54 00007f7b`ea89a770 00007fbc`837f89f2     xunit_execution_dotnet!Xunit.Sdk.TestMethodRunner<Xunit.Sdk.IXunitTestCase>.<RunTestCasesAsync>d__32.MoveNext+0x11d
55 00007f7b`ea89a7d0 00007fbc`837f891f     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncMethodBuilderCore.Start[[NuGet.Protocol.HttpSource+<>c__DisplayClass14_0`1+<<GetAsync>b__0>d[[System.__Canon, System.Private.CoreLib]], NuGet.Protocol]](<<GetAsync>b__0>d<System.__Canon> ByRef)+0xffffffff`feef3512
56 00007f7b`ea89a830 00007fbc`837f888c     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1[[System.Int32, System.Private.CoreLib]].Start[[NuGet.Protocol.TimeoutUtility+<StartWithTimeout>d__0`1[[System.Int32, System.Private.CoreLib]], NuGet.Protocol]](<StartWithTimeout>d__0`1<Int32> ByRef)+0xffffffff`fe01d69f
57 00007f7b`ea89a870 00007fbc`837f820c     xunit_execution_dotnet!Xunit.Sdk.TestMethodRunner<Xunit.Sdk.IXunitTestCase>.RunTestCasesAsync+0xac
58 00007f7b`ea89a8e0 00007fbc`837f7be2     xunit_execution_dotnet!Xunit.Sdk.TestMethodRunner<Xunit.Sdk.IXunitTestCase>.<RunAsync>d__31.MoveNext+0x18c
59 00007f7b`ea89aa00 00007fbc`837f7b0f     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncMethodBuilderCore.Start[[NuGet.Protocol.HttpSource+<>c__DisplayClass14_0`1+<<GetAsync>b__0>d[[System.__Canon, System.Private.CoreLib]], NuGet.Protocol]](<<GetAsync>b__0>d<System.__Canon> ByRef)+0xffffffff`feef2702
5a 00007f7b`ea89aa60 00007fbc`837f7a77     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1[[System.Int32, System.Private.CoreLib]].Start[[NuGet.Protocol.TimeoutUtility+<StartWithTimeout>d__0`1[[System.Int32, System.Private.CoreLib]], NuGet.Protocol]](<StartWithTimeout>d__0`1<Int32> ByRef)+0xffffffff`fe01c88f
5b 00007f7b`ea89aaa0 00007fbc`837f761b     xunit_execution_dotnet!Xunit.Sdk.TestMethodRunner<Xunit.Sdk.IXunitTestCase>.RunAsync+0xa7
5c 00007f7b`ea89ab00 00007fbc`837f337d     xunit_execution_dotnet!Xunit.Sdk.XunitTestClassRunner.RunTestMethodAsync+0xfb
5d 00007f7b`ea89ab90 00007fbc`85891945     xunit_execution_dotnet!Xunit.Sdk.TestClassRunner<Xunit.Sdk.IXunitTestCase>.<RunTestMethodsAsync>d__38.MoveNext+0x3bd
5e 00007f7b`ea89ac20 00007fbc`85484bab     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder<Xunit.Sdk.RunSummary>.AsyncStateMachineBox<<RunTestMethodsAsync>d__38>.ExecutionContextCallback+0xd5
5f 00007f7b`ea89ac70 00007fbc`858916e2     System_Private_CoreLib!System.Threading.ExecutionContext.RunInternal+0xbb
60 00007f7b`ea89acd0 00007fbc`85891529     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder<Xunit.Sdk.RunSummary>.AsyncStateMachineBox<<RunTestMethodsAsync>d__38>.MoveNext+0x192
61 00007f7b`ea89ad70 00007fbc`85798316     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder<System.__Canon>.AsyncStateMachineBox<<RunTestMethodsAsync>d__38>.MoveNext+0x19
62 00007f7b`ea89ad90 00007fbc`8389fc23     System_Private_CoreLib!System.Threading.Tasks.AwaitTaskContinuation.RunCallback+0x46
63 00007f7b`ea89adc0 00007fbc`8584cd73     System_Private_CoreLib!System.Threading.Tasks.Task.RunContinuations+0x143
64 00007f7b`ea89ae70 00007fbc`85877cd6     System_Private_CoreLib!System.Threading.Tasks.Task<System.__Canon>.TrySetResult+0xb3
65 00007f7b`ea89aea0 00007fbc`837f8462     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder<System.__Canon>.SetExistingTaskResult+0x96
66 00007f7b`ea89aee0 00007fbc`858914f5     xunit_execution_dotnet!Xunit.Sdk.TestMethodRunner<Xunit.Sdk.IXunitTestCase>.<RunAsync>d__31.MoveNext+0x3e2
67 00007f7b`ea89b000 00007fbc`85484bab     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].ExecutionContextCallback(System.Object)+0xa92435
68 00007f7b`ea89b050 00007fbc`85891292     System_Private_CoreLib!System.Threading.ExecutionContext.RunInternal+0xbb
69 00007f7b`ea89b0b0 00007fbc`858910d9     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].MoveNext(System.Threading.Thread)+0xa924b2
6a 00007f7b`ea89b150 00007fbc`85798316     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].MoveNext()+0xa92339
6b 00007f7b`ea89b170 00007fbc`8389fc23     System_Private_CoreLib!System.Threading.Tasks.AwaitTaskContinuation.RunCallback+0x46
6c 00007f7b`ea89b1a0 00007fbc`8584cd73     System_Private_CoreLib!System.Threading.Tasks.Task.RunContinuations+0x143
6d 00007f7b`ea89b250 00007fbc`837f8df6     System_Private_CoreLib!System.Threading.Tasks.Task<System.__Canon>.TrySetResult+0xb3
6e 00007f7b`ea89b280 00007fbc`858910a5     xunit_execution_dotnet!Xunit.Sdk.TestMethodRunner<Xunit.Sdk.IXunitTestCase>.<RunTestCasesAsync>d__32.MoveNext+0x356
6f 00007f7b`ea89b2e0 00007fbc`85484bab     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].ExecutionContextCallback(System.Object)+0xa91fe5
70 00007f7b`ea89b330 00007fbc`85890e42     System_Private_CoreLib!System.Threading.ExecutionContext.RunInternal+0xbb
71 00007f7b`ea89b390 00007fbc`85890c89     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].MoveNext(System.Threading.Thread)+0xa92062
72 00007f7b`ea89b430 00007fbc`85798316     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].MoveNext()+0xa91ee9
73 00007f7b`ea89b450 00007fbc`8389fc23     System_Private_CoreLib!System.Threading.Tasks.AwaitTaskContinuation.RunCallback+0x46
74 00007f7b`ea89b480 00007fbc`8584cd73     System_Private_CoreLib!System.Threading.Tasks.Task.RunContinuations+0x143
75 00007f7b`ea89b530 00007fbc`85877cd6     System_Private_CoreLib!System.Threading.Tasks.Task<System.__Canon>.TrySetResult+0xb3
76 00007f7b`ea89b560 00007fbc`84813010     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder<System.__Canon>.SetExistingTaskResult+0x96
77 00007f7b`ea89b5a0 00007fbc`85890c50     Microsoft_DotNet_XUnitExtensions!Microsoft.DotNet.XUnitExtensions.SkippedTheoryTestCase.<RunAsync>d__2.MoveNext+0x210
78 00007f7b`ea89b620 00007fbc`85484bab     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].ExecutionContextCallback(System.Object)+0xa91b90
79 00007f7b`ea89b660 00007fbc`85890a50     System_Private_CoreLib!System.Threading.ExecutionContext.RunInternal+0xbb
7a 00007f7b`ea89b6c0 00007fbc`85890909     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].MoveNext(System.Threading.Thread)+0xa91c70
7b 00007f7b`ea89b740 00007fbc`85798316     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].MoveNext()+0xa91b69
7c 00007f7b`ea89b760 00007fbc`8389fc23     System_Private_CoreLib!System.Threading.Tasks.AwaitTaskContinuation.RunCallback+0x46
7d 00007f7b`ea89b790 00007fbc`8584cd73     System_Private_CoreLib!System.Threading.Tasks.Task.RunContinuations+0x143
7e 00007f7b`ea89b840 00007fbc`85877cd6     System_Private_CoreLib!System.Threading.Tasks.Task<System.__Canon>.TrySetResult+0xb3
7f 00007f7b`ea89b870 00007fbc`837fa675     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder<System.__Canon>.SetExistingTaskResult+0x96
80 00007f7b`ea89b8b0 00007fbc`858908d5     xunit_execution_dotnet!Xunit.Sdk.TestCaseRunner<Xunit.Sdk.IXunitTestCase>.<RunAsync>d__19.MoveNext+0x5c5
81 00007f7b`ea89b9d0 00007fbc`85484bab     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].ExecutionContextCallback(System.Object)+0xa91815
82 00007f7b`ea89ba20 00007fbc`85890272     System_Private_CoreLib!System.Threading.ExecutionContext.RunInternal+0xbb
83 00007f7b`ea89ba80 00007fbc`858900b9     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].MoveNext(System.Threading.Thread)+0xa91492
84 00007f7b`ea89bb20 00007fbc`85798316     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].MoveNext()+0xa91319
85 00007f7b`ea89bb40 00007fbc`8389fc23     System_Private_CoreLib!System.Threading.Tasks.AwaitTaskContinuation.RunCallback+0x46
86 00007f7b`ea89bb70 00007fbc`8584cd73     System_Private_CoreLib!System.Threading.Tasks.Task.RunContinuations+0x143
87 00007f7b`ea89bc20 00007fbc`8481d4a6     System_Private_CoreLib!System.Threading.Tasks.Task<System.__Canon>.TrySetResult+0xb3
88 00007f7b`ea89bc50 00007fbc`8586f7b0     xunit_execution_dotnet!Xunit.Sdk.XunitTheoryTestCaseRunner.<RunTestAsync>d__11.MoveNext+0x4e6
89 00007f7b`ea89bd10 00007fbc`85484bab     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].ExecutionContextCallback(System.Object)+0xa706f0
8a 00007f7b`ea89bd50 00007fbc`8586f5b0     System_Private_CoreLib!System.Threading.ExecutionContext.RunInternal+0xbb
8b 00007f7b`ea89bdb0 00007fbc`8586f469     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].MoveNext(System.Threading.Thread)+0xa707d0
8c 00007f7b`ea89be30 00007fbc`85798316     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].MoveNext()+0xa706c9
8d 00007f7b`ea89be50 00007fbc`8389fc23     System_Private_CoreLib!System.Threading.Tasks.AwaitTaskContinuation.RunCallback+0x46
8e 00007f7b`ea89be80 00007fbc`8584cd73     System_Private_CoreLib!System.Threading.Tasks.Task.RunContinuations+0x143
8f 00007f7b`ea89bf30 00007fbc`85877cd6     System_Private_CoreLib!System.Threading.Tasks.Task<System.__Canon>.TrySetResult+0xb3
90 00007f7b`ea89bf60 00007fbc`85714047     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder<System.__Canon>.SetExistingTaskResult+0x96
91 00007f7b`ea89bfa0 00007fbc`8586f435     xunit_execution_dotnet!Xunit.Sdk.TestRunner<Xunit.Sdk.IXunitTestCase>.<RunAsync>d__43.MoveNext+0xa77
92 00007f7b`ea89c100 00007fbc`85484bab     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].ExecutionContextCallback(System.Object)+0xa70375
93 00007f7b`ea89c150 00007fbc`8586f1d2     System_Private_CoreLib!System.Threading.ExecutionContext.RunInternal+0xbb
94 00007f7b`ea89c1b0 00007fbc`8586f019     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].MoveNext(System.Threading.Thread)+0xa703f2
95 00007f7b`ea89c250 00007fbc`85798316     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].MoveNext()+0xa70279
96 00007f7b`ea89c270 00007fbc`8389fc23     System_Private_CoreLib!System.Threading.Tasks.AwaitTaskContinuation.RunCallback+0x46
97 00007f7b`ea89c2a0 00007fbc`8584cd73     System_Private_CoreLib!System.Threading.Tasks.Task.RunContinuations+0x143
98 00007f7b`ea89c350 00007fbc`85877cd6     System_Private_CoreLib!System.Threading.Tasks.Task<System.__Canon>.TrySetResult+0xb3
99 00007f7b`ea89c380 00007fbc`8571468e     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder<System.__Canon>.SetExistingTaskResult+0x96
9a 00007f7b`ea89c3c0 00007fbc`8586efe5     xunit_core!Xunit.Sdk.ExceptionAggregator+<RunAsync>d__10`1[[System.__Canon, System.Private.CoreLib]].MoveNext()+0x1f16abe
9b 00007f7b`ea89c420 00007fbc`85484bab     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].ExecutionContextCallback(System.Object)+0xa6ff25
9c 00007f7b`ea89c470 00007fbc`8586ed82     System_Private_CoreLib!System.Threading.ExecutionContext.RunInternal+0xbb
9d 00007f7b`ea89c4d0 00007fbc`8586ebc9     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].MoveNext(System.Threading.Thread)+0xa6ffa2
9e 00007f7b`ea89c570 00007fbc`85798316     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].MoveNext()+0xa6fe29
9f 00007f7b`ea89c590 00007fbc`8389fc23     System_Private_CoreLib!System.Threading.Tasks.AwaitTaskContinuation.RunCallback+0x46
a0 00007f7b`ea89c5c0 00007fbc`837fe96f     System_Private_CoreLib!System.Threading.Tasks.Task.RunContinuations+0x143
a1 00007f7b`ea89c670 00007fbc`8586eb90     xunit_execution_dotnet!Xunit.Sdk.XunitTestRunner.<InvokeTestAsync>d__4.MoveNext+0x34f
a2 00007f7b`ea89c6e0 00007fbc`85484bab     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].ExecutionContextCallback(System.Object)+0xa6fad0
a3 00007f7b`ea89c720 00007fbc`8586e990     System_Private_CoreLib!System.Threading.ExecutionContext.RunInternal+0xbb
a4 00007f7b`ea89c780 00007fbc`8586e849     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].MoveNext(System.Threading.Thread)+0xa6fbb0
a5 00007f7b`ea89c800 00007fbc`85798316     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].MoveNext()+0xa6faa9
a6 00007f7b`ea89c820 00007fbc`8389fc23     System_Private_CoreLib!System.Threading.Tasks.AwaitTaskContinuation.RunCallback+0x46
a7 00007f7b`ea89c850 00007fbc`8571542c     System_Private_CoreLib!System.Threading.Tasks.Task.RunContinuations+0x143
a8 00007f7b`ea89c900 00007fbc`8586e80e     xunit_core!Xunit.Sdk.ExceptionAggregator.<RunAsync>d__10<System.Decimal>.MoveNext+0x24c
a9 00007f7b`ea89c980 00007fbc`85484bab     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].ExecutionContextCallback(System.Object)+0xa6f74e
aa 00007f7b`ea89c9a0 00007fbc`8586e6c9     System_Private_CoreLib!System.Threading.ExecutionContext.RunInternal+0xbb
ab 00007f7b`ea89ca00 00007fbc`8586e5c9     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].MoveNext(System.Threading.Thread)+0xa6f8e9
ac 00007f7b`ea89ca50 00007fbc`85798316     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].MoveNext()+0xa6f829
ad 00007f7b`ea89ca70 00007fbc`8389fc23     System_Private_CoreLib!System.Threading.Tasks.AwaitTaskContinuation.RunCallback+0x46
ae 00007f7b`ea89caa0 00007fbc`8586e139     System_Private_CoreLib!System.Threading.Tasks.Task.RunContinuations+0x143
af 00007f7b`ea89cb50 00007fbc`85715f6f     System_Private_CoreLib!System.Threading.Tasks.Task`1[[System.__Canon, System.Private.CoreLib]].TrySetResult(System.__Canon)+0x140c2a9
b0 00007f7b`ea89cb90 00007fbc`8586e595     xunit_execution_dotnet!Xunit.Sdk.TestInvoker<Xunit.Sdk.IXunitTestCase>.<<RunAsync>b__47_0>d.MoveNext+0x75f
b1 00007f7b`ea89cc30 00007fbc`85484bab     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].ExecutionContextCallback(System.Object)+0xa6f4d5
b2 00007f7b`ea89cc80 00007fbc`8586e332     System_Private_CoreLib!System.Threading.ExecutionContext.RunInternal+0xbb
b3 00007f7b`ea89cce0 00007fbc`8586e179     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].MoveNext(System.Threading.Thread)+0xa6f552
b4 00007f7b`ea89cd80 00007fbc`85798316     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].MoveNext()+0xa6f3d9
b5 00007f7b`ea89cda0 00007fbc`8389fc23     System_Private_CoreLib!System.Threading.Tasks.AwaitTaskContinuation.RunCallback+0x46
b6 00007f7b`ea89cdd0 00007fbc`8586e139     System_Private_CoreLib!System.Threading.Tasks.Task.RunContinuations+0x143
b7 00007f7b`ea89ce80 00007fbc`85717397     System_Private_CoreLib!System.Threading.Tasks.Task`1[[System.__Canon, System.Private.CoreLib]].TrySetResult(System.__Canon)+0x140c2a9
b8 00007f7b`ea89cec0 00007fbc`8586e085     xunit_execution_dotnet!Xunit.Sdk.TestInvoker<Xunit.Sdk.IXunitTestCase>.<InvokeTestMethodAsync>d__48.MoveNext+0x517
b9 00007f7b`ea89cf80 00007fbc`85484bab     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].ExecutionContextCallback(System.Object)+0xa6efc5
ba 00007f7b`ea89cfd0 00007fbc`8586de22     System_Private_CoreLib!System.Threading.ExecutionContext.RunInternal+0xbb
bb 00007f7b`ea89d030 00007fbc`8586dbe9     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].MoveNext(System.Threading.Thread)+0xa6f042
bc 00007f7b`ea89d0d0 00007fbc`85881345     System_Private_CoreLib!System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1+AsyncStateMachineBox`1[[System.Threading.Tasks.VoidTaskResult, System.Private.CoreLib],[System.__Canon, System.Private.CoreLib]].MoveNext()+0xa6ee49
bd 00007f7b`ea89d0f0 00007fbc`85795989     xunit_execution_dotnet!Xunit.Sdk.AsyncTestSyncContext+<>c__DisplayClass7_0.<Post>b__1(System.Object)+0x453185
be 00007f7b`ea89d110 00007fbc`85484bab     xunit_execution_dotnet!Xunit.Sdk.MaxConcurrencySyncContext.RunOnSyncContext(System.Threading.SendOrPostCallback, System.Object)+0x1fabac9
bf 00007f7b`ea89d150 00007fbc`837e329a     System_Private_CoreLib!System.Threading.ExecutionContext.RunInternal+0xbb
c0 00007f7b`ea89d1b0 00007fbc`837783b2     xunit_execution_dotnet!Xunit.Sdk.MaxConcurrencySyncContext.WorkerThreadProc+0x19a
c1 00007f7b`ea89d1f0 00007fbc`83778333     xunit_execution_dotnet!Xunit.Sdk.XunitWorkerThread.<>c.<QueueUserWorkItem>b__5_0+0x52
c2 00007f7b`ea89d230 00007fbc`83778271     System_Private_CoreLib!System.Threading.Tasks.Task.InnerInvoke+0xa3
c3 00007f7b`ea89d270 00007fbc`83778172     System_Private_CoreLib!System.Threading.Tasks.Task.<>c.<.cctor>b__272_0+0x41
c4 00007f7b`ea89d2a0 00007fbc`83777e23     System_Private_CoreLib!System.Threading.ExecutionContext.RunInternal(System.Threading.ExecutionContext, System.Threading.ContextCallback, System.Object)+0xd2
c5 00007f7b`ea89d320 00007fbc`83777b60     System_Private_CoreLib!System.Threading.Tasks.Task.ExecuteWithThreadLocal+0x1c3
c6 00007f7b`ea89d3e0 00007fbc`83777abc     System_Private_CoreLib!System.Threading.Tasks.Task.ExecuteEntryUnsafe+0x80
c7 00007f7b`ea89d410 00007fbc`837779ba     System_Private_CoreLib!System.Threading.Tasks.ThreadPoolTaskScheduler.<>c.<.cctor>b__10_0+0x3c
c8 00007f7b`ea89d440 00007fbc`837776c8     System_Private_CoreLib!System.Threading.Thread.StartHelper.RunWorker+0xca
c9 00007f7b`ea89d490 00007fbc`83777625     System_Private_CoreLib!System.Threading.Thread.StartHelper.Run+0x88
ca 00007f7b`ea89d4c0 00007fbc`fb9b87a3     System_Private_CoreLib!System.Threading.Thread.StartCallback+0x35
cb 00007f7b`ea89d4e0 00007fbc`fb7fcefd     libcoreclr!CallDescrWorkerInternal+0x7c [/__w/1/s/src/coreclr/pal/inc\unixasmmacrosamd64.inc @ 849] 
cc (Inline Function) --------`--------     libcoreclr!CallDescrWorkerWithHandler+0x5c [/__w/1/s\src/coreclr/pal/inc/pal.h @ 4715] 
cd 00007f7b`ea89d500 00007fbc`fb814e52     libcoreclr!DispatchCallSimple+0xfd
ce 00007f7b`ea89d590 00007fbc`fb7c316a     libcoreclr!ThreadNative::KickOffThread_Worker+0x92
cf (Inline Function) --------`--------     libcoreclr!ManagedThreadBase_DispatchInner+0x2 [/__w/1/s\src/coreclr/vm/threads.inl @ 42] 
d0 (Inline Function) --------`--------     libcoreclr!ManagedThreadBase_DispatchMiddle+0x48
d1 (Inline Function) --------`--------     libcoreclr!<unnamed-class>::operator()+0x48
d2 (Inline Function) --------`--------     libcoreclr!<unnamed-class>::operator()+0xb7
d3 00007f7b`ea89d5e0 00007fbc`fb7c375d     libcoreclr!ManagedThreadBase_DispatchOuter+0x14a
d4 (Inline Function) --------`--------     libcoreclr!ManagedThreadBase_FullTransition+0x18
d5 00007f7b`ea89d700 00007fbc`fb814f27     libcoreclr!ManagedThreadBase::KickOff+0x2d
d6 00007f7b`ea89d730 00007fbc`fbb46d49     libcoreclr!ThreadNative::KickOffThread+0xb7 [/__w/1/s\src/coreclr/vm/comsynchronizable.cpp @ 230] 
d7 00007f7b`ea89d790 00007fbc`fbf431bb     libcoreclr!CorUnix::CPalThread::ThreadEntry+0x239
d8 00007f7b`ea89d840 00000000`00000000     ld_musl_x86_64_so!pthread_exit+0x277

```
</details>

The segfault is due to the `MethodTable*` being null when inspecting an object during a GC, so I'm inclined to agree with @github that we've found a GC Hole."
How did you update this? Can you try and get the private members of the components that we ship to show up? That breaks the current E2E test in ProjectTemplates. It would be incredibly dope if we updated and enable that.
"Is it possible to add a blanket exclusion for decimal -> `enum` name pseudo-changes? Worst case would be a central list of cases to ignore, probably all involving `[AttributeUsage]` and `[EditorBrowsable]`."
">the connection could theoretically be restarted and stopped by the time this is run causing this to resolve a subsequent call to stop() incorrectly.

I don't think it can, but even if it could, the connection is in the disconnected state already so calling this in the worst case will do nothing.

> just call `this._stopPromiseResolver();` in the `_startInternal()` catch before returning the rejected promise

That's probably fine. Have to check for undefined, but that's ok.

> I think we should remove the following call to `this._stopConnection()` while keeping the log for diagnostic purposes

Hmm interesting, my biggest concern with that is `_stopConnection` cleans up some fields, but I don't think any of them will be set if the transport isn't set. And it gets rid of some extraneous logs if you hit that code path :+1:"
"> we are coupling the framework with the default implementation on the template, aren't we?

Granted this does introduce some coupling between the framework and template, however this is in line with other default handlers we have within Blazor, like the [DefaultReconnectDisplay](https://github.com/dotnet/aspnetcore/blob/main/src/Components/Web.JS/src/Platform/Circuits/DefaultReconnectDisplay.ts). By keeping this code in the framework, we can minimize the amount of JS within `index.html` (or eliminate the need to have a separate `wwwroot/js/loading.js` file). I feel like this degree of coupling isn't the worst thing given this key benefit of template simplicity. 

In fact, we may even want to simplify the additions to `index.html`, to create the `svg` element using JS (assuming that's possible) so that the only addition to `index.html` is:

```diff
-     <div id=""app"">Loading...</div>
+    <div id=""app"">
+        <div id=""blazor-default-loading""></div>
+    </div>
```"
"Generally, going with `default` is wrong because it doesn't even have to be the furthest (it's a valid `TextSpan(0, 0)`), right? But it's not _wrong enough for it to matter_. The worst case scenario is we'll order something slightly lower/higher than we should which IMO isn't all that consequential. Especially given we haven't even order things at all right now. "
"Note that any fs operation in the startup phase here can cause an error and this would be incredibly hard to find out and debug without error handling and fallbacks...
"
"@github okay I have added a new Unit test project for MS.Ext.Options.ConfigExt can you look at if what I did is okay? I took inspiration from how the unit test project for MS.Ext.Options work.

For the new API I also enabled nullable annotations using `#nullable enable/restore` (to avoid touching the existing APIs) in 
14598c1. Not that it matters incredibly much in this case, but if I understand correctly this is the way to move forward with new APIs, so why not? We could probably enable for the whole project, but I figured that would be out of scope for this PR."
"There's a first time for everything. Seriously though, I prefer this for DI-invoked ctors with a lot of arguments. In fact, it looks like we already do this in our HubConnectionHandler."
"> I'm worry that macOS-10.15 Helix machines are significantly slower or have worse network connections than the macOS 10.14 machines we were using before. The most recent `aspnetcore-helix-matrix` run for this PR failed in exactly the same way as before, w/ `IdentityUI_ScriptTags_FallbackSourceContent_Matches_CDNContent(...)` timeouts after trying to download files 3 times â˜¹ï¸. Thoughts @github @githubâ”
> 
> Separately from that and somewhat strangely, we aren't seeing timeouts of the `IdentityUI_ScriptTags_SubresourceIntegrityCheck(...)` tests and those should be using the same CDN links. May eventually need to download the files in a `static` method and save the files for use in these two tests. That's for later though.

_Pasting reply from Teams:_

I think it's a bit of a stretch to extrapolate ""machines are not as good"" from some HttpClient timeouts.  These machines aren't in an Azure data center so literally everything they do (fetch helix work items from Azure Service Bus, send events to Azure Event Hub, download payloads from Azure Storage accounts, etc) involves communicating with and downloading successfully from external resources. 

There is some variance in hardware to be found (some pools have a mix of minis and Pros;) but even the oldest, worst mac minis we have are using gigabit adapters built into their main boards connected to the same general network topology as the others.  If we see a pattern of a specific machine having this problem, we can certainly investigate but my suspicions lie with some kind of DoS prevention system with cloudflare.

When the vendors aren't using every port on the KVM I will fetch some hardware specs off a few machines but I don't think we can realistically blame compute power on a failure to download an 88 KB file from an external source in 100 seconds.

"
"> Why try-finally? 

Because if the test starts failing for some reason, we don't want to leave garbage files on the machine. Using a `try-finally` says ""no matter what happens, delete this file at the end of the test""."
"If you're bored and want to make this more efficient, just using a XmlReader directly might be more sane here. We're using an XML reader to create a full tree, only to immediately get stuff back from it. And you could call XmlReader.ReadInnerXml which would just give you the string you want directly and avoid creating a huge pile of garbage.
"
"We should default all of these XML comments to the same text used for the corresponding int-based overload, e.g. the docs for this one say:
```
Returns, in a specified time-out period, the status of a registered notification for determining whether a full, blocking garbage collection by common language the runtime has completed.
```"
"Nit: Consider adding a comment to line 91 along the lines of ""This is an unsynchronized access to the Elapsed property, which means we could observe a torn write. But the worst case is that there's a brief window where we miss the expiration notification. This is an acceptable risk."""
"Yeah. It would require surgical changes to the workhorse method on `AsciiUtility` to do the early exit here. It's definitely feasible, but I'm not confident I could get those changes through code review. Especially since the current implementation is good enough and does only a tiny amount of extra work.

Related: `Utf8Utility`'s transcoding workhorse routines perform bounds checking on the write buffer all over the place. Those checks show up in profiles. We could eliminate those checks entirely if we knew ahead of time the write buffer was large enough to hold the worst case output."
seriously on the fence about asking for some named structs.
"ok.  i see that now (wasn't showing in the diff since that read already exists).  However, i think we need a *lot* more comments here.  Your comment is accurate.  We capture... but don't catch the exception.  But taht simply explains what is happening, and not at all 'why'.  I say this honestly, but i have no idea why it's not right to propagate teh exception.  If we're reading... and it fails with an exception... why not propagate it?  

I view this code as being as critical as AsyncLazy.  In other words, it's incredibly subtle, but utterly paramount to get right.  Because of htat subtlety, i think these pieces need a *huge* amount of explanation as to what's going on.

Thanks!"
"@github can you please confirm that this (and the below DisposeAsync) is acceptable in an abstract base class, given there are no resources to be disposed on it, and that [the dispose pattern](https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/implementing-dispose#dispose-and-disposebool) could simply be implemented by implementations if needed?

Also, I've made this non-abstract since most implementations aren't expected to override.

"
"I thought that I could cut the state-machine in half (and eleminate awaits) and switch in two paths without any state-machines where in one part the third case is making less work and in another part the first/second ones (NoDataInBuffer) are doing more work (but they're not the most used ones probably), at least that was my reasoning behind it. But thinking about it again and looking at the perf-results not every case is a winner. Specificaly short lines are the worst."
"The intention here was to check that the length is ""reasonable"" for hello. I don't know if we need to. In YARP this can be used to eliminate garbage without even allocating SslStream. We don't do that here but it would be nice IMHO to have both instances same (or close as possible) "
"`""downlevelIteration""` will seriously degrade the performance of all of our `for..of` statements in the compiler. Using native generators would also be a problem for VSCode/Monaco as they still have web hosting scenarios they need to support where native generators are not available."
"IMO, this should still be an error - #31708 came up because we unconditionally treated `object` as a type with no properties for excess property checking (so all usages of `object` used to trigger excess property errors like this), which caused problems in these primitive only unions (where there really isn't any hinting as to what properties an object literal should contain, really), however in case like this one, where there's an explicit object type, using the hints from the specified object types for excess properties is more desirable (and prevents you form passing total garbage and typo'ing the property names from the provided literal)."
"This returns `undefined` for something like `{[idx: number]: number} | [string]`, which is incorrect (the object type has a numeric indexer, after all) - at worst, you'd want something like `number | undefined` for an index of `1` or more, and not `undefined`. Moreover, an intersection, like `{[idx: number]: number} & [string]` should likewise produce `number` for the result of indexing `1`, and not `undefined`."
"Thanks for the strictOptionalProperties, but it seems to me now that it leads to a lot of redundancy that looks like ```fieldName?: TypeName | undefined``` in all over the codebases. For ex. https://github.com/DefinitelyTyped/DefinitelyTyped/pull/54352/files and so forth and so on. It makes code more complicated to read.

This happens because the authors of the libraries have no clue about the state of the strictOptionalProperties  in the project that uses it, therefore they assume ""the worst"" (which is strictOptionalProperties: true) and have to write ```fieldName?: TypeName | undefined```.

The problem is that in the most popular scenario, which is when people don't care about whether ```fieldName``` exists or  undefined, it leads to the most wordy expression ```fieldName?: TypeName | undefined```, should be quite opposite, isn't?

I don't know what were the alternatives. If the only problem was an impossibility to say ""the field may not exist,  but must not be undefined""  (is there any ""real"" situation when people actually needed it, by the way? not Foo?), then ```fieldName?: TypeName && not undefined``` might be a better option? (I know it might be also complicated) . I wonder if there were other options that were considered and refused?"
"@github I fully understand the solution is not ideal, but I believe this is an increment to the right direction. The worst is when users migrating from Framework realize that API-s like `SetTcpKeepAlive` have no effect *at all*, and currently we don't give any warning about that. With this change we would at least cover those cases.

> We generally go through API review for obsoletions.

Just sent an email."
"@github pointed out that teh when-all is not necessary.  these tasks are naturally running in parallel, and it's fine to just have the foreach loop below that does hte awaiting of individual tasks.  worst case is that one loop waits a while for that task to finish. but that's what WhenAll does anyways :)"
"> Then, is it ok to produce garbage on every overtype case? 

I think that should be OK. The suggest controller shares the lifetime with the editor instance, so there will be only a few of those instantiated (as many as there are visible editors at the same time). In there, I think you could hold some property of the last overtyped text. @github can provide better directions if he has a better place to store this in mind.

> Or, it should have some memory space for it, 50-100 bytes, for example, with ability to increase temporarily.

:+1: You could introduce a limit, like if the overtyped text is over 1 million characters, then don't store it. That would make the feature not work in that case, but I guess that is OK.

> How to detect if it expired?

Whenever the code editor gets a new model, the property should be cleared. But it should be cleared in other cases too, like explicit selection change (the selection change event from the editor has a reason that could be used for this purpose). 

@github What are your thoughts ?"
"The observer is never removed, which means the BackgroundNotificationOn delegate contained in the observer will keep a permanent reference to the NSUrlSessionHandler instance, and it will never be collected by the garbage collector."
"Changing `Assign` to `Weak` should always be okay since they ""almost"" mean the same thing. IIRC weak properties are set to nil when deallocated in ARC environments instead of pointing to garbage when `assign` is used. 
"
"I'm not sure how I would assert that in the tests. I just wanted to make sure the string got truncated, even if the final outputted string, when viewed by the user, will be garbage where the emoji was saved."
"I'm not certain whether this has changed in the past few years, but the original rationale was this:

When V8 compiles a function, that compiled function has a fixed number of arguments. If you undersupply arguments to that function, what V8 does is creates a *new* compiled function with that number of arguments that in turn calls the original compiled function, with `undefined` filling in the missing arguments. The reason this can degrade performance is due to the fact that V8 has to generate this new function, and that new function could possibly be garbage collected at some point if it is used infrequently and may eventually need to be generated again.

I *have* seen this issue in practice several years ago, as there was a bug andy-ms found regarding performance in one of our core functions, though I cannot find the bug report or related PR at this time."
"Thanks, @github. That's good to know, but I don't think it explains this, as some of the worst regressions here don't involve any counters."
"Perhaps this already works just fine, but I suspect you might have to trim the trailing `?` before passing it through. This is incredibly rare, but you could do `{id:int:long?}` which basically treats the entire segment as optional, not just the `long?` Would the `:int` constraint get treated as non-optional?"
"TrimStart, just to avoid extra garbage if the end has stuff?
"
"It is incredibly obscure, I understood what it was doing. Can we please replace it with something ""less fancy"". Features like these raise the bar for contributors that need to learn about them when there are equally valid alternatives."
"> and any problems in this code will be revealed without question.)

It really comes down to usage patterns around out-variables.  i.e. do we expect them to be used commonly in things like constructors.  

Now, that said, i would prefer the underlying issue with speculative analysis is fixed.  That's much more critical *especially* as patterns (and nested patterns) are going to be incredibly common.  We cannot properly speculatively analyze here because the compiler gets very unhappy and things the locals from before/after are colliding.  I think a suitable fix would be for the compiler to be ok with the introduction (in speculative code) of variables with the exact same type and name in the same statement.  That could also be controlled by a flag if someone wanted the existing semantics.
"
">FirstOrDefault [](start = 88, length = 14)

potentially returning `null` Location here seems risky. Let's return `Location.None` instead in the worst case #Closed"
"There should at minimum be a comment explaining why the constant was chosen. Keep in mind that the max chars-to-bytes expansion for UTF-8 is 3 (not 4) bytes per char. But clearly we're not transcoding _to_ UTF-8 (since the input was already UTF-8), so using any UTF-8 specific constant would not be correct.

It's perfectly ok if we say something like ""this is a reasonable guess for a worst-case expansion for a typical encoding"". But IMO there needs to be a comment that says _something_. :)"
"""It's never used by users"" seriously contradicts the ""needs to be public for custom renderers"". Please fix as this is public API"
"> If these are running stably for you, I think we should just unskip all these. At worst we have to disable them again

Yep, thats what I did yesterday. They failed, and in the non-streaming case: https://dev.azure.com/dnceng/public/_build/results?buildId=858041&view=ms.vss-test-web.build-test-results-tab&runId=27467314&paneView=debug&resultId=165225"
this is consing up a bunch of garbage. Might be an OK place for mutating the array instead?
"> Does it affect the merge duplicate phase? Would two methods compiled under different modules be different now?

I was worried about that, but this won't contribute to determining if a function is ""unique"". The only thing used is the ""method identity"", which is (1) the IL code, (2) the method signature + calling convention + CorInfoOptions + hot/cold region + jit flags + ISA flags.

SPMI basically assumes that if the (salient) ""input"" to the JIT is the same, it will produce the same output. It's possible there are other inputs that could affect the output, like the assembly, but it's possibly a trade-off between precision and SPMI MCH file size and useful difference of MCs.

> what is the easiest way to dump it during replay for a given mc file?

`mcs -dump N foo.mch` for method context N. Then you'll see something like:
```
GetAssemblyName - 1
0-GetAssemblyName assem-000002E226DC7DD0, value-4 'FPDist_ro'
```
(this example was from `BringUpTest:FPDist(float,float,float,float):float`)

> Just that I know how to use this, can you give an example use-case? Can we use this with /dumpMap , etc?

I thought about adding it to `-dumpMap`. We do get multiple entries, which I believe is when we are considering inlining cross-module. E.g.,

```
GetAssemblyName - 5
0-GetAssemblyName assem-0000027B5D2F2700, value-28 'System.Private.CoreLib'
1-GetAssemblyName assem-0000027B5ED42E30, value-55 'System.Diagnostics.Process'
2-GetAssemblyName assem-0000027B7F8E25A0, value-86 'System.Runtime.InteropServices.RuntimeInformation'
3-GetAssemblyName assem-0000027B5ED417B0, value-140 'System.ComponentModel.Primitives'
4-GetAssemblyName assem-0000027B5ED42DB0, value-4 'Coreclr.TestWrapper'
```
(for `CoreclrTestLib.CoreclrTestWrapperLib:RunTest(ref,ref,ref,ref,ref):int:this`)

So we'd need a way to figure out which is the ""root"" assembly name to make outputting a single ""-dumpMap"" entry valuable.

> Does this work out ok for the unusual cases like stubs, LGC, etc...?

Are you worried that one of getAssemblyName / getModuleAssembly / getClassModule will fail in these cases (and spmi will crash)? I tried to make repGetAssemblyName() resilient to missing info. The others _should_ also be resilient to garbage, but I could be wrong. I can certainly trigger a collection on this PR and see what we get (beyond what I tested locally). Would we see these things in crossgen? crossgen2? PMI? benchmark run? all of the above?
"
"Letâ€™s get this in. At worst, it will tell us if we missed updating something."
"> I was thinking if compiler sees a variable declaration (initialized or non initialized) and in the given scope if that is not used anywhere, then it should warn about it

The best it could do in such a case would be to suggest using a discard, e.g.
```C#
_ = Property;
```
instead of:
```C#
string result = Property;
```
and that doesn't really buy you anything, plus there's the downside to that called out in the docs I linked to:

> The compiler generates this warning only when the variable value is a compile-time constant. Assigning a non-constant expression or method result to a local variable makes it easier to observe those expressions in the debugger. It also makes the result reachable, preventing garbage collection while that variable is reachable.
"
"`=` is lazy, `:=` is strict.

    foo = hi $(bar)
    bar = one
    $(info $(foo))
    bar = two
    $(info $(foo))

prints ""hi one"" ""hi two""

    foo := hi $(bar)
    bar := one
    $(info $(foo))
    bar := two
    $(info $(foo))

prints ""hi "" ""hi "" (empty var).

The first behavior seems incredibly insane, so having the default for most variable assignments be `:=` is good style.

Note it gets weird with recursive variables (e.g. our use of `MSBUILD_ARGS`):

    MSBUILD_ARGS = $(MSBUILD_ARGS) /some:switch

makes things get upset (""`Recursive variable 'MSBUILD_ARGS' references itself (eventually).  Stop.`"")."
">// At worst, this means we have incorrect nullability info for inaccessible properties. [](start = 12, length = 87)

Given complexity of calculation of accessibility for properties (overriding factored in, etc.), from the usability perspective (what other tools would have to do to calculate it), it might be better to always delegate to type's accessibility even on emit. Basically only accessibility of entities that have their accessibility explicitly encoded in metadata should be taken into account for the purpose of filtering. Events probably fall into the same category. #Closed"
"Ok. First, for the registration, you should just have a global API and Registration variable (lazily instantiated?). For session, you should have a global table of sessions that map to the `sslClientOptions`. If the client uses the same options (i.e. same ALPN) for a new connection, they should get the same session as before.

If you are exiting, closing the registration isn't that important. If just your DLL/library is getting unloaded, then it's incredibly important. Can you tell the difference?

BTW, I'd argue that 0-RTT is extremely important, as it's a primary feature of QUIC. You should design with it in mind right now."
"I feel inclined to forgo the blocking here and propagate the exception only if the task has already finished. Kind of best-effort, but not worth the complications. In the worst case scenario, the content exception will only get logged.
@github, @github any input on this?"
"I've never been a fan of updating TFMs in libraries for the sake of updating TFMs. We should only update at the point where we have a clear benefit coming from the new version that we can't easily have otherwise, and at that point we should seriously consider multi-targeting."
Unrelated to this PR: it looks like current logic for x64 might end up allocating just 128Mb for the initial memory in the worst case
"Ah, I missed that parameter ðŸ¤¦â€â™‚ï¸ 

It might actually be nice to allow all extensions here in the future, since we can no longer seriously claim to believe weâ€™re in a world where people only import things we know how to analyze. This doesnâ€™t even account for `declare module ""*.css""` pattern ambient module declarations. But thatâ€™s for another day. Thanks!"
"@github / @github - looks like there are several questions here:

> Why are libraries that aren't part of the shared framework being crossgend? It looks like coreclr currently relies on a ""all libraries produced"" representation which doesn't exist in the product. There are libraries that are part of the shared framework and then there are libraries that are part of packages.

> Bit unrelated but important for the discussion of what should go into CORE_ROOT at all.

I don't really have any good answer to that beyond that's something we haven't touched semantically in the Crossgen2 work. As I described above, previous R2R logic used super-simple scripting to basically ""Crossgen[1]-compile everything in CORE_ROOT and silently ignore all errors caused by hitting native DLL's'"" - I agree that's hacky, unreliable and flaky to some extent and perhaps merit cleaning up.

Cleaning this up can have several forms: as Viktor pointed out, maybe the CORE_ROOT is an overused sink of garbage that was just convenient to put in one place even though it consists of logically distinct components (CoreCLR runtime, JIT, DBI / DAC components, Win32 API contract DLLs, framework assemblies, xunit.console used internally as the test harness), maybe some more.

Making its content more structured, perhaps by using subfolders (like we already do for certain components including Crossgen2), may have positive effect on its overall management including filtering as to what gets included in Helix correlation payloads which now uses one other ad hoc hardcoded list of folders and files. I think this would be a fine thing to add as a small to medium-size (2 work-weeks or so) CoreCLR infra backlog item, potentially up for grabs.

One other thing is how we identify the list of assemblies to Crossgen. Today logic of first copying the assemblies to CORE_ROOT and only then running Crossgen(1|2) on top of them is also quite hacky, unreliable and potentially non-idempotent. A cleaner way to tackle this would be to Crossgen them directly as part of copying them from artifacts/bin to CORE_ROOT. That is also a good suggestion for a one-week CoreCLR infra work item up for grabs. Two things that complicate this are a) that it's necessary to fix this in two places for the cmd | sh build scripts, and b) that the scripts have tons of ""partial execution"" options that need consolidating with this optimized functionality.

Even with both these cleanups in place, we still haven't answered the question of the ""framework extent"" present in CORE_ROOT. In fact I noticed that many devs believe that only System.Private.CoreLib is needed to run CoreCLR tests. That is obviously not the case and the set of assemblies actually needed by the tests is a great unknown that probably also happened historically by itself, so to say.

It would be a very interesting CoreCLR infra design item to think about how to make this more explicit - for instance by explicitly specifying reference assemblies on top of some well-defined minimal set in the test projects. That would let us reason much better about the needed assembly set and its propagation to CORE_ROOT. Having said that, composing CORE_ROOT in such an incremental manner would imply one more iteration over all the 10K Pri1 test build projects which is not ideal perf-wise.

In the past, IIRC @github and @github worked on making the test projects more expressive in the sense of containing more property ""metadata"" letting the test build system dynamically decide what to do without having to stick to hard-coded lists and blindly copying around tons of unused assemblies.

Thanks

Tomas"
"> 
> 
> This makes sense, I do worry that we could end up with a bunch of strings in that table with no visibility and no way to clean them up.

I think the potential downside here isn't too great, since the strings already need to be in the C# intern table for that to happen. So the worst possible scenario is every C# interned string ends up copied into the JS heap, but it's somewhat unlikely. AFAIK the only way that could occur is if they call String.Intern, so they can just Ctrl+F their codebase for that."
"@github Our plan was to do this in early 7.0 previews, but after we were sure 6.0 was stabilized and on its path toward GA. If we merge this eagerly, then any changes/fixes to the file stream strategy work might have conflicts between `main` and the `release/6.0` branch(es). But I'll leave it to your discretion whether or not there's any risk in that. Worst case, we'd have to make a fix directly into `release/6.0` that doesn't land in `main` first."
Please do not parse error message strings.  It's incredibly brittle and sets a very bad precedent.  If you need important compiler information then the best way to get it is for the compiler to just expose an API that provides it.   #Resolved
That's the only settings page that there is not a `ms-settings` link for. Seriously. I checked. I wanted to put it in our documentation. :smile:
"I was just looking at the numbers in CI and compared it with other builds. In most cases I see  improvements but based on the hitting the network the numbers vary slightly accross builds. That said, this should provide confidence that the regression introduced in the previous attempt is now addressed. 

Thoughts on the numbers:
- Overall restore is either on parity or faster. This is because of a) a single entrypoint to restore and b) fast restore mode enabled.
- No-op/Incremental restore is MUCH faster. Incremental restore takes up to 90% less time with NuGet Fast Restore Feature.
- Libraries Test Builds should be noticeably faster in CI as it just restores what it needs.

| Leg | Build&Restore Before | Build&Restore After |
| --- | --- | --- |
| Mono Product Build Linux arm release | 6m 55s | 4m 45s |
| Mono Product Build Windows_NT x64 debug | 4m 0s | 3m 58s |
| Libraries Build Linux x64 Debug | 12m 18s | 12m 18s |
| Libraries Build OSX x64 Debug | 8m 53s | 9m 10s |
| Libraries Build Windows_NT x86 Release | 15m 24s | 13m 22s |
| Installer Build and Test Linux_musl_arm64 Debug | 11m 52s | 5m 34s |
| Installer Build and Test OSX_x64 Debug | 8m 29s | 8m 36s |
| Libraries Test Build Linux x64 Debug | 8m 37s | 7m 30s |

Compared https://dev.azure.com/dnceng/public/_build/results?buildId=582003 with https://dev.azure.com/dnceng/public/_build/results?buildId=583335 and looked into other builds as well to make sure that the before numbers aren't a best or worst case."
"I think that if your theory is correct then we should actually change the way TestFixtures are created in the test infra - instead of trying to reuse folders (`1`, `2`, ...) we should generate a random unique name (GUID?) each time - if one of the fixtures fails to delete itself properly due to file system races it will leave garbage on disk, but it will NOT fail the tests."
"Upon reviewing the code I changed my mind and decided this isn't worth pursuing for .NET 6. The formatting code does run for all scenarios, not just when the diagnostic logs have been enabled. If somehow this change caused a problem it would affect all dotnet-counters users.

Worst case if folks need help diagnosing issues in MetricEventSource for 6.0 we can attach a debugger or (unlikely) fix this as a servicing fix when the issue arises."
"Nothing bad can happen from this, the worst case scenario is described below where the field is set."
"IPolicyEvaluator is a user extensibility point, assume the worst ðŸ˜."
"I really think anything that represents 'Data', but is keyed on 'SyntaxTree' just isn't the right abstraction.  It just means we have this incredibly strange point of inconsistancy in the Compilation API.

Even if we allowed this, the only way to really make things make sense would be to have to update compilations in two steps.  First, replace your trees, then replace your options.  Seems *super* strange and unlike anything in the compilation today.

Could you let me know what the concern would be with the Resolver pattern?  If we didn't do anything like that *today* i could understand a lot better :)   However, since we've commonly used that pattern to say ""hey... host... can you go figure this out on demand"", it seems pretty appropriate to me here :)"
"Example: The length marker says that there should be 2 bytes of data, but the SequenceReader contains only 1 byte of data. This routine stack-allocates a 2-byte buffer (contents uninitialized) then calls `SequenceReader.TryCopyTo`. The `TryCopyTo` method happily copies over the 1 byte of data it has, so now the original stack-allocated buffer contains a mix of initialized + uninitialized data. The partially-uninitialized buffer is then passed to a parsing routine. The behavior of the parsing routine may now be dependent on whatever garbage data was previously on the stack."
"I'd add `IsNotHorizontal` and `IsNotVertical` too, because they're convenient :P

More seriously, we could do with one of them, but definitely not 2. And the naming should be something else. And if we can't agree, let's drop those inconvenient methods :)"
"@github thank you very much for this PR, we appreciate your openness to collaborate and share ideas.

As we have discussed we already started on the 3 way merge implementation, and we have an initial version that we just merged yesterday. It lacks tests and probably does not cover all the corner-cases for viewzones, so there is still a lot of work to be done. This work is captured in this PR https://github.com/microsoft/vscode/pull/150391

Having said that, we have tried out your great PR and there are definitely things we can learn from. The best next step would be for @github to meet with @github so you can go into technical details. I will try to setup something for next week.

Apart from that I really appreciate that you shared the User Feedback, we take this very seriously fyi @github 

I am very excited about this collaboration and am looking forward to bring the 3-way merge editor to all our users out there."
"I'm confused by the null-out here: the underlying data the CancellationTokenSource holds onto is gotten rid of in the Dispose() case, right? The docs say things like ""you must release all references"" but I can't tell if that's just automatic stuff the docs generate for all IDisposable things. Specifically this warning:

> Always call Dispose before you release your last reference to the CancellationTokenSource. Otherwise, the resources it is using will not be freed until the garbage collector calls the CancellationTokenSource object's Finalize method.

seems off because the implementation verbatim says:

> // There is nothing to do if disposing=false because the CancellationTokenSource holds no unmanaged resources.

@github is this null-out actually required, or is this just something the docs generate automatically for those who think IDisposable.Dispose() also makes the direct object go away?"
"I think it's a positive change. I'm not worried about this allocation, but passing in arguments would seem sensible if it helps. Worst case it's possible to add a null check at callsite.
Thanks!"
"it would be the same tree. In a worst and unlikely case I assign it more than once.
We are in a huge trouble if we could have different trees here.

Use of CmpExch was expensive in this method according to profiler. I removed it where it was not adding uniqueness guarantees.
"
"> The ""worst case"" is that we mix in an extra undefined that's already expressed in the property type's constraint

Yup, that's exactly it.

> Maybe an inline comment on why we're making this change on the appropriate line is relevant

Don't think we need a comment. With this change the code is actually more consistent with what we do on the next line to shallowly remove `undefined` from the type.

But, for this and other reasons, it definitely would make sense to add the repro from #36564 as a performance test case. That way we'll see a 10% regression if someone messes with it."
"> Changing exception types isn't considered a breaking change, correct? I am not sure we need a breaking change doc for that.

> Additionally, having the performance degrade beyond 1 GB in certain edge cases isn't really a breaking change either.

I think this is more a bug than a breaking change. The exception is now more appropriate. OOM exception is not normally caught either. A potential breaking change is that there is ""hang-like"" performance characteristics in certain scenarios.

> Typically we don't consider it breaking when we avoid throwing and instead make progress. How likely would you think production code is to get over 1 GB while still advancing small amounts? Have we heard about this from any customers?

I am not aware of any customer issues yet. We have to be cognizant that it may be reported as a hang, not a perf issue.

I believe the sample code that I pasted in the description is a fairly common pattern for ABW: smallish `sizehint` in `Advance()` and many calls to `GetMemory()`. The questionable part is the >1GB size -- I'd guess it is unlikely, and when encountered usually is a flag to refactor the code to not require that much data.

Aside, one benefit of this PR is that is allows a deterministic max size (~2GB) along with a deterministic OOM message that is not ""randomized"" by the amount of free space in the currently allocated buffer. The previous approaches of either always doubling or using `sizeHint` did not calculate in free space and thus may not allow for a full ~2GB size -- in the worst case scenario, the previous code would only allow 1GB in size before throwing.
"
"> If we expect arbitrary encodings, then ignore my comment, since I don't know what the worst case factor would be in that case.

I could change it to 3x. Unicode is the 2nd encoding that we register by default with the formatters, so that makes sense."
"I don't feel incredibly strongly about SignalR and MVC being consistent. I do feel strongly that case-insensitive is the wrong choice for building a REST api. 

I think it's a wierd idea to we would do case-insenstive because legacy. "
"> I don't expect users to use a code fix that removes unused code in presence of errors

I highly disagree with that.  :)   

For one thing, it presumes the user is even aware that there *are* errors.  As a user, I do not have my error list even open most of the time.  So i would trust that faded/suggested code was something i could trust.  :)  

>  and I feel we should wait for user feedback before attempting to detect presence of errors and start bailing out.

Note that we have had feedback here in the past for other features.  I'm fairly certain we have a bug *right now* (i'm just too lazy to look) that this happens with 'usings'.  :)

> Regardless, your concern is not specific to this feature, but IOperation analyzers in general,

No... :)   To be clear, it's specific to the user experience.  This is a feature (and i don't care how it's implemented), and it's one taht could have a poor experience for the user *regardless* of fix vs fix-all.  That's because instead of leniently assuming the worst when we have errors, we conservatively assume the best.  i.e. we assume we can just ignore the error information if the compiler doesn't exactly know what's going on.  Now, that's happens to be because this impl decided to sit on top of IOp.  But tha'ts an implementation detail.  The critique would hold if you'd written this the same way on SemanticModel :)
 #Resolved"
The lack of member tracking in arrays was particularly problematic for this change. Then again this code is essentially paassing around large arrays of mutable `struct` values. It's the worst possible case for nullability. This doesn't make me want to push for array index tracking (that's really expensive) but it does make me think that if we were doing this again we would've chosen a different approach. Possibly wrapping this array in a simple `struct` wrapper that could've handled many of these assertions.
"Shouldn't the `Log` property be set to `log`?

Relatedly, do we actually want `log` to be a `TaskLoggingHelper` and not an `Action<TraceLevel, string>`, as is done elsewhere?  That way if `log` is null, we can default it to `Console.WriteLine()`s as a ""worst case we need to log *something*"" scenario."
"No, the worst thing that will happen is that we won't get this additional debug info in verbose mode."
"> This optimization isn't incredibly important

I would remove it in that case : )

> I think the broader question here is whether we should be relying on implementation details like this in libraries

I would evaluate these on a case-by-case basis. If they are important optimizations I don't mind commiting to them but DiagnosticSource has the extra challenge that users can upgrade their dependency to any future version of it via the out-of-band NuGet package. So any dependency this code takes on its implementation becomes a requirement that every future version of the DiagnosticSource library must maintain."
same comment about garbage
"It's not thread-safe, I just assumed in the worst case scenario the FCall would be repeated, but no harm done? From what I can see, other caching mechanisms for `RuntimeType` also do the same, with the cache potentially being initialized multiple times in case there's a race condition. I assume that that QCall is just returning the fixed address of the right JIT allocation helper, so there's no allocations involved and no concerns in case the call is made multiple times? ðŸ¤”"
"> Library uses asserts internally to validate state and conditions. This is a debug/checked build assert that should be excluded in release builds

Why exclude them in release builds? If the state and conditions being valid is important we should validate it at runtime in all cases, unless the performance hit is too significant. Not validating it could lead to larger bugs and (in the worst case) potential security problems"
"> For the new API I also enabled nullable annotations using #nullable enable/restore (to avoid touching the existing APIs) in
14598c1. Not that it matters incredibly much in this case, but if I understand correctly this is the way to move forward with new APIs, so why not? We could probably enable for the whole project, but I figured that would be out of scope for this PR.

I think it's better to add the API without nullable annotations. They would get covered once the entire assembly is annotated. It could make it harder to review later otherwise."
"I think that's ok.  Ideally we could get the error=>string mapping into a near-term 16.xx release, but even if we couldn't, I'd rather have a better experience here for the longer term.  After all, one of the primary purposes of having this block in place is to provide a better user experience that's explanatory of the situation than what would happen if you could successfully attach but find yourself unable to meaningfully debug and not know why.  And if it is ""Unable to attach. Unknown error: 0x8xxxxxxxx"", worst case that 0x8xxxxxxxx becomes searchable and we can ensure the right information is exposed in docs about what that message means."
Remove this function? Not incredibly useful now.
this is incredibly confusing :)
"I feel like this comment needs a bunch of expansion. The ""the hash codes are the same"" check here I guess is only mattering if _while_ we're building this list the entries are already being garbage collected, and having one garbage collected entry with the same hash code means just as little as two, so mind as well go with the more efficient one. (Ideally we'd trim it back down to none but that's harder.) But this can't ever be hit once we're doing the contains checks, since that would always be using the strong variant.

...right?"
Worst case: this results in 26.5 KB of static cached data that never gets cleaned up during the lifetime of the application. Is this acceptable? Is it worth pointing out as a comment?
"> What's the crossover point where cost of these operations is more expensive with the list?

Depends on the header names. Assuming a reasonable distribution of names (`string.Equals` can exit early), comparing `main` vs this `pr`, `main` is about equal at ~128 headers and faster after ~150. (Time to add all the headers and do one NonValidated enumeration)

The fact that we are searching for the key on every insertion by comparing it to existing keys, the worst-case scenario changes:
With a dictionary, the worst-case is `O(MaxResponseHeadersLength)` to hash all the inputs.
With the new approach, it's `O(MaxResponseHeadersLength * MaxNumberOfHeaders)` if you specifically craft long names of equal length that only change at the end. This makes a limit on the number of headers useful to put a ceiling on how much CPU malicious input could burn.
With the default `MaxResponseHeadersLength = 64k` and `MaxNumberOfHeaders = 128`, the worst-case on my CPU is about `1.6 ms`.

<details>
<summary>Worst-case benchmarks</summary>

| ResponseHeadersLengthKb | NumberOfHeaders |        Mean |
|------------------------ |---------------- |------------:|
|                      16 |              64 |    236.4 us |
|                      16 |             128 |    422.2 us |
|                      16 |             256 |    849.3 us |
|                      16 |             512 |  1,832.2 us |
|                         |                 |             |
|                      32 |              64 |    449.9 us |
|                      32 |             128 |    886.7 us |
|                      32 |             256 |  1,603.5 us |
|                      32 |             512 |  3,299.8 us |
|                         |                 |             |
|                      64 |              64 |    859.6 us |
|    64 = current default |             128 |  1,660.0 us |
|                      64 |             256 |  3,402.1 us |
|                      64 |             512 |  6,260.3 us |
|                         |                 |             |
|                     128 |              64 |  1,689.1 us |
|                     128 |             128 |  3,223.9 us |
|                     128 |             256 |  6,464.7 us |
|                     128 |             512 | 13,477.5 us |

</details>

If we feel that using more headers is realistic, there are ways to eliminate this worst-case as well.
For example, we could store the `Hash(name)` on the descriptor to make comparisons effectively `O(1)`. At that point, we could set `MaxHeaders` to 1000 and be happy.

> As part of this then are you proposing adding a knob?

If we believe using more than a few hundred is reasonable, I would prefer to remove the limitation via the above approach instead.

> I'm not sure why we get to set the threshold for what's a ""reasonable"" number of headers, considering there's effectively no limit today.

There are practical limits you will hit, e.g.
- Kestrel's limit of 100 headers
- Kestrel and Cloudflare's limit of 32 kB for all request headers
- HttpClient's limit of 64 kB on response headers
- IIS seems to have a ~16 kB limit
- Internet says Tomcat has a default of 8 kB

While most are configurable, I'd argue `HttpClient` shouldn't care about a scenario where someone wants to send 1001 headers :)

> What workaround does the client have if it needs to communicate with such a server it's already able to communicate with today?

LLHTTP. Or preferably changing their service to not rely on hundreds of headers.
"
"I can do that, however for the InvalidOperationException, it currently uses a clean ""IInterface"" string.  Whereas if I pull this into a helper, I need to either do string parsing on the generic type: ```typeName.Substring(0, typeName.IndexOf(""`"")``` to lop off the generic tilda garbage at the end or pass in a string value as an additional parameter.  Which would you prefer? "
"I believe the return of `parent_path()` is going to be a [temporary object](https://en.cppreference.com/w/cpp/language/lifetime#Temporary_object_lifetime) here, so it will still be destructed before the `get_hostfxr_path` call (resulting in garbage in `params.dotnet_root`)."
"we should zero out the `memory_details.numa_reserved_block_table` array so we don't get garbage values for the elements that we don't initialize below. "
"The other alternative, of course, is to use some sort of public domain text or a generator of junk/garbage text."
"Note: when new misspellings are added, the tool automatically annotates them. Which makes finding them trivial.

It's possible for the tool to print out samples for existing things, but I've worried about base64 or binary garbage, and for large output runs, it isn't a great idea to flood people.

My personal tooling from which this tool grew has `g` which I use to find instances...

I'm open to whether there should be a flag for samples (just one filename per token, or one line from one file). Would you want that? â€“ I'd imagine it'd be overwhelming..."
"I'll copy and paste something I recently shared in https://github.com/microsoft/vscode/issues/6328#issuecomment-877634046:

> I recently started a .NET 5 project and learned that the old way they did file nesting no longer worked. Seems now they use a configuration file .filenesting.json that has a proper schema and everything.
>
> I know this issue (and solution) has been seriously delayed but for compatibility with big brother Visual Studio and big sister .NET Core I'd suggest that a future solution follow the same pattern...

It seems to me a pull request to enable file nesting that follows this exact pattern **should** be easier for the maintainers to accept. This pattern is robust, documented, and in-use across .NET Core / .NET development.

If we can't get traction on this by taking action (and I commend you for making this PR) and can't make traction by offering monetary rewards (trust me, I've tried) - perhaps someone smarter than myself could look at the wizardy of the Synthwave package. Perhaps this is a way we can deliver an extension that modifies the explorer view without fully replacing it?"
"@github I found it!

Action bars are not disposed when a terminal is closed, they are cached in the list until you open a new terminal, so manually clear it to make it possible to garbage collect the closed terminal directly."
"This was the ""worst"" case of difference that was measured. 

```
1 / 1.12837917f                 == 0.886226892f
ReciprocalEstimate(1.12837917f) == 0.886352539f
```

This gives a difference of ~`0.000125647` in expected vs actual. The instruction documents a maximum error of `1.5 * 2^-12` which is ~`0.0003662109375` while scaling the `CrossPlatformMachineEpsilon` allows for an error of up to ~`0.000476837158`"
"A ""product name"" of `xamarin/xamarin-android@` avoids the need (responsibility) to pick a name, and since I'm being incredibly wishy-washy here, abdicating that responsibility sounds awesome.

â€¦or we just stick with `Xamarin.Android`. :-).

The hash remains useful, and should be extremely helpful for future issue investigations."
"reviewed in depth.  i think this is correct. however, it is definitely on the knife's edge of being really hard to maintain and get right.  this approach does have the virtue of being in line with how we did it before, with very little deviation, sot hat's a plus.

However, we could consider tearing this down (now or int he future) and just syaing: this code will presume that what it is seeing is a lambda, but will require the `=>` to follow.  And, if it doesn't have that, it's not a lambda.  That would likely make this a ton simpler, and would allow us to not have to do all the contortions around identifying and and considering all the ambiguity cases.   In a scannign world that was necessary.  In a parsing world it is not (though we might generate more garbage in common cases) :-/"
"Thanks for clarifying the behavior of the parser, it's quite clear now.
> `else if (addWhenInvalid && info.ParsedAndInvalidValues is null)`

With this change, just `TryGetValues_GetValuesForExistingHeader_ReturnsTrueAndListOfValues` is failing.
I will have some time to dig further into it this weekend, however worst case we can fall back to explicitly checking for CacheControl."
"Well, looks there's still a problem: https://helix.dot.net/api/2019-06-17/jobs/81c16505-2592-44b3-a03d-b8b70965016d/workitems/System.Runtime.Extensions.Tests/console

The results there appear to be garbage but are all convieniently in the range of valid BSF results. The only thing I can think of that would cause that is the emitter encoding the wrong operand.  @github any ideas how that could happen?"
"Ok so that gc assert went away but there is a different one still here (GC profiler tests, e.g. `getappdomainstaticaddress`):
```
CORECLR! MethodTable::GetFlag + 0x1A (0x00007ffd`620178fa)
CORECLR! MethodTable::HasComponentSize + 0x20 (0x00007ffd`6201c4a0)
CORECLR! WKS::my_get_size + 0x2A (0x00007ffd`628c263a)
CORECLR! WKS::gc_heap::walk_heap_per_heap + 0x1A1 (0x00007ffd`628d8e61)
CORECLR! WKS::gc_heap::walk_heap + 0x31 (0x00007ffd`628d8cb1)
CORECLR! WKS::GCHeap::DiagWalkHeap + 0x34 (0x00007ffd`6288b1e4)
CORECLR! GCProfileWalkHeapWorker + 0x291 (0x00007ffd`624b5c91)
CORECLR! GCProfileWalkHeap + 0x55 (0x00007ffd`624b59c5)
CORECLR! GCToEEInterface::DiagGCEnd + 0x27 (0x00007ffd`624b5157)
CORECLR! WKS::gc_heap::do_post_gc + 0x5A (0x00007ffd`628aa03a)
CORECLR! WKS::gc_heap::gc1 + 0xA34 (0x00007ffd`628b0d84)
CORECLR! WKS::gc_heap::garbage_collect + 0x550 (0x00007ffd`628b0260)
CORECLR! WKS::GCHeap::GarbageCollectGeneration + 0x17B (0x00007ffd`6288c17b)
CORECLR! WKS::GCHeap::GarbageCollectTry + 0xFD (0x00007ffd`6288c31d)
CORECLR! WKS::GCHeap::GarbageCollect + 0x2C4 (0x00007ffd`6288bf84)
CORECLR! ETW::GCLog::ForceGCForDiagnostics + 0x286 (0x00007ffd`626cb746)
CORECLR! ProfToEEInterfaceImpl::ForceGC + 0x371 (0x00007ffd`6260ecc1)
PROFILER! <lambda_522beb28ca2a0eee98a1f5349a50fa84>::operator() + 0x114 (0x00007ffd`69bca894)
PROFILER! std::invoke<<lambda_522beb28ca2a0eee98a1f5349a50fa84> > + 0x14 (0x00007ffd`69bc82e4)
```

This one doesn't look **GC Regions** specific and it still reproduces even if I just allocate a frozen segment but never allocate any objects in it (it's basically empty, zeroed)

UPD: it seems to go away when I don't set `ibAllocated = m_Size` https://github.com/dotnet/runtime/pull/49576/commits/89df356ef01ede890be110b91bdd7e7520b2f688 + https://github.com/dotnet/runtime/pull/49576/commits/3cb98e97196208a5099a94c1f5006f2e7246c614"
"> we'll see that the only generators that wrap the AttributedMarshallingModelGeneratorFactory

So this field is only needed when it is wrapped? I was debugging through that code path and it is incredibly complicated and we seem to be referring to each marshaller in a circular manner. At this point I am completely unsure of what is and isn't being used. Debugging this has also become very complicated due to all of the levels of indirection. I fear this is becoming more complicated than the original. The nesting itself makes the delegation difficult enough but add in the validator and it becomes - what is actually responsible in this case?"
"Tagging subscribers to 'arch-wasm': @github
See info in area-owners.md if you want to be subscribed.
<details>
<summary>Issue Details</summary>
<hr />

This PR introduces a JSSynchronizationContext that is installed automatically on the main thread when we initialize the bindings. It implements both Send and Post on top of emscripten's main thread call proxying API when you use it from workers, and on the main thread it just invokes the callback immediately.

I tried to keep the implementation as simple as possible without making it slow enough to produce a meaningful performance regression, since things like await will use this by default once it's merged. The queue of jobs is maintained in managed code and pumped by a dispatcher, and if we notice that the dispatcher is not currently waiting to run when we add something to the queue, we kick off a request to run the dispatcher. This means that worst-case we will run the dispatcher once per work item, but if a bunch of work items end up in the queue, they can get serviced efficiently by a single invocation of the dispatcher. And for calls from the main thread the performance should be equivalent to what it was before due to the fast-path.

<table>
  <tr>
    <th align=""left"">Author:</th>
    <td>kg</td>
  </tr>
  <tr>
    <th align=""left"">Assignees:</th>
    <td>kg</td>
  </tr>
  <tr>
    <th align=""left"">Labels:</th>
    <td>

`arch-wasm`, `area-System.Runtime.InteropServices.JavaScript`

</td>
  </tr>
  <tr>
    <th align=""left"">Milestone:</th>
    <td>-</td>
  </tr>
</table>
</details>"
"Note: something i like about the current system is that it's entirely stateless.  Which means that the provider and the languages are effectively singletons.  I like that that means we don't have to worry about that part of the system producing garbage, and can instead just focus on the individual service implementations being good."
"I had seen this earlier and convinced myself the eventual consistency would be fine because synchronization outside the API but required to use it properly would force consistency, preventing a reader from ever observing it in the inconsistent state. Now your comment made me think about it a lot more and reverse my opinion : )

I suspect this could be remedied doing the CompareExchange on the Next pointer of the final node, then do an unsynchronized write to update _last if the swap was successful. To find the final node a thread would need to start from _last and then iterate until Next == null. It is possible that _last will not point to the final node of the linked list, and it may not become eventually consistent, but that wouldn't interfere with the functional correctness of the API. At worst it creates a slight perf penalty for the next Add() operation."
"> No, because everything is already going away. Once that connection is severed, we're in a sort of Limbo where anything that tries to communicate across the bridge will throw an exception.

> The second the handle transitions to zero, anything we do in the renderer is at risk of throwing the exception. And none of it is necessary, because all of that stuff will be immediately garbage collected

Yea I guess that's why I'm saying we should sever everything that connects the xplat element to the managed android component thus limiting the possibility of anything reaching across the bridge. One the handle is zero and we're in limbo literally all connections from the renderer to the Element should be considered dangerous

For example all the IsDisposed checks should probably also be added to the tracker as well here yea?

https://github.com/xamarin/Xamarin.Forms/blob/92b1e07266c50798c00ff89e1a50a8d10e65b578/Xamarin.Forms.Platform.Android/VisualElementTracker.cs#L212

Which is where I was going with saying that the tracker should be cleaned up once we detect the handle going to zero. 

Android Handle Goes to Zero => all connections to Element are severed instantly 
And then if possible whatever native events and listeners are removed but that might not be possible once a class is in limbo....

So the check in property changed would be something like
```C#
if (this.IsDisposed())
{
	this.Cleanup(); //this removes property handler and whatever else it can
	_tracker.Cleanup() // this removes any connections the tracker has to element or tracker just has a similar IsDisposed check that calls Cleanup()
	return;
}
```


> The only reason we unhook all that stuff during a deterministic dispose is to help the garbage collector.

Yea which is why in one version of the conversation I'd almost recommend that we just completely get rid of dispose calls and turn them all into CleanUp calls unless it's for something large like images.  All dispose does is save us an extra pass of the GC and I don't feel like enough things get disposed that this is helpful overall and it's more problematic to not just let the GC do it naturally. But I think that's a different discussion, a different PR, and something that needs additional measurements  :-) 


Are there scenarios where deterministic dispose doesn't get called and dispose is only going to be called by the finalizer?"
Maybe code got garbage-collected?
"this unfortunatley isn't enough.  There are other constant locations (like attribute arguments).  Until C# has constant interpolated strings, this wouldn't work there.

*That said*.  This is a refactoring, so the worst that will happen is the user will be offered something non-viable.  THat's not the end of the world.   We do have a helper though that can be useful here called IsConstantExpressionContext.  You could use this to make sure this feature isn't offered in those places."
"you have to initialize this, otherwise it's filled with garbage"
This is seriously BODGY too.
"I donâ€™t actually know if thereâ€™s a way to disable the event completely on the receiver side. We already have throttle_func as sort of â€œdisablingâ€ it, but thatâ€™s not good enough. The WinRt event is just so expensive that we canâ€™t afford. 

I think @githubâ€™s idea is actually how UI framework usually works, for example thatâ€™s basically how â€œRunloopâ€ in iOS works. The problem here is that we have two independent â€œframeworkâ€ here, one being the DxRenderer and related, the other being XAML. And the main way for them to communicate was through WinRT events. I see this as another consequence of tight-coupling of renderer.cpp (where the render thread lives) and DxEngine. After #10615 (seriously guys, take a look and you can see how effective that PR is) and future PR lands, we should be able to use the â€œhooking into the render threadâ€ way to do something interesting. Of course the UI thread limitation in XAML still need to be taken into consideration. "
"```suggestion
                _ASSERTE(!""Unknown AltJitOs, it has to be either Windows, Linux or MacOS"");
                targetOs = getClrVmOs();
```

Otherwise we will pass garbage in in retail builds, I guess. Maybe just initialize it above instead? `getClrVmOs` looks free."
"I don't think so. The worst that will happen is that you won't be happy with the shape of the file names appearing in the compiler output.
"
"> what is O(2^n)

The worst case for backtracking."
"You're reading it correctly.  My understanding is the goal is to improve the algorithmic complexity of the worst case,  but in doing so it makes most other cases measurably slower.  I agree with @github this is not something we should take,  but I'd like your opinion as well based on your knowledge of use cases for the library. "
"Assuming the new elevation may be different than a previously recorded elevation for a given `i`: 

Instead of inserting, breaking the strictly increasing invariant of `_minimumElevation`, and then looping over everything could we be smarter? For example:
* If the new value is smaller than we change it to be equal to the current value. 
* If the new and old value are the same then do nothing. 
* If the new value is larger than we iterate forward increasing values. 

Assuming this function is called in a loop for every `i` in random order than this approach is O(n^2) in the worst case (`i` is decreasing but the integer at `i` is always greater than the previous one). That's already a bummer..."
"The original issue this test was written for (https://bugzilla.xamarin.com/show_bug.cgi?id=43941) was a memory leak of the ContentPage. The problem was that the counter could _never_ get to zero. 

As long as the instance count is zero at the end of the test, we don't care what the counter values were in the interim. So there's no reason to keep the list around to prevent garbage collection; garbage collection is the thing we _want_ to happen."
"Actually, that's a good point. You're right -- `%{buildroot}` is by convention used without a trailing slash (so that it doesn't become the root directory if it's unset). But `%{_datadir}` should be followed by a slash, since at worst it's redundant. Thanks!"
"Yea, we're guaranteed to find one. In the very worst case, _this process_ is also registered to be a monarch, so if _every other process dies in succession_, but this process is still running, then _eventually_, we'll settle on _us_. So if this code is still running, then there's a process that's out there that could be the monarch. That's the fundamental idea with all these loops. I think in practice, you'd have to be closing windows _inhumanly fast_ to iterate more than once."
"The hash function we use through the STL (FNV1a) makes that quite unlikely however.
`std::unordered_set` is about ~2x slower than `std::vector` for such worst cases IIRC (with small data sets).

I'm fairly stoked on adding `robin_hood::unordered_map` soon, as it uses a plain vector as the underlying container."
"I really don't care what we set the colors too, as long as it reduces the bug reports, and I'm not sure this helps that much. My main concern is that we're going to be swapping one set of complaints for another. And while I don't mind telling people that don't like Solarized to just not use it, I don't know what to tell people that do actually like Solarized when they ask why we've deliberately broken their color scheme.

That said, if the real Solarized users are a small minority, then maybe this is the least worst option. Your metrics should be able to answer that question."
"> Wow this is an incredibly creative way of tinkering with the tables inside user32. Props to the guy who discovered it.

Oh. That link is to an archive of Michael S. Kaplan posts who I believe worked at Microsoft behind this stuff. So that really explains why it's the right answer then, huh? :)

"
"probably the worst example of clang auto-formatting ðŸ˜„ "
"@github I agree that the ""well-behaved"" assumption would allow us to do some codegen optimizations. 
That would be consistent although we didn't explicitly discuss that in LDM. We'd probably want to bring it up if we want to explore seriously.
Such codegen optimizations seem secondary to addressing the exhaustiveness/subsumption analysis (ie. they would be *allowed* but not *necessary*).

What scenarios might we want to optimize?
- scenarios where the slice is captured (for positional or Type or designation pattern) cannot be optimized, because we don't generally know how to cook up a slice even if we've already fetched all the items.
- scenarios where the slice isn't captured (like `{1, ..{2, ..p}, 3}`) aren't worth optimizing in my opinion. I would keep the ""natural"" codegen implied by the code (ie. calling `Slice`) and leave flattening the list patterns up to the user.

---
In reply to: [856929120](https://github.com/dotnet/roslyn/pull/53891#issuecomment-856929120) [](http://example.com/codeflow?ancestors=856929120)

---
In reply to: [856929120](https://github.com/dotnet/roslyn/pull/53891#issuecomment-856929120) [](http://example.com/codeflow?ancestors=856929120)"
"Overall CI looks good, but it seems that Frozen Objects don't play nice with GC regions, e.g. my ""frozen"" string asserts here:
```
>	coreclr.dll!WKS::gc_heap::get_region_gen_num(unsigned char * obj) Line 11322	C++
 	coreclr.dll!WKS::gc_heap::object_gennum(unsigned char * o) Line 11025	C++
 	coreclr.dll!WKS::gc_heap::ephemeral_pointer_p(unsigned char * o) Line 7756	C++
 	coreclr.dll!WKS::GCHeap::IsEphemeral(Object * object) Line 44694	C++
 	coreclr.dll!SyncBlockCache::GCWeakPtrScan(void(*)(Object * *, unsigned __int64 *, unsigned __int64, unsigned __int64) scanProc, unsigned __int64 lp1, unsigned __int64 lp2) Line 1135	C++
 	coreclr.dll!GCToEEInterface::SyncBlockCacheWeakPtrScan(void(*)(Object * *, unsigned __int64 *, unsigned __int64, unsigned __int64) scanProc, unsigned __int64 lp1, unsigned __int64 lp2) Line 49	C++
 	coreclr.dll!GCScan::GcWeakPtrScanBySingleThread(int condemned, int max_gen, ScanContext * sc) Line 133	C++
 	coreclr.dll!WKS::gc_heap::mark_phase(int condemned_gen_number, int mark_only_p) Line 26159	C++
 	coreclr.dll!WKS::gc_heap::gc1() Line 20758	C++
 	coreclr.dll!WKS::gc_heap::garbage_collect(int n) Line 22519	C++
 	coreclr.dll!WKS::GCHeap::GarbageCollectGeneration(unsigned int gen, gc_reason reason) Line 46299	C++
 	coreclr.dll!WKS::gc_heap::trigger_gc_for_alloc(int gen_number, gc_reason gr, WKS::GCDebugSpinLock * msl, bool loh_p, WKS::msl_take_state take_state) Line 17484	C++
 	coreclr.dll!WKS::gc_heap::try_allocate_more_space(alloc_context * acontext, unsigned __int64 size, unsigned int flags, int gen_number) Line 17630	C++
 	coreclr.dll!WKS::gc_heap::allocate_more_space(alloc_context * acontext, unsigned __int64 size, unsigned int flags, int alloc_generation_number) Line 18102	C++
 	coreclr.dll!WKS::gc_heap::allocate(unsigned __int64 jsize, alloc_context * acontext, unsigned int flags) Line 18133	C++
 	coreclr.dll!WKS::GCHeap::Alloc(gc_alloc_context * context, unsigned __int64 size, unsigned int flags) Line 45256	C++
 	coreclr.dll!Alloc(unsigned __int64 size, GC_ALLOC_FLAGS flags) Line 227	C++
 	coreclr.dll!AllocateSzArray(MethodTable * pArrayMT, int cElements, GC_ALLOC_FLAGS flags) Line 479	C++
 	coreclr.dll!JIT_NewArr1(CORINFO_CLASS_STRUCT_ * arrayMT, __int64 size) Line 2625	C++
```
if I disable GC regions it works fine. I verified that the input object for that `get_region_gen_num` is indeed coming from the frozen segment

cc @github @github (related discussion: https://github.com/dotnet/runtime/pull/59283#issuecomment-1006955877)

Wonder if NativeAOT hit this too

UPD: wonder if https://github.com/dotnet/runtime/pull/49576/commits/28e03312409eb93ce2ce7da261d1b8ad593ebaa7 fixes it"
":up::date: though I doubt tests will work without further changes. I'm building locally to see if I can get more information about template failures in previous builds. Suspect we have additional places where TFMs get mixed up with package versions.

Worst case, we might need a runtime update containing dotnet/runtime#40950. I triggered that (daily) subscription just in case."
"No. Mocks are challenging for maintainability because they do not show up as implementations of classes and interfaces, so they are prone to unintended deviations from adherence to API post-conditions. Loose mocks are the worst case scenario because they also don't show up as _references_ to the API which changed.

Stubs are preferred to mocks because they are true implementations of the API, but one step at a time..."
This should be good. If there are rules that don't match anything. The worst I think that can happen is that it will be a warning - but I don't think that reaches the user...
"ah. Yes. Like lambda but v2 hahaha. But seriously though, can you give this one a more descriptive name too?"
"1. Is anybody currently creating an array of this type?  It doesn't appear to be the case.
2. Do we need to support incredibly suspicious behavior for something we're not using?

Accidentally copying a value, as-is, will result in double-frees:

```cpp
simple_pointer_guard<uint8_t[]>  buf (new uint8_t[cd_size]);
auto oops = buf;
// *boom*
```

If we don't need it, we should remove it.  If we do need it, it needs to be designed appropriately.  What we have isn't designed, and will break badly when accidentally misused."
My guess here is GC is garbage collection. Looks was referring the passed `pBuffer` is not managed memory anyway. but of course I am not sure. I think we can just delete this line.
"Well, this was an interesting experiment, but after finishing the sort code, I found the best performance test is our type baselines. I just ran `time gulp runtests-parallel` and looked at the amount of CPU time spent. It's 10% slower even skipping the less-important-to-sort types. So I don't think this is a viable approach. Probably the only thing that would work is to sort during union creation.

Note that the type baselines are a worst-case test of performance. Lots of projects wouldn't slow down that much, but really big ones with a lot of types generated across modules would resemble the type baselines. "
"That would be reasonable. There's no need to finish parsing, the rest is garbage anyway."
it is seriously weird to try to figure out ```default(SyntaxTokenList) : null as SyntaxTokenList?```.  Is this just:   ```v.Parent is ForStatementSyntax ? default(SyntaxTokenList) : default(SyntaxTokenList?)```?
"But we know from experience that these sort of UI blocking things *do* seriously impact the experience.  That's why we worked so hard in so many components to make them non-blocking.  

Can you point to any discussions where we *both* decided we wanted to go JTF *and* we now decided it was ok to do UI thread work in these cases?

> all other VS components does ths all over the place, so I dont think we don't doing it will move any niddle.

This is not a good reason.  Yes.  Lots of components do this.  Which is why a lot of times there are unnecessary UI pauses.  We shouldn't be compounding that by doing that ourself.  We should be good citizens and we should avoid UI work as much as we can.  Note that i tried this sort of thing with the Nuget work, and it was *actively* bad.  Like nearly unusable at times because of the cost of those UI hiccups interrupting things.  We know the project system can have *serious* perf costs associated with it.   For example, it can cause *full* design time builds (so basically tens of seconds or more).  We should be striving *as much as possible* to ensure we dn't hit the UI thread, not moving back to the world where we do it again, just because other components are also bad.

--

As an aside, roslyn spent literally 10+ years working toward these goals in this space.  Rolling them back should never be done haphazardly in a PR.  Instead, it should be something the team agrees to (and agreeing to use JTF doesn't mean we're agreeing to do UI work in a refactoring), and then we methodically measure and test.

It would be irresponsible to just make this change here with the statement that it's ok because we decided on JTF, or because other teams do it.  We know how bad UI hits are.  So we should do the work correctly from teh start on our end, and not just start rolling back all this hard work and important efforts we worked so hard on for so long."
"really? this is nothing compared to what I'm planning =)
seriously, it's just (exactly) a couple of property patterns, I don't know how else you could use patterns at all"
"Garbage. Also, renaming color -> semiTransparentColor as it's more meaningful and explains what I'm doing there"
"nit: Consider refreshing the comment in light of our discussion today. Maybe something like:

// If the comparison is amenable:
// We take the when-not-null state from visiting the LHS and use that as starting point to visit the RHS ignoring diagnostics. The resulting state is the when-true state resulting from the comparison.
// We take the state (ie. worst case state) from visiting the LHS and use that as starting point to visit the RHS for diagnostics and recording nullabilities for public APIs. The resulting state is the when-false state resulting from the comparison. #Closed"
"Hi @github 

```
diff --git a/src/coreclr/src/vm/arm64/asmhelpers.S b/src/coreclr/src/vm/arm64/asmhelpers.S
index a8b0a7c..f76a748 100644
--- a/src/coreclr/src/vm/arm64/asmhelpers.S
+++ b/src/coreclr/src/vm/arm64/asmhelpers.S
@@ -375,7 +375,7 @@ WRITE_BARRIER_ENTRY JIT_WriteBarrier
     // Update GC Shadow Heap
 
     // Do not perform the work if g_GCShadow is 0
-    ldr  x12, LOCAL_LABEL(wbs_GCShadow)
+    PREPARE_EXTERNAL_VAR g_GCShadow, x12
     cbz  x12, LOCAL_LABEL(ShadowUpdateDisabled)
 
     // need temporary register. Save before using.
```

But still repos:

```
./build-runtime.sh

export CORE_LIBRARIES=/home/zhaixiang/runtime/.dotnet/shared/Microsoft.NETCore.App/5.0.0-preview.8.20361.2
export COMPlus_HeapVerify=2

gdb -ex=r --args /home/zhaixiang/runtime/artifacts/bin/coreclr/Linux.arm64.Debug/corerun /home/zhaixiang/coreclr-mips64-dev/bin/tests/Linux.arm64.Debug/GC/API/NoGCRegion/NoGC/NoGC.exe



Assert failure(PID 2648 [0x00000a58], Thread: 2648 [0x0a58]): !""Pointer updated without using write barrier""
    File: /home/zhaixiang/runtime/src/coreclr/src/gc/gc.cpp Line: 39195
    Image: /home/zhaixiang/runtime/artifacts/bin/coreclr/Linux.arm64.Debug/corerun


Thread 1 ""corerun"" received signal SIGTRAP, Trace/breakpoint trap.
DBG_DebugBreak () at /home/zhaixiang/runtime/src/coreclr/src/pal/src/arch/arm64/debugbreak.S:7
7	    EMIT_BREAKPOINT
(gdb) bt
#0  DBG_DebugBreak () at /home/zhaixiang/runtime/src/coreclr/src/pal/src/arch/arm64/debugbreak.S:7
#1  0x0000007fb770cc9c in DebugBreak () at /home/zhaixiang/runtime/src/coreclr/src/pal/src/debug/debug.cpp:410
#2  0x0000007fb750bcd4 in DbgAssertDialog (szFile=0x7fb7879a20 ""/home/zhaixiang/runtime/src/coreclr/src/gc/gc.cpp"", iLine=39195, szExpr=0x7fb787e994 ""!\""Pointer updated without using write barrier\"""") at /home/zhaixiang/runtime/src/coreclr/src/utilcode/debug.cpp:697
#3  0x0000007fb74b7be4 in WKS::testGCShadow (ptr=0x7f0ffff478) at /home/zhaixiang/runtime/src/coreclr/src/gc/gc.cpp:39195
#4  WKS::testGCShadowHelper (x=0x7f0ffff470 ""\260\274l>\177"") at /home/zhaixiang/runtime/src/coreclr/src/gc/gc.cpp:39212
#5  0x0000007fb7488050 in WKS::checkGCWriteBarrier () at /home/zhaixiang/runtime/src/coreclr/src/gc/gc.cpp:39240
#6  0x0000007fb7487870 in WKS::gc_heap::garbage_collect (n=0) at /home/zhaixiang/runtime/src/coreclr/src/gc/gc.cpp:18093
#7  0x0000007fb74704e0 in WKS::GCHeap::GarbageCollectGeneration (this=0x55555c45f0, gen=0, reason=reason_alloc_soh) at /home/zhaixiang/runtime/src/coreclr/src/gc/gc.cpp:37747
#8  0x0000007fb74722cc in WKS::gc_heap::trigger_gc_for_alloc (gen_number=0, gr=reason_alloc_soh, msl=0x7fb7b7dce8 <WKS::gc_heap::more_space_lock_soh>, loh_p=false, take_state=WKS::mt_try_budget) at /home/zhaixiang/runtime/src/coreclr/src/gc/gc.cpp:13852
#9  0x0000007fb7473640 in WKS::gc_heap::try_allocate_more_space (acontext=0x55555ebd28, size=944, flags=0, gen_number=0) at /home/zhaixiang/runtime/src/coreclr/src/gc/gc.cpp:13975
#10 0x0000007fb7473924 in WKS::gc_heap::allocate_more_space (acontext=0x55555ebd28, size=944, flags=0, alloc_generation_number=0) at /home/zhaixiang/runtime/src/coreclr/src/gc/gc.cpp:14476
#11 0x0000007fb74b30a8 in WKS::gc_heap::allocate (jsize=939, acontext=0x55555ebd28, flags=0) at /home/zhaixiang/runtime/src/coreclr/src/gc/gc.cpp:14507
#12 WKS::GCHeap::Alloc (this=0x55555c45f0, context=0x55555ebd28, size=939, flags=0) at /home/zhaixiang/runtime/src/coreclr/src/gc/gc.cpp:36742
#13 0x0000007fb726ddd8 in Alloc (size=939, flags=GC_ALLOC_NO_FLAGS) at /home/zhaixiang/runtime/src/coreclr/src/vm/gchelpers.cpp:228
#14 0x0000007fb726c440 in AllocateSzArray (pArrayMT=0x7f3e8d9cc0, cElements=915, flags=GC_ALLOC_NO_FLAGS) at /home/zhaixiang/runtime/src/coreclr/src/vm/gchelpers.cpp:483
#15 0x0000007fb729e434 in JIT_NewArr1 (arrayMT=0x7f3e8d9cc0, size=915) at /home/zhaixiang/runtime/src/coreclr/src/vm/jithelpers.cpp:2723
#16 0x0000007fb729df04 in JIT_NewArr1VC_MP_FastPortable (arrayMT=0x7f3e8d9cc0, size=915) at /home/zhaixiang/runtime/src/coreclr/src/vm/jithelpers.cpp:2626
#17 0x0000007f3e5c9490 in ?? ()
#18 0x0000007f10015678 in ?? ()
Backtrace stopped: previous frame inner to this frame (corrupt stack?)
(gdb) x/22i 0x0000007f3e5c9490-44
   0x7f3e5c9464:	ldr	x2, [x2]
   0x7f3e5c9468:	ldr	x2, [x2,#72]
   0x7f3e5c946c:	ldr	x2, [x2,#56]
   0x7f3e5c9470:	blr	x2
   0x7f3e5c9474:	str	w0, [x29,#60]
   0x7f3e5c9478:	ldr	w1, [x29,#60]
   0x7f3e5c947c:	sxtw	x1, w1
   0x7f3e5c9480:	mov	x0, #0x9cc0                	// #40128
   0x7f3e5c9484:	movk	x0, #0x3e8d, lsl #16
   0x7f3e5c9488:	movk	x0, #0x7f, lsl #32
   0x7f3e5c948c:	bl	0x7f3e57dfc0
   0x7f3e5c9490:	str	x0, [x29,#48]
   0x7f3e5c9494:	ldr	x0, [x29,#48]
   0x7f3e5c9498:	str	x0, [x29,#72]
   0x7f3e5c949c:	ldr	w0, [x29,#84]
   0x7f3e5c94a0:	ldr	w1, [x29,#88]
   0x7f3e5c94a4:	cmp	w1, #0x0
   0x7f3e5c94a8:	b.ne	0x7f3e5c94b0
   0x7f3e5c94ac:	bl	0x7f3e57dfa0
   0x7f3e5c94b0:	cmn	w1, #0x1
   0x7f3e5c94b4:	b.ne	0x7f3e5c94c8
   0x7f3e5c94b8:	cmn	w0, w0
```

Please point out my fault.

Thanks,
Leslie Zhai"
"There needs to be a check here that if isFinalBlock, make sure we've consumed all of the data. There could be garbage at the end of the form."
"We don't mock the networking stack part of the sockets transport. There's just no need to reuse things in the face of exception, but I'll add a comment.
- Another successful send will replenish the pool
- The currently rented SocketAsyncEventArgs will be disposed (higher up the stack) (SocketConnection.Start)
- The SocketAsyncEvents has a finalizer in the *worst* case scenario where this code becomes buggy in the future ðŸ˜„ 

I don't see the value in making sure this gets disposed by trying to mock the SocketSender. We don't have tests making sure that the Socket itself is disposed."
">= new CachedAssemblySymbolList() [](start = 65, length = 32)

Consider moving this to an `else` in the private constructor and explicitly initializing in the other constructors to avoid generating unnecessary garbage."
"That is what I was saying the description above:

```
The search handles are a little bit different as it needs to include the source string we need to search in and the pattern string we need to search for. This require the search handle should be used exclusively and not be shared between 2 operations in same time.
```

The worst case when having a raise condition, we'll just create a new search handle as we already doing today."
"> Hey @github thanks for your patience on this one! I've been looking at it for a bit and seems all OK to me. If you have been digging into the more complex scenarios, feel free to do a follow up PR. Thanks for all your time and efforts on this one!

Hi @github, yes I have been doing much more and it has been time consuming. I could not wait for this to be merged and actually did not most of the Shell advantages so in the mean time I have rewritten our application without the Shell. I have found that also not all pages are being GC'd. I though I would provide another PR but have found this - once you set `NavigationPage`/`Shell`.`TitleView` to any ""complex"" view (`StackLayout`, `Grid`, etc.) your pages will not be GC'd in **Debug build**. In Release build they eventually will be, but you have to make sure to do a clean and then build (I also manually removed the `obj` folder and uninstalled the application from target device just to be sure, this method worked) every time, otherwise this might not work. That process is quite slow as you might imagine. Our CI creates new build when we deploy our app so this is not a real problem in production but it is quite painful to debug. Also this problem starts from XF 5.0.0.2021 so my guess is it is related to XAML hot reload. I don't know how would I troubleshoot hot reload issues so I eventually gave up.

Summary - `ContentPage`s are not garbage collected if you:

* Use Xamarin Forms 5.0.0.2012 or higher, and
* use Debug build, and
* set `TitleView` to any `Layout` or other complex `View`

In Release build they should be but make sure you create clean build to be sure.

@github Since we don't use XF Shell any more I will not test changes in this PR in production but I think that you can close issue #14657, if GC releated issues are okay in debug build."
"I don't want to compare against only `Encoding.UTF8` because it doesn't hit the case where somebody passed `new UTF8Encoding(/* ... */)`. Both of these patterns are very common in .NET code. I feel that if we special-case the former we should also special-case the latter.

In reality, what I care about is that: (a) the implementation says it's UTF-8; and (b) if an error is encountered, the implementation will either throw or emit no more than a single character like `'?'` or `'\uFFFD'`. If both of these are true, then I know the worst-case possible expansion of the input data is 1 char -\> 3 bytes. (If the implementation claims to be UTF-8 but actually does something else to violate the above assumption, bad things happen.)

In an ideal world I could call `Encoding.GetMaxByteCount(1)`, compare it against 3 to set the ""fast-path"" flag, and call it a day. But `Encoding.UTF8.GetMaxByteCount(1)` weirdly returns 6 instead of 3 for historical reasons. A more straightforward way to check this would be to say `if (e is UTF8Encoding /* matches subclasses as well */ && e.EncoderFallback.MaxCharCount <= 1)`, but per an earlier suggestion from @github a few months back it seems like querying the virtual `CodePage` property may be more reliable than checking the type hierarchy."
"These are APIs that are approved but not implemented on System.Math or System.MathF

Ideally we get them also implemented and exposed for .NET 7, but the ""worst case"" is they become DIMs in .NET vNext"
"This seems like a good approach assuming that the allocated byte count is reliable.

I could be wrong but I don't think we actually need the NoGC region because GetAllocatedBytesForCurrentThread() does not go down after a garbage collection (it returns the number of bytes that have *ever* been allocated). Does that sound right?"
"Alright done with the review. Blocking over concerns for the XML serialization issue, but overall this does seem fine given the direction it's trying to take. My biggest concerns are mostly high level:

1. Do we need a doc for a newbie to our repository to understand how this works? This PR does a great job using type safety and analyzers to keep some of the worst mistakes from happening, but I'd worry that the ""how to use options"" might not be obvious for somebody starting out, since we're more or less abandoning our public surface area. If somebody is trying to something that matches what's been done before it might be easy to just copy/paste and follow the pattern, but any ""newer"" code is decidedly less obvious how to write.
2. How do we test this safely? There's a number of IVT risks as well as a few places where a subtle change could pretty easily break something. Do we have a good plan in place?
3. What's the next steps? We're still tracking thinking of possible better approaches, or there a good list of next steps? This moves us forward, but at the same time still feels like we're creating debt that needs to be cleaned up...

But let's go with it, assuming we've tested. :smile:"
"Further reading now leads me to believe that finalizers don't run ""during"" a GC where the runtime might suspend threads, but that finalization merely triggered by the collection adding objects-waiting-to-be-finalized to the finalizer queue. So the object being finalized is still alive as far as GC is concerned until the object is dequeued from the finalizer queue by the finalizer thread and the object is no longer referenced (that is *if* the object is no longer referenced). When collections are actually run, even the finalizer thread is suspended at least if concurrent garbage collection is disabled.

This is making me think that the effort we go through to reconstruct the MemoryPoolBlock might not be worth it, and that we should just add `this` back to the pool in the finalizer and ""resurrect"" the MemoryPoolBlock. We'd probably need call GC.ReRegisterForFinalize though.

I'm not writing this long explanation because I'm confident I'm correct. I just want to explain my current understanding so the experts can clear up what I'm wrong about."
"honestly I think this is actually more clear and simpler, IsRefStruct was not used anywhere else and force garbage values to be assigned."
"Wait, seriously? Frick, just put it back on the AcceleratorKey then. Just so long as it can be read."
"I think it might be helpful to come up with code-snippets that should not leak, but do.

As far as I understand, the original problem is this failing test:

```ts
test('Emitter dispose should clean up leaking listeners', () => {
    const emitter = new Emitter<number>();
    const evenNumbers = filter(emitter.event, n => n % 2 === 0);

    const disposable = evenNumbers(n => { /* NO OP */ });

    const obj = { value: 0 };
    const objWR = new WeakReference(obj);
    evenNumbers(n => obj.value = n);

    emitter.dispose();

    GC.collect(); // assuming such a function exists

    // Fails!
    // `disposable` is kept alive and keeps the hidden emitter alive, which keeps all other event subscriptions alive, including `obj`.
    assert(!objWR.hasValue); 

    keepAlive(disposable);
});
```

Note that this is not really a leak per se, as `disposable` itself leaks through `keepAlive`!
Without `keepAlive`, the garbage collector would just clean up everything.
I failed to come up with a code-snippet that increases memory usage while a reviewer might think the code-snippet would not.


However, this PR would have caused this real memory leakage:

```ts
test('Registering an handler und unregistering it should not leak', () => {
    const emitter = new Emitter<number>();

    // Fails!
    // This code snippet strictly increases the memory usage
    assertNoHeapGrowth(() => {
        const evenNumbers = filter(emitter.event, n => n % 2 === 0);

        const log: number[] = [];
        const disposable = evenNumbers(n => log.push(n));
        disposable.dispose();
    });
});
```

`assertNoHeapGrowth` could look like this (pseudo code):
```ts
function assertNoHeapGrowth(fn: () => void) {
    GC.collect();
    const initialSize = GC.heapSize;
    fn();
    GC.collect();

    assert(GC.heapSize <= initialSize);
}
```"
"I think we should insert:

```cc
if (value) {
    *value = NULL;
}
```

so that if the path cannot be opened, `*value` isn't potentially left as garbage."
"### Discussion points
1. How do we feel about `Enter` being bound to `copy()` by default?
   - It's consistent with conhost, and worst-case, people can disable it. I don't expect people to bind `Enter` to anything nowadays anyways.
2. Should we backport a fix for the copy on select stuff?
   - It's a decently sized fix and nobody's complained about it. Ethically, we should, but there is _some_ risk due to the size. I'd basically copy over anything that references `IsInQuickEditMode()`.
3. Also found another ""bug?"" where if you select empty text while copy on select is enabled, we end up copying literally nothing. I'm guessing this is because we switched the default `copy()` behavior to trimming whitespace. Should that be a part of this PR?
   - I vote no. I'm also not sure if we should even commit to fixing this for 1.14 and 1.15. But I could 100% be wrong. Either way, bringing it to your attention.

CC @github as my favorite `copyOnSelect` user who definitely has opinions on this stuff haha
"
"Ok, then we throw my comments as garbage :)"
"This is a good question ðŸ‘ 

This test code was mostly copied/modified from here: https://github.com/whitneyschmidt/xamarin-macios/blob/d3f0916cdb5a43ad1a18d7271cbe19603e004a07/tests/monotouch-test/Foundation/UrlTest.cs

The `using` ensures that the variable inside the method is disposed as soon as possible.

From my understanding (@github + @github please correct as needed ðŸ˜† ) if we didn't wrap the variable in `using`, it would be picked up for disposal by the garbage collector, which we have no control over.

I think we don't need to worry about `nullFormat` because there's no object created in that variable's assignment and the value is never modified so there's no potential for weirdness with memory management.

The choice to make format a variable inside of individual test methods comes from the other test, and fwict we try to stick to the policy of making sure things are disposed asap in testing code (even if it is not strictly necessary).

Maybe someone else can give an example of a bug that might occur w/o `using` to handle disposal?"
"I can add an assert here that would catch if we ever change the behavior to return different instances (I added a test for this in Diagnostics tests already).

This optimization isn't incredibly important, but we may choose to make similar ones in the future. For example, we could avoid allocating an `Activity` per request when `NoOutputPropagator` is used and there are no other listeners present.

I think the broader question here is whether we should be relying on implementation details like this in libraries. We already do (e.g. Roslyn's tratment of `static ROS<byte> => new byte[]` / when making decisions on which things to cache), but is that more of an exception to the rule?"
"This code contains zero loops (and zero generics and zero overloads), however, which is why this code, specifically, being problematic to us seems so odd to me. I definitely understand the desire to have some sort of graceful-failure-like hard cap on work done here, however I don't think this example is a good justification for it - the actual complexity of the given code is relatively low; it just currently triggers an unoptimized worst case (deep chains of apparently dependent statements with uncached checks on them). IMO, that itself merits some fixing, at which point this test won't be that useful for testing this limiter. :(

Although maybe this is also just a good example of why we should have a merge-able hierarchical cache; we call `getTypeOfExpression` with caching disabled in control flow because we don't know if we're in an inferential/contextual context or not, and caching control flow results in those contexts does bad things to the types we produce - if we could simply ""always cache"" and have call resolution merge/toss the cache layer as appropriate we'd _probably_ not have this problem, at least in this case.

Really, the heart of it is that while I believe the limiter is probably needed for more graceful failure modes, I don't think it's really a ""fix"" - error'ing on _completely valid_ input code should be a last resort for us, yeah? To be used when resources are truly stretched thin? This is just a poignant example of something we need to optimize, especially when the input code, to a human like myself, seems so simple."
"> I'm not sure if we could call this change an ""optimization"" unless there are demonstrable performance improvements in benchmarks.

Here's the source code of the benchmark https://github.com/dotnet/performance/commit/eb01ff12c7d35f6d2148df45a7d0d1dfbebb67dc. I do benchmark for the worst case which is when collection is not complete. The opposite case will have the same results I guess, my change is not supposed to impact it

* Test scenario `AddRemoveFromDifferentThreads<T>.BlockingCollection` :
  * Without double check :
```
AddRemoveFromDifferentThreads<Int32>.BlockingCollection: Job-RYUCKU(PowerPlanMode=00000000-0000-0000-0000-000000000000, Toolchain=CoreRun, InvocationCount=1, IterationTime=250.0000 ms, MaxIterationCount=20, MaxWarmupIterationCount=10, MinIterationCount=15, MinWarmupIterationCount=6, UnrollFactor=1, WarmupCount=-1) [Size=2000000]

BenchmarkDotNet=v0.13.1.1786-nightly, OS=Windows 10 (10.0.19044.1706/21H2/November2021Update)
Intel Core i7-10875H CPU 2.30GHz, 1 CPU, 16 logical and 8 physical cores
.NET SDK=7.0.100-preview.1.22110.4
  [Host]     : .NET 7.0.0 (7.0.22.7608), X64 RyuJIT
  Job-RYUCKU : .NET 7.0.0 (42.42.42.42424), X64 RyuJIT

PowerPlanMode=00000000-0000-0000-0000-000000000000  Toolchain=CoreRun  InvocationCount=1
IterationTime=250.0000 ms  MaxIterationCount=20  MaxWarmupIterationCount=10
MinIterationCount=15  MinWarmupIterationCount=6  UnrollFactor=1
WarmupCount=-1

|             Method |    Size |     Mean |    Error |   StdDev |   Median |      Min |      Max |      Gen 0 |     Gen 1 |     Gen 2 | Allocated |
|------------------- |-------- |---------:|---------:|---------:|---------:|---------:|---------:|-----------:|----------:|----------:|----------:|
| BlockingCollection | 2000000 | 400.9 ms | 15.39 ms | 17.72 ms | 406.5 ms | 367.3 ms | 422.7 ms | 30000.0000 | 1000.0000 | 1000.0000 | 260.14 MB |

AddRemoveFromDifferentThreads<String>.BlockingCollection: Job-RYUCKU(PowerPlanMode=00000000-0000-0000-0000-000000000000, Toolchain=CoreRun, InvocationCount=1, IterationTime=250.0000 ms, MaxIterationCount=20, MaxWarmupIterationCount=10, MinIterationCount=15, MinWarmupIterationCount=6, UnrollFactor=1, WarmupCount=-1) [Size=2000000]

BenchmarkDotNet=v0.13.1.1786-nightly, OS=Windows 10 (10.0.19044.1706/21H2/November2021Update)
Intel Core i7-10875H CPU 2.30GHz, 1 CPU, 16 logical and 8 physical cores
.NET SDK=7.0.100-preview.1.22110.4
  [Host]     : .NET 7.0.0 (7.0.22.7608), X64 RyuJIT
  Job-RYUCKU : .NET 7.0.0 (42.42.42.42424), X64 RyuJIT

PowerPlanMode=00000000-0000-0000-0000-000000000000  Toolchain=CoreRun  InvocationCount=1
IterationTime=250.0000 ms  MaxIterationCount=20  MaxWarmupIterationCount=10
MinIterationCount=15  MinWarmupIterationCount=6  UnrollFactor=1
WarmupCount=-1

|             Method |    Size |     Mean |    Error |   StdDev |   Median |      Min |      Max |      Gen 0 |     Gen 1 |     Gen 2 | Allocated |
|------------------- |-------- |---------:|---------:|---------:|---------:|---------:|---------:|-----------:|----------:|----------:|----------:|
| BlockingCollection | 2000000 | 452.8 ms | 20.90 ms | 23.23 ms | 453.4 ms | 411.5 ms | 509.4 ms | 31000.0000 | 2000.0000 | 2000.0000 | 276.14 MB |
```
  * With double check :
```
AddRemoveFromDifferentThreads<Int32>.BlockingCollection: Job-ZTJPHQ(PowerPlanMode=00000000-0000-0000-0000-000000000000, Toolchain=CoreRun, InvocationCount=1, IterationTime=250.0000 ms, MaxIterationCount=20, MaxWarmupIterationCount=10, MinIterationCount=15, MinWarmupIterationCount=6, UnrollFactor=1, WarmupCount=-1) [Size=2000000]

BenchmarkDotNet=v0.13.1.1786-nightly, OS=Windows 10 (10.0.19044.1706/21H2/November2021Update)
Intel Core i7-10875H CPU 2.30GHz, 1 CPU, 16 logical and 8 physical cores
.NET SDK=7.0.100-preview.1.22110.4
  [Host]     : .NET 7.0.0 (7.0.22.7608), X64 RyuJIT
  Job-ZTJPHQ : .NET 7.0.0 (42.42.42.42424), X64 RyuJIT

PowerPlanMode=00000000-0000-0000-0000-000000000000  Toolchain=CoreRun  InvocationCount=1
IterationTime=250.0000 ms  MaxIterationCount=20  MaxWarmupIterationCount=10
MinIterationCount=15  MinWarmupIterationCount=6  UnrollFactor=1
WarmupCount=-1

|             Method |    Size |     Mean |    Error |   StdDev |   Median |      Min |      Max |      Gen 0 |     Gen 1 |     Gen 2 | Allocated |
|------------------- |-------- |---------:|---------:|---------:|---------:|---------:|---------:|-----------:|----------:|----------:|----------:|
| BlockingCollection | 2000000 | 428.9 ms | 16.28 ms | 18.75 ms | 432.1 ms | 395.4 ms | 465.6 ms | 30000.0000 | 1000.0000 | 1000.0000 | 260.14 MB |

AddRemoveFromDifferentThreads<String>.BlockingCollection: Job-ZTJPHQ(PowerPlanMode=00000000-0000-0000-0000-000000000000, Toolchain=CoreRun, InvocationCount=1, IterationTime=250.0000 ms, MaxIterationCount=20, MaxWarmupIterationCount=10, MinIterationCount=15, MinWarmupIterationCount=6, UnrollFactor=1, WarmupCount=-1) [Size=2000000]

BenchmarkDotNet=v0.13.1.1786-nightly, OS=Windows 10 (10.0.19044.1706/21H2/November2021Update)
Intel Core i7-10875H CPU 2.30GHz, 1 CPU, 16 logical and 8 physical cores
.NET SDK=7.0.100-preview.1.22110.4
  [Host]     : .NET 7.0.0 (7.0.22.7608), X64 RyuJIT
  Job-ZTJPHQ : .NET 7.0.0 (42.42.42.42424), X64 RyuJIT

PowerPlanMode=00000000-0000-0000-0000-000000000000  Toolchain=CoreRun  InvocationCount=1
IterationTime=250.0000 ms  MaxIterationCount=20  MaxWarmupIterationCount=10
MinIterationCount=15  MinWarmupIterationCount=6  UnrollFactor=1
WarmupCount=-1

|             Method |    Size |     Mean |    Error |   StdDev |   Median |      Min |      Max |      Gen 0 |     Gen 1 |     Gen 2 | Allocated |
|------------------- |-------- |---------:|---------:|---------:|---------:|---------:|---------:|-----------:|----------:|----------:|----------:|
| BlockingCollection | 2000000 | 448.5 ms | 17.31 ms | 19.24 ms | 444.8 ms | 413.2 ms | 486.6 ms | 31000.0000 | 2000.0000 | 2000.0000 | 276.14 MB |
```
* Test scenario `AddRemoveFromSameThreads<T>.BlockingCollection` :
  * Without double check :
```
AddRemoveFromSameThreads<Int32>.BlockingCollection: Job-OTUVIK(PowerPlanMode=00000000-0000-0000-0000-000000000000, Toolchain=CoreRun, InvocationCount=1, IterationTime=250.0000 ms, MaxIterationCount=20, MaxWarmupIterationCount=10, MinIterationCount=15, MinWarmupIterationCount=6, UnrollFactor=1, WarmupCount=-1) [Size=2000000]

BenchmarkDotNet=v0.13.1.1786-nightly, OS=Windows 10 (10.0.19044.1706/21H2/November2021Update)
Intel Core i7-10875H CPU 2.30GHz, 1 CPU, 16 logical and 8 physical cores
.NET SDK=7.0.100-preview.1.22110.4
  [Host]     : .NET 7.0.0 (7.0.22.7608), X64 RyuJIT
  Job-OTUVIK : .NET 7.0.0 (42.42.42.42424), X64 RyuJIT

PowerPlanMode=00000000-0000-0000-0000-000000000000  Toolchain=CoreRun  InvocationCount=1
IterationTime=250.0000 ms  MaxIterationCount=20  MaxWarmupIterationCount=10
MinIterationCount=15  MinWarmupIterationCount=6  UnrollFactor=1
WarmupCount=-1

|             Method |    Size |     Mean |    Error |   StdDev |   Median |      Min |      Max |      Gen 0 |     Gen 1 |     Gen 2 | Allocated |
|------------------- |-------- |---------:|---------:|---------:|---------:|---------:|---------:|-----------:|----------:|----------:|----------:|
| BlockingCollection | 2000000 | 498.4 ms | 37.05 ms | 42.66 ms | 496.4 ms | 440.9 ms | 580.3 ms | 31000.0000 | 2000.0000 | 2000.0000 | 276.15 MB |

AddRemoveFromSameThreads<String>.BlockingCollection: Job-OTUVIK(PowerPlanMode=00000000-0000-0000-0000-000000000000, Toolchain=CoreRun, InvocationCount=1, IterationTime=250.0000 ms, MaxIterationCount=20, MaxWarmupIterationCount=10, MinIterationCount=15, MinWarmupIterationCount=6, UnrollFactor=1, WarmupCount=-1) [Size=2000000]

BenchmarkDotNet=v0.13.1.1786-nightly, OS=Windows 10 (10.0.19044.1706/21H2/November2021Update)
Intel Core i7-10875H CPU 2.30GHz, 1 CPU, 16 logical and 8 physical cores
.NET SDK=7.0.100-preview.1.22110.4
  [Host]     : .NET 7.0.0 (7.0.22.7608), X64 RyuJIT
  Job-OTUVIK : .NET 7.0.0 (42.42.42.42424), X64 RyuJIT

PowerPlanMode=00000000-0000-0000-0000-000000000000  Toolchain=CoreRun  InvocationCount=1
IterationTime=250.0000 ms  MaxIterationCount=20  MaxWarmupIterationCount=10
MinIterationCount=15  MinWarmupIterationCount=6  UnrollFactor=1
WarmupCount=-1

|             Method |    Size |     Mean |    Error |   StdDev |   Median |      Min |      Max |      Gen 0 |     Gen 1 |     Gen 2 | Allocated |
|------------------- |-------- |---------:|---------:|---------:|---------:|---------:|---------:|-----------:|----------:|----------:|----------:|
| BlockingCollection | 2000000 | 546.2 ms | 28.08 ms | 32.34 ms | 541.6 ms | 501.0 ms | 620.9 ms | 31000.0000 | 2000.0000 | 2000.0000 | 308.15 MB |
```
  * With double check :
```
AddRemoveFromSameThreads<Int32>.BlockingCollection: Job-BWWGOB(PowerPlanMode=00000000-0000-0000-0000-000000000000, Toolchain=CoreRun, InvocationCount=1, IterationTime=250.0000 ms, MaxIterationCount=20, MaxWarmupIterationCount=10, MinIterationCount=15, MinWarmupIterationCount=6, UnrollFactor=1, WarmupCount=-1) [Size=2000000]

BenchmarkDotNet=v0.13.1.1786-nightly, OS=Windows 10 (10.0.19044.1706/21H2/November2021Update)
Intel Core i7-10875H CPU 2.30GHz, 1 CPU, 16 logical and 8 physical cores
.NET SDK=7.0.100-preview.1.22110.4
  [Host]     : .NET 7.0.0 (7.0.22.7608), X64 RyuJIT
  Job-BWWGOB : .NET 7.0.0 (42.42.42.42424), X64 RyuJIT

PowerPlanMode=00000000-0000-0000-0000-000000000000  Toolchain=CoreRun  InvocationCount=1
IterationTime=250.0000 ms  MaxIterationCount=20  MaxWarmupIterationCount=10
MinIterationCount=15  MinWarmupIterationCount=6  UnrollFactor=1
WarmupCount=-1

|             Method |    Size |     Mean |    Error |   StdDev |   Median |      Min |      Max |      Gen 0 |     Gen 1 |     Gen 2 | Allocated |
|------------------- |-------- |---------:|---------:|---------:|---------:|---------:|---------:|-----------:|----------:|----------:|----------:|
| BlockingCollection | 2000000 | 516.9 ms | 36.66 ms | 42.22 ms | 523.8 ms | 445.6 ms | 610.5 ms | 31000.0000 | 2000.0000 | 2000.0000 | 276.15 MB |

AddRemoveFromSameThreads<String>.BlockingCollection: Job-BWWGOB(PowerPlanMode=00000000-0000-0000-0000-000000000000, Toolchain=CoreRun, InvocationCount=1, IterationTime=250.0000 ms, MaxIterationCount=20, MaxWarmupIterationCount=10, MinIterationCount=15, MinWarmupIterationCount=6, UnrollFactor=1, WarmupCount=-1) [Size=2000000]

BenchmarkDotNet=v0.13.1.1786-nightly, OS=Windows 10 (10.0.19044.1706/21H2/November2021Update)
Intel Core i7-10875H CPU 2.30GHz, 1 CPU, 16 logical and 8 physical cores
.NET SDK=7.0.100-preview.1.22110.4
  [Host]     : .NET 7.0.0 (7.0.22.7608), X64 RyuJIT
  Job-BWWGOB : .NET 7.0.0 (42.42.42.42424), X64 RyuJIT

PowerPlanMode=00000000-0000-0000-0000-000000000000  Toolchain=CoreRun  InvocationCount=1
IterationTime=250.0000 ms  MaxIterationCount=20  MaxWarmupIterationCount=10
MinIterationCount=15  MinWarmupIterationCount=6  UnrollFactor=1
WarmupCount=-1

|             Method |    Size |     Mean |    Error |   StdDev |   Median |      Min |      Max |      Gen 0 |     Gen 1 |     Gen 2 | Allocated |
|------------------- |-------- |---------:|---------:|---------:|---------:|---------:|---------:|-----------:|----------:|----------:|----------:|
| BlockingCollection | 2000000 | 566.6 ms | 41.47 ms | 47.76 ms | 560.5 ms | 511.2 ms | 672.3 ms | 31000.0000 | 2000.0000 | 2000.0000 | 308.15 MB |
```"
"If failure happens, `memory_details.numa_reserved_block_table[b].memory_base` will be garbage for `b` > `block_index`."
"Yes.

It's worth noting that some editors, eg vscode, appear to perform filtering of results on the client side (haven't found the exact spot in the source where this occurs). I'm not sure what the general expectation from clients is on this front, but if we can quickly tell whether the sub-string occurs or not, I imagine it is more correct and (speculatively) performance is improved by not sending garbage to the client.
"
"@github Seriously, can't this be automatically analyzed â€” without an attribute?"
"> we already have situations today where parts of the collection are uninitialized.

Today, all parts of the collections are either zero-initialized or initialized to a valid value. This change makes it possible to see values that are complete garbage.

I agree with @github that this weakens security defense-in-depth in the presence of race conditions. "
"Even if such an alert happens, it seems like the worst case here is a race condition where we try to use a connection that's about to be closed and then we end up needing to retry... but such a race condition already exists,  which is why we already have said retry code.  Yes?"
"> Is this kind of refactoring appreciated/encouraged, or rather frown upon?

I would have to see the change suggested.  Based on that, i could tell you.  Worst case, you simply revert that change in the PR. :)"
"@github good point thinking about privacy. I think risk is low since button does not give enough horizontal space to put anything sensitive there. Worst case the button might have a filename in it. Though from all my usage of VS Code I have never seen PII in action labels. But extensions can get creative.  Also we already record the label of some actions [here](https://github.com/microsoft/vscode/blob/isidorn/notificationsTelemetry/src/vs/editor/browser/editorExtensions.ts#L336) for example. So I say let's include it and if we see issues we change it.

I do not want to have any special code to treat these actions separately and hard code ids. I believe just capturing the label will give you the information which you are asking for.

Then we will go with just a `source` property.
Also you might have missed my comment here https://github.com/microsoft/vscode/pull/119075/files#r595427028

@github I am looking into potential firing of mutliple events for each notification shown when the hide notification action is exectued [here](https://github.com/microsoft/vscode/blob/isidorn/notificationsTelemetry/src/vs/workbench/browser/parts/notifications/notificationsCommands.ts#L162). The problem I hit is that the NotificationsController is unaware of current notifications shown. Which I need in order to compute notification ids and fire mulitple events."
otherwise it gets initialized as from pointer+length pair every iteration of the loop (worst case)
"> Otherwise, the resources it is using will not be freed until the garbage collector calls the CancellationTokenSource object's Finalize method.

Honestly, if this is the case, I'd be 100% ok not disposing then at all. It just adds complexity to lots of scenarios that we could avoid if we know the GC will just handle this for us"
I would add a test that verifies this is the behavior on AOT platforms (ie it doesn't throw and doesn't return garbage values)
"> ... but only include its results if it's completed by the time task (1) is completed ...

@github Is there a way to turn this off? The IDE keeps adding ""features"" to reduce the accuracy of IntelliSense lists and they are the absolute worst.
![image](https://user-images.githubusercontent.com/1408396/152096426-3e05d19b-3a36-4560-a7a4-154239c1ef2b.png)

"
"@github With AES-256 encryption, the extraction needs to take a slightly different approach:

        Bit 6: Strong encryption.  If this bit is set, you MUST
               set the version needed to extract value to at least
               50 and you MUST also set bit 0.  If AES encryption
               is used, the version needed to extract value MUST 
               be at least 51. See the section describing the Strong
               Encryption Specification for details.  Refer to the 
               section in this document entitled ""Incorporating PKWARE 
               Proprietary Technology into Your Product"" for more 
               information.

With weak ZipCrytpo encryption, the compressed data is encrypted as well, but our underlying implementation is not checking that and it results in essentially extracting garbage data. One could argue that this inconsistent behavior is a bug and that attempting to extract weak encrypted entries should also throw. I expect we'll reevaluate those behaviors if/when #1545 is addressed."
Incredibly hot path.  And the common case is to be ascii.  this avoids one check+branch in char where it checks if is latin first.
FWIW: this is a very important scenario.  I've included notes on this topic in the linked issue.  But this can seriously affect partner teams.  And i'm also concerned about how it will make things feel for C#/VB #Resolved
"> What happen when calling ICU API which ends up failing because of OOM and then calling it again (or calling other ICU API)? does ICU will retry to load the data or it just fails all the way?

I believe it depends on the API. Though, from testing, I found that for most APIs the ICU will try again to load the data again and if success, then it will continue fine.

However, there are some unfortunate ICU API (often older ones) that don't have an error code parameter at all, so if an OOM failure happens then you don't know about it.

For example, consider the API [`uloc_toUnicodeLocaleType`](https://unicode-org.github.io/icu-docs/apidoc/released/icu4c/uloc_8h.html#a0261e5fbb0ea96e16d111b681426f2a4), it has no way to report back an a detailed error status. So if any failure happens it returns `nullptr`, meaning that you don't know if the failure was OOM, or if maybe the input was garbage, etc.

So I think it sort of depends on if you want to force any OOM failures to happen up-front, or if you are okay with maybe some mysterious error happening later on.

> Also, does the whole data is just one thing? or can we have multiple data files?

There is only one single data file for the Window OS version, so if it loads successfully then you don't have to worry about other files failing later, etc.

> I am asking because we can avoid the OOM situation by just calling one of the API right after we load and that will ensure loading the data I guess. but if we have multiple data files, we can run into the situation later again. maybe this will be ok for us to throw exceptions during that time.

Good point/question. For a customized/hacked app-local copy of ICU, you could have multiple data files, so you might have to deal with OOM later on.
But again, I don't know if this is really something that you want to support or not, as I think it would be really esoteric and unlikely to happen in the wild at all.
"
"> Right. So at the point that the managed and unmanaged connection has been severed shouldn't we aggressively disconnect everything?

No, because everything is already going away. Once that connection is severed, we're in a sort of Limbo where anything that tries to communicate across the bridge will throw an exception.

> why don't we unhook from the OnElementPropertyChanged event

I suppose we could, but the only thing we'd be preventing is subsequent calls to the property changed handler, which is already guarded. It's only worth doing if there will be subsequent calls; otherwise, we're just adding an extra delegate remove call. 

> I feel like the second the Handle transitions to zero then all of this code should be executed

The second the handle transitions to zero, anything we do in the renderer is at risk of throwing the exception. And none of it is necessary, because all of that stuff will be immediately garbage collected. The only reason we unhook all that stuff during a deterministic dispose is to help the garbage collector. "
" > volatile in C has nothing to do with atomicity or ordering. It means two things:

But then you go on to say it does have to with ordering:

> Don't reorder loads/stores involving this location with respect to other volatile loads/stores

Which is indeed useful, assuming there are other relevant volatiles.
Volatile really was the semantically correct and cheap ""atomic"", for x86, until ARM came along (and PowerPC, Sparc, etc...and worst of all, Alpha).
And then, it does work on ARM, but is no longer cheap and I guess there are cheaper alternatives?
Esp. ""half volatile"", i.e. half barriers instead of full barriers.

Usually a better solution, ignoring Alpha, is to have ""data dependencies"" where the would-be-volatile variable, is a pointer that points to on-demand initialized data, which is highly similar to the old pattern of InterlockedCompareExchangPointer on a pointer that points to the data.

And more so, C++11 static locals with initializers are really quite nice in the vast majority of runtime environments (the Windiws implementation is kinda horrible, if you have to run on XP or in kernel, but for the most part, ok).

There is some question, in general, with on-demand initialization, as to run-exactly-once or run-at-least-once or run-maybe-multiple-times-concurrently-but-commit-once."
"The pattern @github landed on for this is outlined in:
https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/implementing-disposeasync#the-disposeasync-method
It's complicated by how disposal is now split between different layers of this strategy pattern, but I think here the _strategy.DisposeAsync is essentially the DisposeAsyncCore method, which would lead to this being:
```C#
await _strategy.DisposeAsync().ConfigureAwait(false);
_strategy.DisposeInternal(false);
GC.SuppressFinalize(this);
```
What you have above might be fine, but it does result in a difference between whether finalization is suppressed if disposal throws an exception, as Stream.Dispose will suppress finalization after calling Dispose(true) whereas what's in the PR will suppress before."
"> After what size does Array.Resize become more costly than using a resizable builder?

The worst case for the `Array.Resize` approach is when we have just one element; in that case we have to allocate an array of `Size` when we could have allocated barely anything with `List<T>`. If we use that metric we'd end up with a very low threshold for `List<T>` being better (~50 on my machine).

The best case for the array approach is when we have exactly `Size` elements and don't need to resize at all. That should win out over `List<T>` for any size.

Without making assumptions about chunk size and the number of elements, there's no perfect threshold. That said, any reasonably small threshold avoids the worst case for the array approach."
"Yea, if there's a need for writing the mocking script using CJS, we can add it here.  But since the goal here is just to have a tool to improve the server's robustness (for example: make scripts that send garbage and verify that the DS can recover), I don't think there's a need for CJS.  This isn't something we're ever going to ship. It's just for better testing."
"> Do we need a doc for a newbie to our repository to understand how this works? This PR does a great job using type safety and analyzers to keep some of the worst mistakes from happening, but I'd worry that the ""how to use options"" might not be obvious for somebody starting out, since we're more or less abandoning our public surface area. If somebody is trying to something that matches what's been done before it might be easy to just copy/paste and follow the pattern, but any ""newer"" code is decidedly less obvious how to write

In theory, all of our code base should continue working on either set of types - outside of the shared layer, using either of these types should not cause any issues. I completely get your point though that is indeed adding an extra layer of confusion because you now have two available similar looking types to choose from. Let this change bake in for a week or so and I can ask the team how often they are hitting this hurdle and the common pain points and confusions, so the doc has appropriate content. In meantime, hopefully we can convince the compiler team to move Options down + update CodeStyle layer to newer Microsoft.CodeAnalysis, and I would be extremely happy to just delete the internal types added here and avoid anyone going through the pain of reading through this stuff.

> How do we test this safely? There's a number of IVT risks as well as a few places where a subtle change could pretty easily break something. Do we have a good plan in place?

Our existing unit tests caught quite a bit of stuff during the implementation to give me enough confidence, but I agree that there can be things missed out, so we would have to just keep an eye on bugs from dogfooding. Regarding future IVT risks, we should encourage everyone to move the external access layer for internal APIs being used - I was safely able to avoid any breaking changes for F# usage as they follow this model very nicely, other teams should follow suite if they wish to be resistant to such breaks.

> What's the next steps? We're still tracking thinking of possible better approaches, or there a good list of next steps? This moves us forward, but at the same time still feels like we're creating debt that needs to be cleaned up...

I am going to file a separate follow-up issue with suitable context for compiler team for possibility of moving Options down to compiler layer (unless @github already filed in the past which we can re-open). If that does not fly, we should consider your suggestion of packaging Workspaces assembly somehow with the NuGet package. I do feel the latter is quite an inferior solution due to the large number of compat/versioning/appdomain issues that come along with it, so hopefully it  doesn't come down to it."
"At least what I'm looking at is not *garbage*, at least not the portion that this infrastructure is using.  It chose to use the library folder that contains all assemblies built in the vertical vs a different folder we already produce that contains only shared framework assemblies.  We're not suggesting a huge investigation here or any large investment, just consider using a different folder.

So instead of `artifacts\bin\runtime\net6.0-windows-Debug-x64`
https://github.com/dotnet/runtime/blob/6ec82c58ffa891ff42391b8848b524742896b488/src/libraries/Directory.Build.targets#L68
Use `artifacts\bin\pkg\net6.0\runtime\windows-Debug-x64`
https://github.com/dotnet/runtime/blob/6ec82c58ffa891ff42391b8848b524742896b488/src/libraries/Directory.Build.targets#L79

If you're already consuming test directories which carry the individual test assemblies then the former is redundant and latter is all that's needed.  This clean-up was something @github already did when changing what we binplace in the libraries test shared framework, maybe this is just something that needs to be copied into the coreclr infra?"
"> To measure the time, you could try running asmdiffs with default clrjit.dll vs. JitStressRegs=8 and see how long it takes to produce it. That would be a good stress scenario.

`git diff` is incredibly fast. It is likely significantly faster than `jit-analyze` even. It is effectively one of the ""core"" parts of `git` and is central to almost everything else it does.

The ""worst"" cost here would likely be the time it takes to download/upload the relevant file(s), whether that is a single file containing all diffs or many files containing separate diffs."
"FYI the selectormodel concept + the way these attributes work for MVC is super complex for a few lame reasons/scenarios that don't really get used in practice.

I'd suggest trying to design the system so that these cases aren't required. The thing is that if the attributes aren't expressive enough for what you want then **worst-case** you need to write a second method that forwards to your first one.

Example of what I mean:

```C#
[Route(""/foo"")]
[HttpPost]
[HttpPut(""/bar"")]
public void MyMethod() { ... }

[HttpGet]
[HttpPost(""/foo"")]
[HttpPut(""/bar"")]
public void MyMethod() { ... }
```

What do those mean? (MVC knows). 
"
"So I changed this PR a bit to hopefully fix all the issues that had apparently been in the original one (seriously what was I thinking).

I'll be updating the description with some additional notes from my investigation. @github, @github, I'm dismissing your old approvals, since there's a good amount of new code. Fortunately, everything new is in a8913ce."
"I think we should keep `JSDocAugmentsTag` and just let `@extends` be a silent synonym for it in the parser. I don't think it's bad, for example, if the roundtrip emit converts `@extends` to `@augment`. And that's the worst side-effect I can think of."
"### How to read the results

![obraz](https://user-images.githubusercontent.com/6011991/81195110-b666c200-8fbd-11ea-95ac-e282488478a0.png)

`before #35330` means results before merging #35330
`#35330` means code after merging #35330
`xET yD`  means code after merging #35330 with the micro-optimizations from this PR, using `x` epoll threads, using `y` Dictionary. `y`:  `C` stands for **C**oncurrent while `L` for generic dictionary used under **L**ock. So `1ET CD` means single epoll thread using Concurrent Dictionary.

`Fortunes Batching` means Fortunes Platform benchmark executed with a copy of `Npgsql.dll` provided by @github that implements batching

Colors: default MS Excel color scheme where red means the worst and green means the best result.

### x64 12 Cores (the `perf` machine)

Let's start with something simple:

![obraz](https://user-images.githubusercontent.com/6011991/81195394-080f4c80-8fbe-11ea-914c-ef731efb4b1a.png)

As we can see, switching to a single epoll thread and using `ConcurrentDictionary` gives the best results - the `1ET CD` column is the greenest one. No regressions, pure win.

There are two cases where having more epoll threads gives better results:

* JsonPlatform using 512 connections. We could get 130k instead of 128k. The difference is so small that it's ignorable
* PlaintextPlatform using 20_000 connections. The difference is small, but IMHO Plaintext is the most artificial benchmark (because of the pipelining and super small response) and making the heuristic more complex to get few extra % here is not worth it.


### x64 28 Cores (Citrine, the TechEmpower machine)

TechEmpower hardware:

![obraz](https://user-images.githubusercontent.com/6011991/81196562-7f91ab80-8fbf-11ea-940d-f075bcadc339.png)

Again, switching to a single epoll thread and using ConcurrentDictionary gives the best results - the `1ET CD` column is the greenest one.

There are few cases where having more epoll threads gives better results:
* small and ignorable differences within the marigin of error:
   * 300k vs 305k for Fortunes using 128 connections
   * 311k vs 318k for Fortunes using 512 connections
   * 9268k vs 9373k for Plaintext using 1024 connections
* a regression from 742k to 723k for JsonPlatform with 20_000 connections. It's a `2.5%` regression, so it's small and the two other benchmarks (Plaintext and Fortunes) give the best results for this config so I think that it's acceptable

Very good thing: the throughput of JSON and Fortunes benchmarks rise when the number of clients increases (to some point ofc). We did not have that before.
Another great thing: `417,499` for Fortunes 1024 connections with latest bits from @github It's top 10 of Fortunes ;)

### x64 56 Cores (Mono machine)

![obraz](https://user-images.githubusercontent.com/6011991/81198297-9a651f80-8fc1-11ea-8698-5929627ee12e.png)

With 56 cores having a single epoll thread is not enough. Having two gives us the most optimal solution that is improving all cases.

There are two cases where having more epoll threads gives better results, but all of them are small and ignorable differences within the margin of error:
* 6954k vs 6950k for Plaintext using 256 connections
* 6964k vs 6960k for Plaintext using 512 connections

There are two where having less epoll threads gives better results:
* ignorable 660k vs 673k for JsonPlatform using 128 connections
* 6523k vs 6964k for PlaintextPlatform using 128 connections. Having a single epoll thread could give us better results, but we still have an improvement compared to base 6011k. We could reach it by setting the `MinHandles` to 128 instead of 32, but I don't think that it's worth it - it's rather unlikely that such a beefy machine is going to be used for handling such a small load.

Very nice thing: the gains are really big. Even up to x2 for Json with 512 connections.

The `Fortunes` benchmark is not included because for some reason this machine can not currently access the db server.

### ARM64 32 Cores

Here is where things get complicated:

![obraz](https://user-images.githubusercontent.com/6011991/81199859-80c4d780-8fc3-11ea-8b92-af1afa8a431a.png)

Having a single epoll thread, no matter what dictionary we use gives us a lot of red color (except the case with 20k connections).

There is no obvious dependency between the number of connections and the number of threads (like the more connections the more threads we need). If we take a look at the numbers before our changes it looks like this machine is struggling to scale up when the number of connections grows (JSON numbers are: 470->455->425->350->246).
This requires an independent investigation.

Using 4 epoll threads gives us more improvements than using two. There is only one regression: JSON using 128 connections. Again, I think that for this number of Cores we should optimize for many connections and I hope that this is acceptable.



"
"> Interested to see what scenarios want to notify multiple items complete and why

For navto we may have to sweep over the projects multiple timse.  So we have the progress represent hte worst case results.  However, if we decide to shortcircuit and avoid some of those passes, we can just add in all the missing work we skipped in one op."
"The failures are unrelated (at least I have not seen any JIT asserts and they failed in the previous run, so looking like infa).
The regressions are small:
the worst is 
```
coreclr_tests.pmi.Linux.arm.checked.mch 
Total bytes of delta: 110, 
9 total methods with Code Size differences (0 improved, 9 regressed)).
```
 I will fix them in a separate PR (#54841), this is just correctness. "
"`<value>Using this element instead of 'returns' makes it actually show up in API docs both in IntelliSense and docs.microsoft.com</value>` :trollface: 

But seriously, compare one of our properties:

[![image](https://user-images.githubusercontent.com/7574/60037419-50ec3a00-9666-11e9-86ac-f99d45cadf3e.png)](https://docs.microsoft.com/en-us/dotnet/api/microsoft.extensions.options.optionsmonitor-1.currentvalue?view=aspnetcore-3.0)

With one of the BCL ones:

[![image](https://user-images.githubusercontent.com/7574/60037462-6feacc00-9666-11e9-8a2f-0fd2ab2c9222.png)](https://docs.microsoft.com/en-us/dotnet/api/system.net.websockets.clientwebsocketoptions.clientcertificates?view=netcore-3.0#System_Net_WebSockets_ClientWebSocketOptions_ClientCertificates)

If we think that it doesn't really matter to fill in the ""Property Value"" heading (because the summary is sufficiently explaining the property) then I'm **fine** with that, but the `<returns>` element does nothing.

So, I guess I'm kinda responding more to the other one (where you did put a `<returns>`). If you don't want to do it, let's just not do it, rather than half doing it (by using `<returns>`, which doesn't work, instead of `<value>`, which does).
"
"We weren't doing that because there's no user code that runs inside this try/catch block ðŸ˜†  

After the comment at L104, it was expected that everything would succeed unless something is seriously wrong.

It was intentional before your changes, and based on my other feedback, I'm not sure it needs to change."
"I did a bunch of perf runs locally to generally get a picture of what's going on.

With a custom logging provider which noop'd (to remove the impact of the actual logging), I saw around a 3% regression with logging the request, response, and response body.  

Before (no HttpLoggingMiddleware)
```
Bombarding http://localhost:5000 for 10s using 125 connection(s)
[=================================================================================================================================================================================================================================================================================] 10s
Done!
Statistics        Avg      Stdev        Max
  Reqs/sec     70548.12   12436.92  144929.16
  Latency        1.72ms   763.52us    70.14ms
  HTTP codes:
    1xx - 0, 2xx - 713518, 3xx - 0, 4xx - 0, 5xx - 0
    others - 0
  Throughput:    14.36MB/s
```

After (with middleware and noop logging)
```
Bombarding http://localhost:5000 for 10s using 125 connection(s)
[=================================================================================================================================================================================================================================================================================] 10s
Done!
Statistics        Avg      Stdev        Max
  Reqs/sec     68877.66   14550.83  114951.68
  Latency        1.77ms   629.66us   139.06ms
  HTTP codes:
    1xx - 0, 2xx - 696073, 3xx - 0, 4xx - 0, 5xx - 0
    others - 0
  Throughput:    14.01MB/s
```

I averaged a few runs here and in general, removing the middleware showed around a 3% gain, which is expected.

I'm going to try some worst case scenarios just for kicks now. But there isn't anything obscenely wrong perf wise."
"As I see it we have two options:

1. Take everything publish puts out and just trust that it's right.
2. Hard failure on anything unexpected being introduced and require a hand-audit for each new assembly.

How it is now seems like the worst of all worlds."
"> These analyzers could give more specific suggestions about alternate APIs to use in various cases though...

@github do you have a sense for what that looks like? Outside of the MVC filter ones (and I'm not sure that one is incredibly valuable), the fix is a general purpose change the signature to be `async Task`. 

My gripe is having to having to ask users to reference the VS threading package to benefit from the analysis. The users most likely to use `async void` are also less likely to read our docs and guidance for best practices."
"So what is the likelihood of having false negatives from SimpleCoreIdSpeedCheck in the worst case?

I think it is important to have predictable performance. I do not think it is ok for one context switch in a bad spot to affecting performance characteristics for the duration of the process. Getting 5 or 10 consecutive context in bad spots - I can live with that affecting performance characteristics."
"In UTF-8, 1 char worst-case expands to 3 bytes. 1 byte does not expand to 3 chars."
... and then we will be freeing garbage here.
"Nit: This (and the two following asserts) are only valid if the input was originally a string. If the input could ever be a `ROS<char>`, this code must be audited to be resilient to the fact that the ROS contents could mutate while the method is executing. Here, ""audited"" means that the code would be allowed to produce garbage output given mutated input, but it shouldn't buffer overrun or go into an infinite loop or anything like that.

I don't think this is a problem right now since all the callers appear to work only with string instances, but this assumption should be called out somewhere as a code comment."
"Do we need to care about non-`null` data?  Worst case, we just wind up with an ""empty"" line dumped to diag build output, so why check?"
"`xref` links could be converted to `<see cref="""">` but then we probably have to find out how many other tags are worth converting.

```suggestion
        /// Calling `DisposeAsync` allows the resources used by the <see cref=""System.IO.Compression.BrotliStream""> to be reallocated for other purposes. For more information, see [Cleaning Up Unmanaged Resources](/dotnet/standard/garbage-collection/unmanaged).
```"
"@github  `written` needs to be initialized to the length of `destination` here. From OpenSSL docs:

> If out is not NULL then before the call the outlen parameter should contain the length of the out buffer, if the call is successful the decrypted data is written to out and the amount of data written to outlen.

So `written` is an in/out, not just an out and might be garbage, which makes sense why all of the failures were in checked / release builds. OpenSSL then checks the that the `outlen` input is long enough to write to.

The validation call is here:

https://github.com/openssl/openssl/blob/52c587d60be67c337364b830dd3fdc15404a2f04/crypto/evp/pmeth_fn.c#L200

And if `arglen` (written in your case) is too small after checking it against the key size, it throws.

https://github.com/openssl/openssl/blob/52c587d60be67c337364b830dd3fdc15404a2f04/crypto/evp/pmeth_fn.c#L29-L30

If you initialize `written` to zero first, the error will happen consistently, just to demonstrate that the value prior to calling `EVP_PKEY_decrypt` matters."
"I originally had `Environment.FailFast` in here but figured that there _might_ be a race condition where the OS is publishing new data at the same time we're running this method. If something's borked in the OS we'll probably loop forever. Hopefully we'll only loop until midnight (which is 2 seconds away at worst). :)

If this is a concern I can perhaps keep a counter limiting us to 10 tries before we failfast? Or maybe always fall back to `new DateTime(GetSystemTime())`?"
"What is the significance of ""Managed pointers shall be reported to the garbage collector even if they do not point to managed memory.""? It feels like a internal implementation detail that does not need to be in the spec."
"@github you posted the same question on three different issues/PRs (here, https://github.com/microsoft/TypeScript/pull/39267#issuecomment-732779585, https://github.com/microsoft/TypeScript/issues/34677#issuecomment-732756179) within the course of an hour, before anyone had a chance to respond. That makes it incredibly difficult for us, and for future users who might have similar questions, to keep track of the conversation. Iâ€™m going to mark these off-topic, except for the one [here](https://github.com/microsoft/TypeScript/pull/39267#issuecomment-733113283), to avoid confusion."
"It's trivial today with a type like `List<T>` for erroneous code to see values that were never there, e.g.
```C#
using System;
using System.Collections.Generic;
using System.Threading.Tasks;

class Program
{
    static void Main()
    {
        var list = new List<(long, long)>();
        list.Add((1, 1));

        Task.Run(() =>
        {
            while (true)
            {
                list[0] = (1, 1);
            }
        });

        Task.Run(() =>
        {
            while (true)
            {
                list[0] = (2, 2);
            }
        });

        while (true)
        {
            var item = list[0];
            if (item.Item1 != item.Item2)
            {
                Console.WriteLine(item);
            }
        }
    }
}
```
Every single item stored into the list has the exact same value in each item, but because of tearing the reader can see unequal values. From a ""This change makes it possible to see values that are complete garbage"" perspective, how is this substantially different?  Yes, you could see arbitrary bytes that were never written, but from the perspective ""could the state here be valid instance of the value type"", it seems like it's the same impact."
"I guess it's fine for the legacy contained language scenario, since we're there's still so much other legacy going on that this isn't really the worst bit? :smile:
"
"Yes, follows the ""you asked for meaningless garbage? I'll give you `string.Empty`"" philosophy of MSBuild.

(It's not a _good_ philosophy. But it's the one we have.)"
"if this fails, GUID is filled with garbage memory. initialize it?"
Can we write this without a lamdba expression? This is incredibly difficult to follow.
"This is an interesting solution! I agree it's good to let the `IsFixed` parameter stay but instead cause the child subtree to reinitialize if the context changes.

However, could I suggest that instead of using the region with the hashcode sequence number, we eliminate the region and put a `key` on the `CascadingValue<InputRadioContext>` instead? For example, by calling `builder.SetKey(_context);` just before or after its attributes are set.

If you already tried that and it doesn't work for some reason, then perhaps the region is the best approach. However if the key is viable, I'd say it's preferable because:

 * This is exactly what `key` is supposed to be for, so there's less magic going on and less need to understand diffing internals
 * It doesn't set a pattern of dynamic sequence numbers that have to be justified on a case-by-case basis
 * Although it's incredibly improbably, it's technically possible for two different `_context` values to have the same hash code just by chance. I know it's literally one-in-4-billion but hey maybe someone somewhere will encounter an unreproducible bug :)
"
"> so rather than we do fix in batch, we fit it one by one? or is SyntaxEditorBasedCodeFixProvider take care of separating batching and serial fixes?

Correct, SyntaxEditorBasedCodeFixProvider already batches.  That's the point of it.  It makes one editor, and applies *all* fixes to all diagnostics in a single document to that single editor.  It then produces the final tree once.  There is no need for going and producing N changes that then have to be merged.

This is a really good solution for our fixers because it completely avoids the text-based logic of our batch-merge algorithm.  This algorithm often will fail because it doesn't know how to merge certain types of text changes.  Whereas this is not a problem for the SyntaxEditor based fixer.

SyntaxEditor one is also faster and produces less garbage by converting all edits into a single walk upwards, instead of having to fork N documents, and hten apply each batch of text edits a group at a time to the document.

This also allows things like simplification and formatting to just run *once* on the document, instead of N times on each fork.

Overall it's a substantive improvement over the default batch fixer.  In measurements i made when i originally added it, this could be many orders faster for when you were makign a ton of changes (for example turning on 'use var') and running over Roslyn itself.  i.e. making things take only around a minute or so, instead of 10+ minutes.


--

Note: the default batch fixer is still great as a quick/dirty way to get batch fixing, esp. when your edits are far apart.  But it's not a great choice for C#/VB syntax-based changes that might touch tons of locations in lots of files.
"
"So, no matter what the allocation of hte large list is happening.  It's not like these are actually lazy.  And i don't really want IEnumerables because it can lead to accidents, and it is more difficult for conusmers to deal with.  i.e. everyone has to assume the worst and convert back to different forms and whatnot.  "
Ooh. I am seriously curious how much it will cost to just copy this and break the lock... (versus sending the ref)
"I think we do, because when we get the region from the region allocator, the memory will be committed, but if it's already committed, any garbage values from previous objects would just stay there."
"Regardless, because of this, we still have incredibly difficult issues to track down.  The feature is _conditionally_ optional.  Optional parameters should only be used when the value is truly optional and will cause no problems if the caller forgets to pass them.
"
"1. same, ""should be an integer, beginning with ...""
2. ""passed in"" -> ""provided""
3. ""for example, new Date(1990, 12, 1) will return January 1st, 1991"" .... is `Date` the worst built-in in Javascript? 

(no need to do anything for (3), I just didn't know that)"
"> Nit: Just so that we have good test coverage, can you please add a test (non-single-file is fine) which intentionally has some garbage after the main object in `.runtimconfig.json`. Just so that we have a test covering that part of the functionality.

Sure, I'll add a test case. 
However, the `kParseStopWhenDoneFlag` is actually relaxing a restriction. We don't need to explicitly support the case where there is junk content after the root element in a json file. The useful case of memory-maps is already tested by `PublishSingleFile` apps. 
"
"Alright I've made some changes in https://github.com/dotnet/runtime/pull/54246/commits/4ea8df3af6026ea9e3778635a622ddc39db8ac3d to fix that race condition. To recap:

- I've added back the `InternalGetTargetAndDependent` FCall that had been removed.
- I've added `UnsafeGetTargetAndDependent` to use it in `CWT<,>` and fix that race condition.
- I've added the public API `GetTargetAndDependent` back so that `DependentHandle` consumers will be able to use this as well. I've kept the method as I felt like the additional ""atomicity"" semantics was relevant enough for the operation as a whole to make it more explicit than just a property, which (at least to me) conceptually tends to refer to accessing entities without any specific semantics. I know there's a number of different opinions on this in the team so I'm happy to change this to eg. a value-tuple property if the API review team expressed a preference for a property instead ðŸ™‚
- I haven't added that micro-optimization to avoid the memory barrier call we've discussed given that with the FCall being back and taking `Object**` as secondary parameter for the dependent object, that address has to be fixed, so we can't just assign directly to the incoming `out TValue` parameter anymore (unless there's some magic here that can be done) ðŸ˜„

Hopefully this should at least fix all tests and get the PR as a whole in a better state.

To reply to @github' second point instead:

> ""This comment suggests that there is another bug that would be triggered by setting Target to non-null on a handle that has the target set to null. It may get the handle into a state where Target is non-null but the Dependent is a bogus value (since it was not tracked while Target was null).""

I was wondering how to interpret that comment, as in, at first I had read that to use ""track"" to just mean that if the target is set to `null` then the dependent might be collected at any time, so if you eg. set the target to `null` and then back to some object, there were no guarantees that the dependent would necessarily remain alive during that transition (in case a GC happened exactly between the two operations). Are you sure that it instead refers to that tracked address potentially remaining uninitialized and causing the GC to track garbage? As in, it seems that `CWT<K, V>` is relying on that working fine, given that it regularly reuses allocated handles by just setting targets to `null` and then eventually just updating them with a new target and then dependent? It's very possible I'm just missing something here, so happy to make any changes if needed ðŸ˜„"
"> I need to ensure that 'Total bytes of delta' is always negative on the different OS/Arch? 

Ideally, yes, a change like this would only produce smaller code. But sometimes this isn't possible. 

What we typically do is look at the worst cases and see why the code size has increased. Often this uncovers things that can be improved in the optimization or reveals weaknesses in a related optimization that we should try and address.

You should be able to run SPMI on just the specific method instances that have regressed and from there use jit dumps and to analyze what the jit is doing. Let me know if you need help getting this running.

Sometimes the root cause of the regressions is complicated, and the regressions are hard to address. We then need to weigh the pluses and minuses to decide if we want to move forward with a change."
"> Benchmarks for various inputs, not just the worst cases for the currently used algorithm

I'd previously added the 
https://github.com/dotnet/performance/blob/d1d3e0490bf866d4f9df1c9f86205a3d1281e8cc/src/benchmarks/micro/libraries/System.Text.RegularExpressions/Perf.Regex.Industry.cs#L82
test largely with this change in mind (you can see it's around the same time I opened the associated issue :smile:).  Regex now uses this IndexOf overload, and this test runs a whole bunch of searches, searching a large text document for every occurrence of every word in that same document.

I tried running it with this PR and got this:

| Method |         Toolchain |                     Options |       Mean | Ratio |
|------- |------------------ |---------------------------- |-----------:|------:|
|  Count | \main\corerun.exe |                        None | 3,847.0 ms |  1.00 |
|  Count |   \pr\corerun.exe |                        None |   583.9 ms |  0.15 |
|        |                   |                             |            |       |
|  Count | \main\corerun.exe |                    Compiled | 3,830.5 ms |  1.00 |
|  Count |   \pr\corerun.exe |                    Compiled |   569.4 ms |  0.15 |
|        |                   |                             |            |       |
|  Count | \main\corerun.exe |             NonBacktracking | 3,899.1 ms |  1.00 |
|  Count |   \pr\corerun.exe |             NonBacktracking |   645.4 ms |  0.17 |"
"The way the pooling works, is that the queue is full of these awaitables. When one gets awaited on, and has `.GetResult` called, it resets itself and puts itself back into the `_cache` field. So there's a pool, containing: the awaitables in active use + one additional cached awaitable. 

If the queue goes back down, from say 1000 items to 0, then 999 of those thousand will get GCd and 1 will be kept. What this optimizes for is the case of heavy load / ddos, where the queue is stuck at full and requests keep coming in; it avoids allocating extra completion sources in this state.

-------------------
So I found why the unit tests broke, and it was because the queue policy needs to reset the cache to null to prevent to awaitable from getting regrabbed before it's ready.

Ended up testing five different scenarios, by far the best performance was the simple approach of:

```
// no locking or checking within the CompletionSource reset
_mrvts.Reset();
_queue._cachedResettableTCS = this;
```

```
// no additional locking within the queue policy either, setting to null after
var tcs = _cachedResettableTCS ??= new ResettableBooleanCompletionSource(this);
_cachedResettableTCS = null;
```

This feels sloppy but safe. The policy can never double grab from the cache, because it's stuck in a lock, and if the reset fails to go through (which happens <1% on my benchmarks) worst case is allocating a couple hundred bytes.

----------

Net perf gains are (in terms of strict overhead from my middleware):
 - empty queue **14% faster** (466 -> 399ns)
 - full queue being emptied **24% faster** (554 -> 422 ns)
 - full queue actively rejecting **8% faster** (732 -> 670ns)

Good catch!

"
"So the loop in the background task will be `do { } while (true);`

I'll add a comment that it is never exited and will ""stop"" when the state machine is garbage collected along with Http2OutputProducer."
"`CancelAfter(-1)` stands for infinite timeout which can be used here from my first thought. Of course, I can remove it because it seems unnecessary at all. Just wonder that, in which case, we can use `CancelAfter(-1)` since it exists? 
And for the dispose of CTS, can't we rely on the garbage collector to recycle the resource? "
"`refresh` is already debounced after the first call! So in worst case it is refreshed twice.
"
"Updated as per feedback.
I have not handled the null case explicitly as I don't think it would ever happen, as the Logger set the value to ""(null)"".
Worst case scenario it will fall back to the string case which handles null."
"Yes, it's possible to hit this. The encoder used by the XML stack has a worst-case `GetMaxByteCount(1)` return value of __72 bytes__. This is calculated as `(1 + 1) * 12 * 3`, where the 12 comes from [here](https://github.com/dotnet/runtime/blob/8a52f1e948b6f22f418817ec1068f07b8dae2aa5/src/libraries/System.Private.Xml/src/System/Xml/Core/CharEntityEncoderFallback.cs#L35-L41) and the 3 comes from [here](https://github.com/dotnet/runtime/blob/8a52f1e948b6f22f418817ec1068f07b8dae2aa5/src/libraries/System.Private.CoreLib/src/System/Text/UTF8Encoding.cs#L56)."
"Seriously though, what we (and, of course, other clang frontends - that is specific language compilers) generate is LLVM IR (Intermediate Representation) code which allows one to describe both data and code in a type-safe manner that is abstract enough to allow for different translations into native assemblers and also allows easy(ier) optimization of the **IR** code before it's translated into the target architecture code. The target code generator takes the already somewhat optimized IR code and generates the most optimal native assembler for the given abstract construct. That's a very, very rough approximation of how it works :)"
"It'd be great if these globals could be statically initialized!
I agree the fix isn't right, because it can call the initializers multiple times concurrently, leaving really totally corrupt data, depending on underlying details..

The mono_once should probably be bought to bear, generally, seriously."
"This callstack looks like a potential use of the string that would occur concurrently with any usage from the diagnostic server thread:
ep_rt_diagnostics_command_line_get
log_process_info_event
disable_holding_lock
disable_helper
ep_disable
EP_RT_DEFINE_THREAD_FUNC (streaming_thread)

For practical purposes the odds of hitting that race are probably incredibly small, but at least theoretically I think the issue Jan was worried about can happen. It wouldn't make me too excited to introduce new potential AVs via servicing. If the value is known to only change once in the app lifetime I'd suggest it is safer to leak that one allocation rather than take risks freeing strings that could be in use."
"> `git diff` is incredibly fast.

Yes. I am worried about the size of file that is produced by the `git diff` and uploading it from helix will be costly and we don't want to do that. That is the reason we started with uploading zero files but then uploading handful of files with `--retainOnlyTopFiles` option."
"Also, why should we run this in proc?  It seems like that will generate a lot of garbage in the IDE that then has to be cleaned up (and GC's impact other IDE features).  Why not run these out of proc, but still just compute *only* for the single doc, and not all docs?"
"I get a 500 error when trying to view the nodejs docs. Did the potential return values of `process.pid` change recently? In the worst case, we would need to add more fallback logic if it really is the case that sometimes, `process.pid` isn't valid anymore."
"Without this check, only a couple of tests break.
Here's one:
```
    int M1(string s1, string s2)
    {
        return (s1, s2) switch {
            (string x, string y) => x.Length + y.Length
            };
    }
```
That scenario gets a new warning:
```
 // (6,25): warning CS8655: The switch expression does not handle some null inputs (it is not exhaustive). For example, the pattern '(null, _)' is not covered.
                    //         return (s1, s2) switch {
                    Diagnostic(ErrorCode.WRN_SwitchExpressionNotExhaustiveForNull, ""switch"").WithArguments(""(null, _)"").WithLocation(6, 25),
```
So this isn't so much ""treating pure null tests seriously"" as ""consider branches unreachable when there is an implicit null-test but the input is not-null"".

Pure null tests are handled in the `BoundDagExplicitNullTest` arm below. Those will change an input from not-null to maybe-null.

---
In reply to: [574012059](https://github.com/dotnet/roslyn/pull/51001#discussion_r574012059) [](ancestors = 574012059)"
"So I would say it actually doesn't regress vim -H, barring an incredibly bizarre font fallback accident to make it work.
Re #7149: in terms of fixing RTL this replaces it. However, 7149 may be worthwhile to adapt into a ransom note fix on its own."
"You can just run the script locally, then commit & push the changes. The script will auto-format the code for you. This will resolve the ""Code Health Scripts Proper Code Formatting Check"" check, which will also in turn resolve the ""Terminal CI"" check. 

We're also just gonna get rid of the ""Lint Code Base"" step, because it is incredibly flaky. So don't worry about that."
"I guess I am incredibly bad at reading and got too excited AND AUTOMERGED THE ""DO NOT AUTOMERGE"" PR."
"The `Main` part of the filename should match afaik, but the `~ipad` portion could be `~iPad` sometimes (I'm not sure what causes that but that was a bug I recently fixed that lead to this refactoring).

That said, since the ""prefix"" of the filenames is what matters for sorting, we might be able to safely use `Ordinal` or at worst, `OrdinalIgnoreCase`?"
We are leaving CallKit out from mac os x until we get our feedback ticket reply. The bindings are there for iOS and worst case we remove the constants.
"I think the difference is that far more people want logging by default than will want `MemoryCache.GetCurrentStatistics()` by default. On the other hand as @github said in API review, this benchmark is pretty much the worst-case overhead.

I think defaulting to not tracking makes sense, but I'm not super opinionated."
"Please tell me that when a TS team member types `@typescript-bot sudo make me a ðŸ¥ª`, something really cool happens. If not, you need to seriously evaluate priorities in the roadmap. Ha!

"
"> Thanks for the info. Is it possible to provide some perf numbers which compare perf on windows, linux (with and without the change)? That will help with a justification of backporting. I see that details are included in [#49058 (comment)](https://github.com/dotnet/runtime/issues/49058#issuecomment-808191298), but a summarized table would be helpful is possible.
> 
> Thx!

We have a closed-source app with the following behavior

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
|OS     |Total RAM on machine|Free RAM on machine at start|JobObject/Container memory limit|Typical JobObject/Container memory usage        |Runtime Seconds|GC Counts                   |
|-------|--------------------|----------------------------|--------------------------------|------------------------------------------------|---------------|----------------------------|
|Windows|768GiB              |~700GiB                     |36GiB                           |15GiB (excl file cache)                         |7190           |G0=111202, G1=29408, G2=344 |
|Linux  |768GiB              |~700GiB                     |36GiB                           |36GiB (incl file cache)                         |10421          |G0=111932, G1=29224, G2=812 |        
|Linux  |768GiB              |27GiB                       |36GiB                           |27GiB (incl file cache)                         |5592           |G0=110433, G1=29013, G2=98  |
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
On Windows, memory usage is limited with a JobObject. JobObject limits don't include the file cache, so the bug doesn't occur.
On the first Linux run the bug is occuring. Note increased runtime and G2 GC count.
On the second run we have worked around the bug by putting the machine under memory pressure (this stops the comtainer memory usage from hitting the container memory limit).
Working round the bug roughly halves runtime and reduces G2 garbage collections by 8x (compared to Linux with the bug).
Marcin's fix for the bug should have a similar performance impact for our application.

(These numbers are for dotnet core 5.0)"
"Need to consider the worst here: that we're on a case sensitive file system. 

Note: worst is not Linux because Linux can be case insensitive.  The worst is a path where different parts have different casing rules.  "
"> How much memory is idled between 40 and 100 streams?

We should collect a profile and get a real number for this, but I think it's safe to say the memory usage for streams on an idle connection that previously had at least 100 concurrent streams open (the worst case) will be at most 2.5x worse what it was before.

> What do we think of supporting expiring pooled streams?

I think that's a great idea. @github also suggested ""Pruning pool over time for long running connections"" when he first added the 40 stream pool (#18601) though it wasn't really discussed much then IIRC. I think it can be done in a separate PR though as it's something we should do whether or not this PR is merged."
"> theoretically sure, it can have many. but we still show all fixes for diagnostics on same line. they do sort them

Right.  But htat's because diagnostics have locations.  And thus it's fairly intuitive that the squiggle that is *closer* to your cursor is more relevant than one farther away.  No such location/sorting exists for refactorings.  A refactoring is just based on your cursor/selection.  So all refactorings are equally suitable.

> I think we choose line since it is well balanced between convenience vs too much offering.

We never chose 'line'.  And, indeed, we've take a ton of refactorings and made them non-line because they were showing up too often :)

> I dont want to go specific ""token"" to get my refactoring or remember what the specific token is for the refactoring I want. if I choose to do ""ctrl+."", most of time, it should offer what I want if I moved my caret closely enough.

Yes.  That's what this PR does.  it keeps the feature available on intuitively understand parts of the for-statement, while ensuring it doesn't show up in areas i don't want it to (like when i'm on the 'var' part of a for-loop).  It's designed based on everything we've learned so far about refactorings showing up too much, and i'd like it to use those lessons and be consistent with them.

> unless we actually have a case where we show too many refactoring on a line that makes navigating offerings are more work than moving caret to exact position, I think following roughly same pattern as code fix is less burden to users.

This was not the case.  We found that having too many refactorings showing up *was* a burden to users.  So we spent a *lot* of time (seriously, tons and tons of fixes) to try to address this.  I do not want us rolling back here.

--

Note: as i mentioned above, i would be willing to offer this before the 'for' keyword, up to the start of the line it is on.  but that's the only extension to this check that would be reasonable and would not cause the same sort of problems we spent so much effort trying to address."
"So what is the worst case regression, e.g. for program that just calls `GetCurrentProcessorId` (e.g. via ArrayPool) right number of times and does not do much of anything else?"
"so! I had a quick play with this, and went absolutely wild with hyperlinks.

**it turns out** that `++it` on an AttrRowIterator walks _cell by cell_, not _attribute by attribute_. We end up in the pathological case constantly, scanning each individual cell, and rolling a line off the top of the buffer becomes incredibly expensive."
"Without the double cast, It would fail to compile with `error CS0030: Cannot convert type 'System.IntPtr' to 'nuint'` or `Cannot implicitly convert type 'nint' to 'nuint'. An explicit conversion exists (are you missing a cast?)`.

It has to do with the fact that `IntPtr` is weird type with unnaturally defined operations. The double casts are not worst. The worst is when the code does not do what you want without any compiler warnings, like what happened in https://github.com/dotnet/runtime/pull/41198"
These packages were actually broken for UAP in last release (and maybe even previous) because the netstandard ref was versioned while UAP still got a placeholder (and thus would provide the older version inbox assembly).  The use of place holders is a relic of the old pre-conflict-resolution way of dealing with assemblies that were part of the framework.  We can remove that placeholder and simplify these cases while fixing the netstandard consumption case.  Worst case that happens here is that the implementation provided by the package is somehow worse than an old UAP configuration that was in the meta-package itself.  That's actually no worse than the type of breaks we already made when removing all the UAP package configurations from the repo.
">If build-in COM support is disabled such attributes should not be even read by the runtime and if someone would request explicit COM support I'd expect it will get PNSE well before it hits any COM code path.

I think this is a general issue with not built with `FEATURE_COMINTEROP` vs having the feature flag for Com interop set to false. The type loader sets many flags that fundamentally change some behavior &ndash; @github's example is spot on.

I will look at each of these. The `Guid` is the worst offender for me but there are others (for example, `CoClassAttribute`) that are incredibly obnoxious."
"Seems like it might be worth using the 'timeout' option on execFileSync. Seeing as 'fs.watch' by default will keep the process alive until 'close' has been called on the filewatcher, if the callback that calls 'close' in 'watchGuard' doesn't run for some reason, this would effectively hang tsserver. Seems like 1000ms should be a generous worst-case choice."
"all my tests test for trivia :)  That's why teh methods don't look incredibly wonky after the rewrite.  
"
"Here's a quick benchmark comparing 3 modes:
* ""Native"" (the current implementation)
* ""List"" (reuse a `List<T>` instance to build each chunk)
* ""First"" (use `List<T>` to build the first chunk *regardless of chunk size*)

In these scenarios, List and First are both **much** better in the lopsided case (1000,1000000) which is expected. In the worst case for First (1000,999), List is slightly better. In other cases, First comes in slightly/moderately ahead of List. This largely lines up with what intuition suggests. 

As you say, First is a bit more complex than list (~10 extra lines of code) but on balance has slightly better performance. I'm not sure whether it is enough to be worth it. Thoughts?

|      Method |        Source Count,Chunk Size |      Mean |     Error |     StdDev |    Median | Ratio | RatioSD |    Gen 0 |    Gen 1 |    Gen 2 | Allocated |
|------------ |------------- |----------:|----------:|-----------:|----------:|------:|--------:|---------:|---------:|---------:|----------:|
| **ChunkNative** |     **1000,100** |  **21.74 Î¼s** |  **0.313 Î¼s** |   **0.293 Î¼s** |  **21.70 Î¼s** |  **1.00** |    **0.00** |   **2.6550** |        **-** |        **-** |      **8 KB** |
|   ChunkList |     1000,100 |  24.92 Î¼s |  0.470 Î¼s |   0.440 Î¼s |  24.80 Î¼s |  1.15 |    0.03 |   3.3569 |        - |        - |     10 KB |
|  ChunkFirst |     1000,100 |  23.31 Î¼s |  0.461 Î¼s |   1.050 Î¼s |  23.17 Î¼s |  1.08 |    0.04 |   3.3569 |        - |        - |     10 KB |
|             |              |           |           |            |           |       |         |          |          |          |           |
| **ChunkNative** | **1000,1000000** | **883.51 Î¼s** | **99.235 Î¼s** | **291.040 Î¼s** | **787.80 Î¼s** |  **1.00** |    **0.00** | **173.8281** | **170.8984** | **170.8984** |  **7,821 KB** |
|   ChunkList | 1000,1000000 |  25.58 Î¼s |  0.480 Î¼s |   0.426 Î¼s |  25.47 Î¼s |  0.04 |    0.00 |   7.8735 |        - |        - |     24 KB |
|  ChunkFirst | 1000,1000000 |  25.54 Î¼s |  0.497 Î¼s |   0.440 Î¼s |  25.59 Î¼s |  0.04 |    0.00 |   7.8735 |        - |        - |     24 KB |
|             |              |           |           |            |           |       |         |          |          |          |           |
| **ChunkNative** |     **1000,999** |  **22.88 Î¼s** |  **0.439 Î¼s** |   **1.078 Î¼s** |  **22.52 Î¼s** |  **1.00** |    **0.00** |   **5.1270** |        **-** |        **-** |     **16 KB** |
|   ChunkList |     1000,999 |  27.74 Î¼s |  0.552 Î¼s |   1.090 Î¼s |  27.64 Î¼s |  1.20 |    0.07 |   7.8735 |        - |        - |     24 KB |
|  ChunkFirst |     1000,999 |  29.22 Î¼s |  0.805 Î¼s |   2.360 Î¼s |  29.09 Î¼s |  1.27 |    0.14 |  10.4370 |        - |        - |     32 KB |
|             |              |           |           |            |           |       |         |          |          |          |           |
| **ChunkNative** |    **10000,999** | **231.38 Î¼s** |  **4.590 Î¼s** |   **6.127 Î¼s** | **232.73 Î¼s** |  **1.00** |    **0.00** |  **27.8320** |        **-** |        **-** |     **86 KB** |
|   ChunkList |    10000,999 | 267.46 Î¼s |  6.508 Î¼s |  18.776 Î¼s | 264.60 Î¼s |  1.12 |    0.06 |  30.7617 |        - |        - |     95 KB |
|  ChunkFirst |    10000,999 | 221.95 Î¼s |  4.243 Î¼s |   4.167 Î¼s | 222.43 Î¼s |  0.96 |    0.03 |  33.2031 |        - |        - |    103 KB |
"
"We should probably avoid printing these interstitial `sourceMappingURL` comments in concatenated files. They have the potential to seriously confuse tools, should the files actually exist on disk."
"This is just the worst.
"
"Looking at historical builds, there is quite a bit of variability on checkout times. We have only a single build with this change, and it's certainly no worse, but it's hard to state that it's better from that data point alone.

Here are reasons why this should be theoretically faster:

- `--no-auto-gc` avoids running the maintenance than usually occurs after a fetch, as we know this is a freshly fetch repo so will be packed with no garbage to clean up (replacing the `git config` call of the built-in checkout task).
- `--no-recurse-submodules` avoids checking anything related to submodules, which Roslyn don't use.
- `--no-show-forced-updates` avoids checking for remote branches which had forced updates when the fetch completes.
- `-c advice.detachedHead=false` avoids logging that we're in a detached head, which is expected and therefore not noteworthy.
- `--no-progress` avoids filling the log with tens of messages during checkout.
"
"I know that you don't look at timestamps, but still it would be an incredibly valuable diagnostic tool, as the rest of the system is using timestamps pervasively.

This would help correlate timestamps to MVIDs, and enable writing binlog analyzers that track flow of binaries across the build.

I've really wished on several occasions that I knew what the MVIDs and timestamps were before and after CopyRefAssembly.

---
In reply to: [1130469277](https://github.com/dotnet/roslyn/pull/61384#issuecomment-1130469277) [](http://example.com/codeflow?ancestors=1130469277)"
"> From a ""This change makes it possible to see values that are complete garbage"" perspective, how is this substantially different? 

For example, you can potentially use a race condition like this for information disclosure attack after this change."
"@github was right that the `pNestedFrame` points to a potential garbage. In fact, the real issue is not in this assert, but rather in the stack walker accidentally skipping the `InlinedCallFrame` processing during first pass of exception handling. That is caused by the fact that the test calls managed method (marked as `UnmanagedCallersOnly`, so it seems it should be valid) using an unmanaged delegate and the stack walker doesn't work correctly when there are no native frames above the pinvoke IL stub frame on x86. The `InlinedCallFrame` is expected to be processed before the stub and it doesn't happen in this case, as a native frame is required to make the stack frame iterator move to the explicit `InlinedCallFrame`. The fact that it is not processed results in ignoring the upper limit on the stack walk and walking the stack further than expected. When that frame is processed correctly, it causes aborting the stack walk and returning from the `CPFH_FirstPassHandler` back to the OS exception dispatcher. Then the OS dispatcher calls us again, but with a different establisher frame that doesn't have the `InlinedCallFrame` linked to it and everything works properly and no assert fires.

So this PR should be closed and the stack frame iterator fixed so that it behaves correctly in this case. I am working on a fix."
"IMO, it's ""Garbage in, Garbage Out"".

I don't think it's worth going overboard with checks for an API like this. If you're trying to get the Visual Element Children, and you don't get the values you expect, you should be willing to check those parameters to see what's up and why it's not what you expect. If it were more forward-facing (Like something you would use for building an MAUI app) then I would be more concerned. But considering the audience for an API like this, you should be able to have some expectations of what you need to get the data to return.

If we normalize values, then it creates the expectation that you can't trust what gets passed in is actually used for calculation. 

IMO I like giving the option for a series of points rather than enforcing rectangles since then you could have hit detection based on, say, a lasso. 

But on that point, I do want to make sure that it at least returns an empty list and never blows up, I can write unit tests around that to make sure if you do give it garbage, it will return garbage ;)"
"It would be, this is just garbage left behind from the original attempt. I can either revert these changes or remove the accessor completely. I believe is like 5/10 lines of code changes total. I'm happy to do so in another Pr too"
"That only helps if you mark `TextInfo.ToLowerInvariant(char)` with aggressive inlining, which I did not do here. The codegen is a bit too large to make that optimization worthwhile.

Worst case it adds 1/4 nanosecond latency, so whatever. :)"
"> Are you asking for a delegate defined in a when clause that contains an assignment to the pattern variable, which is then invoked in that or a subsequent when clause?

I am asking for a test that would get us here with an invocation of a delegate that modifies ""an interesting"" local. The delegate could be based on a lambda or a local function. Whatever is the ""worst"" situation.

---
In reply to: [405803545](https://github.com/dotnet/roslyn/pull/42972#discussion_r405803545) [](ancestors = 405803545,405762952,405756878)"
No need to map or assign because revive changes objects in-place (for the sake of avoiding garbage). Same on line 80
"I'd suggest to consider using an associative [MaxHeap](https://en.wikipedia.org/wiki/Binary_heap) (i.e. MaxHeap combined with Dictionary to access elements for O(1)) where each node stores open connection count for a single HttpConnectionPool.

On connection open or close, the pool will increase or decrease the respective counter. Since each pool will change only their own counter, it can be easily done thread safely. When the polling counter `_maxHttp11ConnectionsPerPoolCounter ` triggers, it will call Heapify method to rebuild the heap and get the max counter on the top. Heapify runs in O(logn) in the worst case so it seems fast enough.

There is one more issue though, that is to ensure Heapify can run concurrently with heap nodes changes. To solve that, I'd suggest to guard Heapify call with `lock`, but allow concurrent changes to counters on heap as explained above."
Will this run during a design time build? It's probably not the worst thing to be doing as there's far greater sins...
"> the worst that can happen is that we compute the same thing twice.

Typically, it is proportional to degree of parallelism.
>The delegate is called outside locks, so we could still end up computing the same things twice.

The lock before second check will prevent it from happening."
"[SecCertificateCreateWithData](https://developer.apple.com/documentation/security/1396073-seccertificatecreatewithdata?language=objc) says it works on iOS 2.0+, I don't think most of these ctors should be marked as unsupported.  (Worst case, some of the file formats don't work, but the API will still work in most cases)"
"It's safe to assume that strings are immutable. __However__, code paths like this have a tendency to get refactored such that their inputs become `ROS<char>` instead of `string`, and at that point all immutability guarantees are void.

That said, what's the worst thing that can happen if the span contents end up changing mid-operation? We accidentally read the wrong directory names, maybe resulting in an i/o error and exception? That's probably ok, not the end of the world. We accidentally read off the end of a buffer and start corrupting memory? That's unacceptable.

So if we pessimistically assume that the contents may be mutable in a future release, and we also think that the consequences of something being mutated out from under us isn't really all that bad, then I wouldn't worry too much about it. :)

As an example of this in action, see [`string.Replace`'s implementation](https://github.com/dotnet/runtime/blob/10e107debae28de4bd1e710cfe448be3c293e841/src/libraries/System.Private.CoreLib/src/System/String.Manipulation.cs#L1055), which uses an index-based approach as you ponder here, and which does not defend against mutation. If it ever gets refactored to depend on `ROS<char>`, worst case behavior is that we'll end up inserting improper substitutions into the output string. But we _won't_ run off the end of the buffer and corrupt memory. So it's not worth putting a guard in here."
"Well, so this _works_, but it has 2 problems:

1. Most consumers do not expect the very specific type the jsx factory function actually returns. Usually, this manifests in reassignments, where
```ts
let a = <div></div>;
a = <img />;
```
ends up being an error because rather than both being `JSX.Element`, the first assignment makes `a` a `React.ReactElement<HTMLDivElement>`, while the second line is a `React.ReactElement<HTMLImageElement>`. This is, unfortunately, a breaking change we'd force on people. It's not hard to fix (annotate `a` with the more vague `JSX.Element` yourself), but could be frustrating. That break reduces the appetite to take this change a bit.

2. Performance is _terrible_. Like beyond bad. This makes _every_ jsx tag tree in your program into a series of nested generic context sensitive function calls (and _most_ jsx apps have a _lot_ of nested tags!). That's just about the worst-case scenario for the type checker. If we could make nested context sensitive function calls much more performant to check, it's go a long way towards making merging this feasible."
"> Can you elaborate on that?

In general, logic that sits in `isRelatedTo` is on the uncached side and logic that sits in `structuredTypeRelatedTo` is on the cached side (by way of `recursiveTypeRelatedTo`). The PR makes the following optimizations:

* Some logic is moved from `isRelatedTo` to `structuredTypeRelatedTo`, ensuring that we only execute it once for a given pair of types.
* The `isRelatedTo` function would often make *two* calls to `recursiveTypeRelatedTo` in order to check relations involving unions or intersections. That incurred the cost of computing keys and checking the relation cache twice. We now only call `recursiveTypeRelatedTo` once for a given pair of types.
* The `reportErrorResults` function is un-nested such that closures don't get allocated in `isRelatedTo`.
* The `captureErrorCalculationState` function is no longer called by `isRelatedTo` which cuts down on garbage.

We could further consider moving the excess and common property checking code to the cached side, though it isn't run all that often and would require some restructuring in `structuredTypeRelatedTo`."
"> The small region free lists on the heaps look ok, but we have several heaps where the large region free lists have hundreds of entries.

I see; so the large object is larger than any of the regions on the large region free lists and that's why you wanted to coalesce them. 

> I started out using decommit_heap_segment (not decommit_heap_segment_pages), but it didn't decommit the first page, so I would still have to call memclr on that. It looks like decommit_heap_segment_pages would have the same issue. Not decommitting the initial pages is a problem because of later coalescing - you may end up with garbage in the middle of a region.

you could just add an argument to decommit_heap_segment_pages and have it completely decommit all memory on the region (since the region info isn't on the region), correct?
"
"Hmm, this is indeed more nuanced than I had considered. Is there any logical sense of ""this things is definitely garbage and I definitely want to destroy everything"", aside from waiting for GC, which is unpredictable? Or is this just not logically possible and it's truly up to each component to define a _heuristic_ (which I define to mean ""provably incorrect logic that approximates truth"") and work off of that?"
"And it was incredibly easy to implement! :blush: Just plumb `Lifted` in a handful of spots, and literally everything is already taken care of.

(See the tests that are already functioning)"
"@github Agreed, something is up. If we want the perf optimization that's fine, but this is incredibly easy to break. :-)"
"Specifically, I wanted to address the case of one window trying to save, say, command line history, and if this window is trying to save the window layout they should not overwrite each other. I definitely think the solution can be improved upon. I'm less concerned about the window layout being overwritten on the automatic save (worst case the data is stale by a minute), as much as the window layout autosave clobbering some other user input."
"nit: I think that we can avoid using `goto` here:

```suggestion
            var garbage = new string[] { ""Background Intelligent"", ""Intel"", ""Defender"", ""Intune"", ""BITS"", ""NetBT""};

            _output.WriteLine(""=====  Dumping recent relevant events: ====="");
            
            EventRecord? record = null;
            while ((record = eventReader.ReadEvent()) != null)
            {
                string description = """";
                try
                {
                    description = record.FormatDescription();
                }
                catch (EventLogException) { }

                if (description == """" || !garbage.Any(term => description.Contains(term, StringComparison.OrdinalIgnoreCase))
                {
                    _output.WriteLine($""{record.TimeCreated} {record.ProviderName} [{record.LevelDisplayName} {record.Id}] {description.Replace(""\r\n"", ""  "")}"");
                }
            }
```"
"IMHO it's worth a try. We can check strings in the common implementations (FreeBSD, GNU Inetutils, macOS*). Worst case it will behave the same way as it does now, best case we can provide slightly more accurate result.

However, the code needs to do way more input checking than it does now. The `.IndexOf` calls may fail to find anything and it would throw `ArgumentOutOfRangeException`."
"> Is there a reason for the extra ILLink folder?

There are a set of ILLink related files that traditionally have been in the root of the project. Mono started grouping them into a folder specifically for this area, and it has worked out well as we've been adding more files in this space. We are already putting ILLink related files in `src/ILLink` folders. This change is just making all libraries uniform.

In the worst case, we have almost 10 ILLinker files for one library - https://github.com/dotnet/runtime/tree/master/src/libraries/System.Private.CoreLib/src/ILLink"
"This could just be `Add`, worst case it's called more than once, but only on first concurrent requests."
More seriously I'm sure @github can give you samples of creating the certs during runtime for tests so you don't need the test files any more
"Refactored. It duplicates a bit of other code but yeah it's better: https://github.com/dotnet/roslyn/compare/5205913227e97cb0fcf276b0371a5d23f35af45d..736d3f6a7c00819ee8e2a697ae7b3044922433b4

Helper method won't be done since some of the code there is calling local functions which captures state, and I don't think that's remotely the worst sin happening in that code at this point."
I'm seriously considering using it for my PR.
"> Sorry about my debt item #127313 being incorrect. I believe the current complexity is O(N + NlogM) where N is the number of changed lines and M is the number of injected texts in all the lines, not O(NlogM) as stated in #127313.

`O(N + NlogM)` = `O(NlogM)`, since `NlogM` grows at least as fast as `N`, but if `M` depends on `N` even faster.

> I am now indeed very curios to understand the algorithm, I have rarely seen a big O complexity use division i.e. O(N * log (M / N)).

This is due to `N` being the number of lines and `M` being the number of injected texts.
`M / N` is the average number of injected texts per line.
We don't need to do a linear search to find all injected texts on the same line if they are sorted - we can do an [exponential search](https://en.wikipedia.org/wiki/Exponential_search).

Assuming all injected texts are equally distributed among the lines, we don't need to walk over `M / N` injected texts for each line, but only have a look at `log2(M / N)` many.
I think equal distribution is actually the worst case here. If the first line contains all the injected text, we actually have a runtime of `O(N + log M)`, which is even better.

I don't know the runtime of slice (O(1) or O(n)), but at least the skipping part can make use of this improved runtime (even though the current implementation returns a slice in both cases)."
"The default-constructed value is garbage data, like an uninitialized integer. I had to make it ""Primary"" to make it not `co_return` directly. *read the logic*"
"I never know how to think about whether to add doc comments to existing APIs that don't have them.

On the one hand, if the API already has good documentation, I don't want to arbitrarily add some garbage that's worse than what already exists, and end up making things worse.

On the other hand, sometimes you end up with this: https://docs.microsoft.com/en-us/previous-versions/visualstudio/hh137994(v=vs.118)

@github Guidance here?"
"From our offline discussion, we need to not only maintain the ""state when everything went well"" (stateWhenNotNull), but also the worst case state (which factors all the possibilities, such as the chain partially executing).
Your example `a?.b(x = null)?.c(x = new object())` illustrates the problem. #Closed"
"I'm totally comfortable with that outcome, honestly. The biggest issue with C1 controls originates from Windows' unique disposition with codepages, and so it feels right for the default in _Windows' console host_ to be ""ignore"". Terminal wanted to be slightly less tied to Windows' console's needs in a lot of regards, and... accepting C1 by default surely doesn't seem like the worst of those regards."
"@github are you double-checking ? or am I double-checking ? ;-)

Seriously, yes and this, and the other one, are still needed. It's quite possible the beta 2 build was already frozen when the rdar were filed."
"If you have the file moves (only) in a separate commits, then git should properly detect that and not show that many additions/deletions.

Then in a separate commit you can change things (in this case I see you've changed the name of the class, but I have no idea if you've changed anything else in the file, which seriously complicates reviewing the PR).
"
"> On Windows we'd end up avoiding pinning, yes?

Yes, that's true. I was hoping you wouldn't notice that :)

That said, I'm not sure how much that matters in practice, particularly if we assume that we *do* implement smart buffer management -- meaning, do zero-byte reads against the underlying stream and don't hold buffers when no data is available.

The interesting case to consider here is something like this: we have read a partial TLS frame into our read buffer. We need to read more data in order to be able to decrypt the entire frame; until we read that, we can't complete the user's zero byte read because we don't have any available data yet.

Should we perform a zero-byte read against the underlying stream in this case? Doing so would avoid pinning on Windows, which is good. However, since we already have a partial frame, it's almost certain that the rest of the frame has already arrived in the kernel buffer, or at worst, will arrive very soon. Sure, in weird cases we could have packet loss or just a very slow pipe and we'd end up pinning the buffer for a non-trivial amount of time. But that seems rare, and it's not at all clear that the benefit of avoiding pinning in this case is worth the cost of the zero-byte read.

And of course there's no pinning on Linux so in that case the zero-byte read is pointless.

(I used SslStream as the example because it's most relevant there. For chunked response streams, the equivalent would be that we have a partial chunk header -- we only need the chunk header plus one byte of the chunk to have data available. But chunk headers are pretty small in general so the impact on the chunked response stream is small.)

If we care about avoiding pinning of buffers on Windows, then there are a lot of cases that cause pinning beyond just the ones described above, and we should look at ways to address the problem holistically, like having pools of perma-pinned or native buffers.

"
"That makes a lot more sense now. I converted this PR to a draft because when I saw examples like those I got really confused and I was worried that this PR was not the appropriate way to handle this topic, but knowing those are bugs changes things.

You obviously have much more experience with this topic than I do (I've only been seriously looking into `inheritdoc` in the last week really). If you feel like this PR was the appropriate way to handle the issue, let me know and I will un-draft it. :)"
"We discussed `string` as definitely The Worst Thing to `for-in` on, but decided that keeping subtyping symmetry was more important (`string` is a `{ }` and it's legal to `for-in` a `{ }`)"
"This is... pretty big.  (Bigger than I have time to finish reviewing today).

What's the cost if we don't do this for this release?  Worst case, the CFB implementations could be

```C#
using (xform = CreateTransform(CFB))
{
    xform.TransformFinalBlock(...);
    I assume that now we need to copy stuff?
}
```

which gets the API out the door and then we can factor stuff out in late August to get cleaner for 7.


Or if it's just ""oh, it takes longer than 10 minutes"" then I can give it the longer than 10 minutes next week :)"
"We do, that's what this does. Or are you suggesting this should be implemented differently?

I've tried to keep the MSBuild logic here incredibly basic. For example, not assuming the developer even knows what an item group is. It's just properties, which should be familiar to any OO developer."
"> It allows to write unit tests that work against a `CertificateStore`.

We only use integration/manual tests for these types of things, since automation for these things is problematic on our CIs.

> And having the collection makes explicit how something like `IsTrusted` is related to the different locations that can trust the certificate.

I don't feel incredibly strong about this, however the reason that I don't like having a collection is that we are unlikely to add more stores than the ones we have now (at least frequently) and that it makes it harder to debug things or emit specific logs for a given store."
"@github we generally do not backport new features or non-critical bug fixes to earlier releases. You could consider using this PR as a template to copy to your application if you require this to work in your application.

On a side note, it's incredibly easy for us to miss comments on closed issues or PR. For the next time, please consider filing a new issue so there's clear visibility."
"I would prefer if we revisit this in 6.0. We don't really have much in term on run way to figure out how this affects apps this late in 5.0. If it turns out to be incredibly nasty, we could always patch it."
"> Need to look at this in detail, but should we treat collection interfaces as being FromBody? Itâ€™s not incredibly common to try and get optional services in your action, much more common to want to read an IList<T>

Sounds reasonable to me. In fact, the DI contract only demands that `IEnumerable<T>` can be injected (though other containers might support other syntax).

I also realize that DbContext types won't work with the interface only constraint. "
"I decided against it in this case since we calculate the hash of a byte array twice in the worst case: in `TryGetValue` and when it fails, when we add the byte array to the hash. I didn't see a measurable improvement above the noise for composite framework which has ~200000 debug info records."
"> Is thread safety a concern? 

1. the boosters are not shared.  so no threading concern there.
2. a latch would only ever go from ""supported"" to ""not supported"".  so the worst that happens in a few requests throw, but eventually you get into the not-supported state, and stop throwing.  i don't think we'd need a latch that guaranteed only one throw."
"@github 

> I thought that `iconv-lite` through webpack would support all encodings because there are no native dependencies, but please correct me if that assumption was wrong.

Your assumption is correct. It's not about native dependencies, it's about `iconv-lite` relying on the fact that it gets `Buffer` as input and not `Uint8Array`. The solution is very straightforward - adding `Buffer.from` in couple places. So in the worst case scenario we can always go with a fork.

> Yeah I think for desktop I would like to preserve that because it will test the actual stack down to the disk.

ðŸ‘ 

> However, we have a way to register file system providers and we could simply swap out the disk based provider with an in-memory one here

Yeah, that's exactly what I am doing currently. Will adjust then to reuse the same tests in-between environments with different file services.

"
"We need to match the .NET guidelines

https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/implementing-disposeasync#implement-both-dispose-and-async-dispose-patterns"
">  think that it should be an edge case as the current trend in the industry to have small microservices running in containers.

This is for Windows. I can easily imagine big first party workloads on Windows hitting this limit.

> the cost of allocating a bigger array should be small compared to the rest of the work.

The problem are secondary effects of allocating the big buffer. For example, this API can be called in background monitoring tasks. When this background monitoring tasks starts producing a lot of Gen2 garbage, the health of the whole process gets effected.

> From my experience WeakHandle-based caches are typically more expensive than allocation as they are used just once and get released with next GC.

That's true for caches of small objects - they will get released with next Gen0 GC. This buffer is huge - it will only get released with next Gen2 GC.

> do we have any other custom byte buffer caches in the framework?

Caching huge buffers is generally problematic. An alternative is to not use ArrayPool at all here, and just use unmanaged memory allocator, without any caches. It would eliminate the need for the alignment tricks. The memory returned by `Marshal.AllocHGlobal` is guaranteed to have sufficient alignment."
"> That's fine, but we're seriously busted at this point, right? Should we consider throwing an unreachable or something?

An emit layer takes care of this"
"```markdown
Context: https://github.com/xamarin/monodroid/commit/9f59e196782f5ffa8c31471a629dd45cf2538e85

When `Java.Interop.dll` was first integrated into Xamarin.Android,
the integration resulted in *two separate* bindings for the JNIEnv
JNI structure.  xamarin/monodroid@9f59e196 replaced the original
`Android.Runtime.JNIEnv` binding with a set of methods which just
forwarded to the new `Java.Interop.dll`-based binding:

	partial class JNIEnv {

	    // Pre xamarin/monodroid@9f59e196 binding behavior
	    public static IntPtr CallObjectMethod (IntPtr jobject, IntPtr jmethod)
	    {    
	        if (jobject == IntPtr.Zero)
	            throw new ArgumentException (""'jobject' must not be IntPtr.Zero."", ""jobject"");
	        if (jmethod == IntPtr.Zero)
	            throw new ArgumentException (""'jmethod' must not be IntPtr.Zero."", ""jmethod"");

	        IntPtr tmp = Env.CallObjectMethod (Handle, jobject, jmethod);

	        Exception __e = AndroidEnvironment.GetExceptionForLastThrowable ();
	        if (__e != null)
	            ExceptionDispatchInfo.Capture (__e).Throw ();

	        return LogCreateLocalRef (tmp);
	    }

	    // Post xamarin/monodroid@9f59e196 binding behavior
	    public static unsafe short CallShortMethod (IntPtr jobject, IntPtr jmethod, JValue* parms)
	    {
	        return JniEnvironment.InstanceMethods.CallShortMethod (
	                new JniObjectReference (jobject),
	                new JniMethodInfo (jmethod, isStatic: false),
	                (JniArgumentValue*) parms);
	    }
	}

The problem with this ""binding unification"" is that all codepaths
using the original `JNIEnv.Call*Method()` methods can result in the
creation of ""extra garbage"" which the GC needs to deal with, e.g. the
`new JniMethodInfo(â€¦)` expression in the above snippet.  These methods
are for *compatibility*, not performance.

We *should* migrate *off* those methods, but we haven't done so.

The legacy `JNIEnv.FindClass()` and `JNIEnv.CallStaticObjectMethod()`
methods show up in profiling the .NET Podcast app:

	15.09ms Mono.Android!Android.Runtime.JNIEnv.FindClass
	 6.25ms Mono.Android!Android.Runtime.JNIEnv.CallStaticObjectMethod

This is called in one place by:

	 6.16ms Mono.Android!Android.Content.Intent..ctor(Android.Content.Context,System.Type)

Update `JNIEnv.FindClass()` so that it doesn't use
`JNIEnv.CallStaticObjectMethod()`, but instead use
`Java.Interop.JniEnvironment.StaticMethods.CallStaticObjectMethod()`.
This in turn requires updating `Java.Lang.Class` to declare:

	internal static JniPeerMembers Members => _members;

so that the `jclass` instance for `java.lang.Class.class` can be
used as part of the static method invocation.

With this change in place, I'm don't really seen a noticeable
performance difference, but it should help a small amount by
reducing GC allocations.
```"
"Yes, I just tested it and if I pass a unicode comment and truncate the bytes, the last character may become garbage. I need to think how to fix this."
"It ends up taking 20% longer. ðŸ¤• 
Here are the measurements for running `map4` with closure shims, with ""Delta"" relative to `map4` with class shims.

##### Monaco - node (v6.7.0, x86)

| Project | Baseline | Current | Delta | Best | Worst |
| --- | --- | --- | --- | --- | --- |
| Memory used | 203,967k (Â±  0.01%) | 224,241k (Â±  0.01%) | +20,274k (+  9.94%) | 224,220k | 224,263k |
| Parse Time | 2.27s (Â±  0.39%) | 2.39s (Â±  3.65%) | +0.12s (+  5.30%) | 2.32s | 2.51s |
| Bind Time | 0.68s (Â±  1.17%) | 0.83s (Â±  1.52%) | +0.15s (+ 21.41%) | 0.81s | 0.84s |
| Check Time | 3.86s (Â±  2.12%) | 4.72s (Â±  3.44%) | +0.86s (+ 22.15%) | 4.55s | 4.89s |
| Emit Time | 1.62s (Â±  2.07%) | 2.30s (Â± 14.05%) | +0.68s (+ 41.80%) | 2.04s | 2.72s |
| Total Time | 8.43s (Â±  1.26%) | 10.23s (Â±  3.36%) | +1.80s (+ 21.34%) | 9.78s | 10.47s |

##### TFS - node (v6.7.0, x86)

| Project | Baseline | Current | Delta | Best | Worst |
| --- | --- | --- | --- | --- | --- |
| Memory used | 175,828k (Â±  0.01%) | 188,541k (Â±  0.01%) | +12,713k (+  7.23%) | 188,529k | 188,551k |
| Parse Time | 1.40s (Â±  0.75%) | 1.51s (Â±  4.24%) | +0.11s (+  7.80%) | 1.46s | 1.60s |
| Bind Time | 0.57s (Â±  1.02%) | 0.68s (Â±  2.26%) | +0.12s (+ 20.63%) | 0.67s | 0.70s |
| Check Time | 3.00s (Â±  0.53%) | 3.60s (Â±  3.55%) | +0.60s (+ 19.88%) | 3.41s | 3.69s |
| Emit Time | 1.37s (Â±  1.09%) | 1.71s (Â±  5.74%) | +0.34s (+ 25.18%) | 1.66s | 1.86s |
| Total Time | 6.33s (Â±  0.28%) | 7.50s (Â±  2.73%) | +1.16s (+ 18.38%) | 7.24s | 7.71s |

Here's the code I tested with (with equivalents for NumberMap and StringSet):

``` ts
export const createStringMap: <T>(pairs?: [string, T][]) => Map<string, T> = usingNativeMaps
    ? <T>(pairs?: [string, T][]) => new Map(pairs)
    : <T>(pairs?: [string, T][]) => {
        let data = createDictionaryModeObject<T>();
        if (pairs) {
            for (const [key, value] of pairs) {
                data[key] = value;
            }
        }
        return {
            clear(): void {
                data = createDictionaryModeObject<T>();
            },
            delete(key: string): void {
                delete data[key];
            },
            get(key: string): T {
                return data[key];
            },
            has(key: string): boolean {
                // tslint:disable-next-line:no-in-operator
                return key in data;
            },
            set(key: string, value: T): void {
                data[key] = value;
            },
            forEach(action: (value: T, key: string) => void): void {
                for (const key in data) {
                    action(data[key], key);
                }
            }
        }
    }
```

I bet this is allocating a different closure for each method, wheras a class just allocates space for `data` and shares a single prototype among all instances.
"
">  That's why I want to remove as many of these calls from under the lock as possible, ideally remove the lock itself. The new SetCurrentSolution method makes that possible.

Sure.  I get the goal.  My point is: without understanding more about hte system i don't know if that will just cause other problems.  i.e. if that will mean we switch from deadlocks to race-conditions/corruption.  It may be that deadlocks are in a better state to be in now because we at least strongly know we're in a bad state.  removing the deadlocks, while not being sure about correctness can put us into a worse state where we have incredibly hard to find bugs that we have no idea how to repro. "
"No, I'm sorry I didn't mean it seriously. It just sounded funny that you were justifying functionality for the sake of testing. I realize and appreciate your desire to mitigate risk of breaking changes, particularly unnecessary ones."
"Thanks for the feedback @github, my take is that infrastructure shouldn't cost more than the work it's supporting. The change was motivated by the fact that orchard pays for the overhead of DI as being the top allocation.

I think I've struck a reasonable balance based on your feedback and have adjusted numbers appropriately. I think one last tweak would be to tweak the maximum capacity so the worst case behavior of completely heterogeneous scopes is reasonable (but I think that scenario is more rare). In a typical ASP.NET Core app, the scopes are per request and are similar in size. The trick here is to find limit that cover reasonable size apps but don't harm other use cases. I think ~50 scoped services and a max of 100 initialCapacity is good for this (I will change the current numbers).

"
"Tagging subscribers to 'arch-wasm': @github
See info in area-owners.md if you want to be subscribed.
<details>
<summary>Issue Details</summary>
<hr />

Right now InvokeJS has to marshal the entire expression across the boundary every time, which adds a considerable cost to every invocation, especially if the blob being invoked is pretty big. This PR does an intern check so that if the expression is an interned string, it will only ever be marshaled once and then will live in the wasm bindings' intern table.

In the process of implementing this I discovered that our performance for String.IsInterned is Extremely Suboptimal, so I added a second lookup table that's maintained along the main intern table. The main intern table does value lookups (which means checking whether a string is interned requires looking up its *value* in the intern table) while this new table does pointer lookups, making it closer to constant-time (instead of taking longer for longer strings). The overhead of maintaining this table (along with it not currently being used anywhere else) means we probably want to make it wasm-only, though I'd argue that it might be useful for specific non-wasm scenarios. **At present the bindings rely on String.IsInterned when marshaling all strings across the boundary, which means without this performance optimization they have very bad worst-case performance. If we can't land some form of this PR we will need to remove all the interning support from the bindings.**

I don't like having to use a second table for this - I'd rather tuck the 'is interned' flag into the string itself, perhaps by using the length's unused sign bit. But doing that seems a lot more difficult / risky so I decided not to start with the absolute hardest solution :)

Here are some performance measurements from BDN (some additional benchmarks in a dotnet/performance fork):

```
upstream/main
|                                            Method |     Mean |     Error |
|-------------------------------------------------- |---------:|----------:|
|      InvokeJS_NoResult_NonLiteralString_SameValue | 1.983 us | 0.0135 us |
| InvokeJS_NoResult_NonLiteralString_DifferentValue | 1.968 us | 0.0165 us |
|                                 InvokeJS_NoResult | 1.954 us | 0.0152 us |
|                            InvokeJS_NumericResult | 1.498 us | 0.0087 us |
|                             InvokeJS_StringResult | 8.644 us | 0.0566 us |

this PR without mono_string_instance_is_interned
|-------------------------------------------------- |---------:|----------:|
|      InvokeJS_NoResult_NonLiteralString_SameValue | 2.267 us | 0.0340 us |
| InvokeJS_NoResult_NonLiteralString_DifferentValue | 2.227 us | 0.0167 us |
|                                 InvokeJS_NoResult | 1.170 us | 0.0071 us |
|                            InvokeJS_NumericResult | 1.468 us | 0.0109 us |
|                             InvokeJS_StringResult | 9.505 us | 0.1005 us |

this PR
|      InvokeJS_NoResult_NonLiteralString_SameValue | 2.095 us | 0.0137 us |
| InvokeJS_NoResult_NonLiteralString_DifferentValue | 2.082 us | 0.0156 us |
|                                 InvokeJS_NoResult | 1.043 us | 0.0076 us |
|                            InvokeJS_NumericResult | 1.413 us | 0.0083 us |
|                             InvokeJS_StringResult | 8.471 us | 0.0899 us |
```

You'll note that the worst case (the invoke expression is not interned) is slightly slower, but in practice most InvokeJS expressions should be interned. If we find a way to tuck the interned state into the string object itself I suspect we could bring the overhead down to zero.

Not included are performance numbers for the positive impact this has on the bindings as a whole, because I can't measure those right now (I hit a bug in BenchmarkDotNet). I am confident this will speed them up, and I will do some manual measurements to verify that if we think we're going to land it.

<table>
  <tr>
    <th align=""left"">Author:</th>
    <td>kg</td>
  </tr>
  <tr>
    <th align=""left"">Assignees:</th>
    <td>-</td>
  </tr>
  <tr>
    <th align=""left"">Labels:</th>
    <td>

`* NO MERGE *`, `arch-wasm`

</td>
  </tr>
  <tr>
    <th align=""left"">Milestone:</th>
    <td>-</td>
  </tr>
</table>
</details>"
"> > after we drag the ""debug arrow"" back to sumcheck() after Console.ReadLine() we might end up with a garbage in ecx
> 
> Do we know if the impact will be visible to the user? In your example, I don't think that would be the case.

This problem (screenshot) reproduces on .NET Framework x64 (32bit is fine) and I guess it can be reproduced on CoreCLR as well, just not for this specific repro since Console.WriteLine would clear ecx itself.

I ran a test https://github.com/dotnet/runtime/pull/63698/commits/0f6911443e16556b1064fa8ab893e5489c98bdf9 where I disabled reuseArg completely - as expected it passed all tests and the spmi diffs were quite big - `Total bytes of delta: 28970 (0.09 % of base)` for crossgen libs"
"Fix content pages not Garbage collected
https://github.com/xamarin/Xamarin.Forms/pull/14717"
"There's a certain amount of de-duplication that could be done between these two pages (controller and pages). 

I'm not incredibly keen because to a certain extent these code paths need to each have their own types for everything because of DI/metadata. Open to suggestions/comments ðŸ˜† "
"Hrm, this is really weird, so I'm seeing what looks like garbage in the corehosttrace on some of the failed runs, the logging in the test output looks fine, I ran the test 10 times via test explorer and it only failed 2 times with the mangled parameters.:

```
    | [1.921s] Microsoft.AspNetCore.Server.IntegrationTesting.IIS.IISDeployer Information: [2022-02-03T23:33:23.902Z, PID: 5096] [aspnetcorev2.dll] Initializing logs for 'C:\Github\aspnetcore\src\Servers\IIS\IIS\test\IIS.NewShim.FunctionalTests\bin\Release\net7.0\ANCM\x64\aspnetcorev2.dll'. Process Id: 5096. File Version: 17.0.22034.0. Description: IIS ASP.NET Core Module V2. Commit: 481e0e995e68293460dc1696de7004a2f6d40216.
| [1.921s] Microsoft.AspNetCore.Server.IntegrationTesting.IIS.IISDeployer Information: [2022-02-03T23:33:23.905Z, PID: 5096] [aspnetcorev2.dll] Resolving hostfxr parameters for application: 'C:\Github\aspnetcore\.dotnet\dotnet.exe' arguments: '.\InProcessWebSite.dll' path: 'C:\Users\haok\AppData\Local\Temp\27f8684e044a4966958fc2a5932ab893\'
| [1.921s] Microsoft.AspNetCore.Server.IntegrationTesting.IIS.IISDeployer Information: [2022-02-03T23:33:23.907Z, PID: 5096] [aspnetcorev2.dll] Known dotnet.exe location: ''
| [1.921s] Microsoft.AspNetCore.Server.IntegrationTesting.IIS.IISDeployer Information: [2022-02-03T23:33:23.908Z, PID: 5096] [aspnetcorev2.dll] Process path 'C:\Github\aspnetcore\.dotnet\dotnet.exe' is dotnet, treating application as portable
| [1.921s] Microsoft.AspNetCore.Server.IntegrationTesting.IIS.IISDeployer Information: [2022-02-03T23:33:23.909Z, PID: 5096] [aspnetcorev2.dll] hostfxr.dotnet_root: 'C:\Github\aspnetcore\.dotnet'
| [1.921s] Microsoft.AspNetCore.Server.IntegrationTesting.IIS.IISDeployer Information: [2022-02-03T23:33:23.910Z, PID: 5096] [aspnetcorev2.dll] hostfxr.assembly_path: 'C:\Users\haok\AppData\Local\Temp\27f8684e044a4966958fc2a5932ab893\'
| [1.921s] Microsoft.AspNetCore.Server.IntegrationTesting.IIS.IISDeployer Information: [2022-02-03T23:33:23.912Z, PID: 5096] [aspnetcorev2.dll] get_hostfxr_path failed, trying again with dotnet path as dotnet root.
| [1.921s] Microsoft.AspNetCore.Server.IntegrationTesting.IIS.IISDeployer Information: [2022-02-03T23:33:23.913Z, PID: 5096] [aspnetcorev2.dll] - hostfxr.dotnet_root: ''
| [1.921s] Microsoft.AspNetCore.Server.IntegrationTesting.IIS.IISDeployer Information: [2022-02-03T23:33:23.915Z, PID: 5096] [aspnetcorev2.dll] - hostfxr.assembly_path: 'C:\Users\haok\AppData\Local\Temp\27f8684e044a4966958fc2a5932ab893\'
| [1.921s] Microsoft.AspNetCore.Server.IntegrationTesting.IIS.IISDeployer Information: [2022-02-03T23:33:23.916Z, PID: 5096] [aspnetcorev2.dll] Event Log: 'Application 'C:\Users\haok\AppData\Local\Temp\27f8684e044a4966958fc2a5932ab893\' failed to start. Exception message:
| [1.921s] Microsoft.AspNetCore.Server.IntegrationTesting.IIS.IISDeployer Information: Failed to find host fxr.' 
```

But when I look at the corhost.trace, I see what looks like the timestamps getting passed in:
```
Tracing enabled @ Thu Feb  3 23:33:23 2022 GMT
Using dotnet root parameter [2022-02-03T23:33:23.909Z] as runtime location.
A fatal error occurred. The folder [2022-02-03T23:33:23.909Z\host\fxr] does not exist
Using dotnet root parameter [] as runtime location.
A fatal error occurred. The folder [host\fxr] does not exist
```"
"I wanna be clear. The ConHost behavior is unbearable. cat doesn't work. ls doesn't work. I actually use Hebrew. You can see from the original issue and the way people have commented on thread #538 (and all the closed dupe arabic threads) that nobody really wants the ConHost behavior in place.
So yeah, `vim -H` doesn't work and I don't care. But as for ""Hebrew users"" -- I am one. I have special profiles for fonts that support glyphs because I <3 the current RTL behavior when it doesn't run into font issues. Seriously. Because really, honest to god, 90% at least of my actual, real world, my student job doing Hebrew & Arabic NLP thingies *do work*.
I'm sorry that I've gotten so worked up. But you keep saying stuff like ""Hebrew users will not thank you for this"" and don't stop and consider why I've made three PRs already focused on RTL rendering."
"is this used in IDE side as well or planned to be used? this run this concurrently. we already had this experiment in IDE side and decide generating garbage is too much. and decide to never run analyzers concurrently.
"
"> Instead of waiting until next user op. can we just add the items unilaterally once the computation is done?

I don't think this is possible with async completion API.  The only opportunities we have to update the list are user typing and filter selection. We could talk to Editor team about request to new API though

>  In the worst case, the session should have a single backpointer (either on its property bag, or through a CWT) to a single roslyn object that represents all complex state about it.

Sounds reasonable to me, I can consolidate all properties we current have into a single object"
"`ImmutableArray.CreateBuilder` always ends up with some garbage as the builder itself becomes garbage.  With ArrayBuilder+ToImmutableAndClear (and a known fixed-length) you always end up with a resultant ImmutableArray with no copying and no garbage (because the builder is pooled).  "
"According to the manpage of WC2MB the return value is either the number of bytes written or 0. So, there is no negative value returned. That the worst case size fits into an `int` has been already evaluated for the `lengthRequired` variable, the return value can't be greater than that. `int` has _at least_ the same size as `size_t` and thus, a positive `int` will never lead to change the value if casted to `size_t`."
"I believe you that it's incredibly minor, so I am okay leaving out the optimization. It used to be a std::map, but then it was replaced with an std::array and not revisited. No need to share performance traces as a requirement for landing this :smile:"
"Yeah, there is now a possibility we offer a code action that doesn't end up applying changes, but it's a rare edge case at worst and if we need to clean it up we can do so without reintroducing this problematic dependency."
"> The worst case when having a raise condition, we'll just create a new search handle as we already doing today.

That's my point. It doesn't seem so much like a race condition to me. If a server is doing a lot of concurrent during processing, it could be that just one core gets these speedups and the rest don't, so the benchmarks aren't necessarily meaningful for a solution at scale. Have I misunderstood? That's why I was asking about an alternative that is more consistent."
"> > Config file handling was revamped to fix common issues, and now supports reading TOML.
> 
> Adding options is significantly more powerful with support for things like
> std::tuple and std::array, including with transforms. Several new
> configuration options were added to facilitate a wider variety of apps. GCC
> 4.7 is no longer supported.
> 
> Config files refactored, supports TOML (may become default output in 2.0) https://github.com/CLIUtils/CLI11/issues/362
> Added two template parameter form of add_option, allowing std::optional to be supported without a special import https://github.com/CLIUtils/CLI11/issues/285
> string_view now supported in reasonable places https://github.com/CLIUtils/CLI11/issues/300, https://github.com/CLIUtils/CLI11/issues/285
> immediate_callback, final_callback, and parse_complete_callback added to support controlling the App callback order https://github.com/CLIUtils/CLI11/issues/292, https://github.com/CLIUtils/CLI11/issues/313
> Multiple positional arguments maintain order if positionals_at_end is set. https://github.com/CLIUtils/CLI11/issues/306
> Pair/tuple/array now supported, and validators indexed to specific components in the objects https://github.com/CLIUtils/CLI11/issues/307, https://github.com/CLIUtils/CLI11/issues/310
> Footer callbacks supported https://github.com/CLIUtils/CLI11/issues/309
> Subcommands now support needs (including nameless subcommands) https://github.com/CLIUtils/CLI11/issues/317
> More flexible type size, more useful add_complex https://github.com/CLIUtils/CLI11/issues/325, [https://github.com/CLIUtils/CLI11/issues/370][]
> Added new validators CLI::NonNegativeNumber and CLI::PositiveNumber https://github.com/CLIUtils/CLI11/issues/342
> Transform now supports arrays https://github.com/CLIUtils/CLI11/issues/349
> Option groups can be hidden https://github.com/CLIUtils/CLI11/issues/356
> Add CLI::deprecate_option and CLI::retire_option functions https://github.com/CLIUtils/CLI11/issues/358
> More flexible and safer Option default_val [https://github.com/CLIUtils/CLI11/issues/387][]
> Backend: Cleaner type traits https://github.com/CLIUtils/CLI11/issues/286
> Backend: File checking updates [https://github.com/CLIUtils/CLI11/issues/341][]
> Backend: Using pre-commit to format, checked in GitHub Actions https://github.com/CLIUtils/CLI11/issues/336
> Backend: Clang-tidy checked again, CMake option now CL11_CLANG_TIDY https://github.com/CLIUtils/CLI11/issues/390
> Backend: Warning cleanup, more checks from klocwork https://github.com/CLIUtils/CLI11/issues/350, Effective C++ https://github.com/CLIUtils/CLI11/issues/354, clang-tidy https://github.com/CLIUtils/CLI11/issues/360, CUDA NVCC https://github.com/CLIUtils/CLI11/issues/365, cross compile https://github.com/CLIUtils/CLI11/issues/373, sign conversion https://github.com/CLIUtils/CLI11/issues/382, and cpplint https://github.com/CLIUtils/CLI11/issues/400
> Docs: CLI11 Tutorial now hosted in the same repository https://github.com/CLIUtils/CLI11/issues/304, https://github.com/CLIUtils/CLI11/issues/318, https://github.com/CLIUtils/CLI11/issues/374
> Bugfix: Fixed undefined behavior in checked_multiply https://github.com/CLIUtils/CLI11/issues/290
> Bugfix: ->check() was adding the name to the wrong validator https://github.com/CLIUtils/CLI11/issues/320
> Bugfix: Resetting config option works properly https://github.com/CLIUtils/CLI11/issues/301
> Bugfix: Hidden flags were showing up in error printout https://github.com/CLIUtils/CLI11/issues/333
> Bugfix: Enum conversion no longer broken if stream operator added https://github.com/CLIUtils/CLI11/issues/348
> Build: The meson build system supported https://github.com/CLIUtils/CLI11/issues/299
> Build: GCC 4.7 is no longer supported, due mostly to GoogleTest. GCC 4.8+ is now required. https://github.com/CLIUtils/CLI11/issues/160
> Build: Restructured significant portions of CMake build system https://github.com/CLIUtils/CLI11/issues/394
> Converting from CLI11 1.8:
> Some deprecated methods dropped
> add_set* should be replaced with ->check/->transform and CLI::IsMember since 1.8
> get_defaultval was replaced by get_default_str in 1.8
> The true/false 4th argument to add_option is expected to be removed in 2.0, use -capture_default_str() since 1.8

I was trying to update several deps at once or I would of already done this one. I also verified this change did not cause any issues. But it brings more features and bug fixes that may make our current parser a bit better. I do seriously recommend allowing ingestion of TOML at some point, not just json as it could prove useful in extensions or settings in general."
"```markdown
[tests] enable SubscribeToAppDomainUnhandledException in .NET 6 (#6119)

Context: c1a2ee70214e86757541b5759c9ed54941bd4680
Context: https://github.com/dotnet/runtime/issues/44526
Context: https://github.com/dotnet/runtime/issues/44526#issuecomment-886998881
Context: https://github.com/dotnet/runtime/issues/44526#issuecomment-887052471
Context: start: https://discord.com/channels/732297728826277939/732297837953679412/869330822262587392
Context:   end? https://discord.com/channels/732297728826277939/732297837953679412/869343082552893440
Context: https://github.com/dotnet/runtime/issues/57304

Now that we are calling `mono_unhandled_exception()` when an unhandled
exception is reported (c1a2ee70), we should be able re-enable the
`InstallAndRunTests.SubscribeToAppDomainUnhandledException()` unit
test on .NET 6, which was disabled in 6e3e3831af.

What seemed like it should have been a 1-line removal ballooned a bit
due to a confluence of factors:

 1. Debugger component stubs, and
 2. .NET 6 `Console.WriteLine()` behavior.

On .NET 6, `JNIEnv.mono_unhandled_exception()` is
`monodroid_debugger_unhandled_exception()`, which calls
`mono_debugger_agent_unhandled_exception()`; see also e4debf72.

The problem is that in our current world order of ""Mono components""
(0f7a0cde), if the debugger isn't used, then we get ""debugger stubs""
for the mono debugger agent, which turns
`mono_debugger_agent_unhandled_exception()` into an [*assertion*][0]:

	static void
	stub_debugger_unhandled_exception (MonoException *exc)
	{
		g_assert_not_reached ();
	}

The result is that when an exception is thrown, *before* the
`AppDomain.UnhandledException` event can be raised, the runtime dies
in a horrible flaming assertion death:

	E andledexceptio: * Assertion: should not be reached at /__w/1/s/src/mono/mono/component/debugger-stub.c:175

Avoid this issue by checking `Debugger.IsAttached` *before* calling
`monodroid_debugger_unhandled_exception()`.

Additionally, remove some obsolete comments: .NET 6 couldn't resolve
`Debugger.Mono_UnhandledException()` because .NET 6 never had it, so
the linker was right to warn about its absence.

The problem with .NET 6 & `Console.WriteLine()` is that when format
strings are used, the output may be line-wrapped in unexpected ways;
see also dotnet/runtime@#57304.  Additionally, the `sender` parameter
value differs under .NET 6.  These two issues broke our unit test;
we *expected* `adb logcat` to contain:

	# Unhandled Exception: sender=RootDomain; e.IsTerminating=True; e.ExceptionObject=System.Exception: CRASH

Under .NET 6, `adb logcat` *instead* contained:

	# Unhandled Exception: sender=System.Object; e.IsTerminating=True; e.ExceptionObject=System.Exception
	: CRASH

Yes, `: CRASH` was on a separate `adb logcat` line.

Fix the `SubscribeToAppDomainUnhandledException()` unit test so that
`adb logcat` messages can now span multiple lines (which is sure to
cause all sorts of GC garbage!), and update the expected message so
that we look for the right message under legacy & .NET 6.

Co-authored-by: Jonathan Pryor <jonpryor@vt.edu>

[0]: https://github.com/dotnet/runtime/blob/16b456426dfb5212a24bfb78bfd5d9adfcc95185/src/mono/mono/component/debugger-stub.c#L172-L176
```"
"```markdown
[Xamarin.Android.Build.Tasks, monodroid] LLVM Marshal Methods Infra (#7004)

Context: https://github.com/xamarin/xamarin-android/wiki/Blueprint#java-type-registration

Introduce low-level ""plumbing"" for future JNI marshal method work.

A JNI marshal method is a [JNI-callable][0] function pointer provided
to [`JNIEnv::RegisterNatives()`][1].  Currently, JNI marshal methods
are provided via the interaction between `generator` and
`JNINativeWrapper.CreateDelegate()`:

  * `generator` emits the ""actual"" JNI-callable method.
  * `JNINativeWrapper.CreateDelegate()` uses System.Reflection.Emit
    to *wrap* the `generator`-emitted for exception marshaling.
    (Though see also 32cff438.)

JNI marshal methods are needed for all Java-to-C# transitions.

Consider the virtual `Activity.OnCreate()` method:

	partial class Activity {
	  static Delegate? cb_onCreate_Landroid_os_Bundle_;
	  static Delegate GetOnCreate_Landroid_os_Bundle_Handler ()
	  {
	    if (cb_onCreate_Landroid_os_Bundle_ == null)
	      cb_onCreate_Landroid_os_Bundle_ = JNINativeWrapper.CreateDelegate ((_JniMarshal_PPL_V) n_OnCreate_Landroid_os_Bundle_);
	    return cb_onCreate_Landroid_os_Bundle_;
	  }
	
	  static void n_OnCreate_Landroid_os_Bundle_ (IntPtr jnienv, IntPtr native__this, IntPtr native_savedInstanceState)
	  {
	    var __this = global::Java.Lang.Object.GetObject<Android.App.Activity> (jnienv, native__this, JniHandleOwnership.DoNotTransfer)!;
	    var savedInstanceState = global::Java.Lang.Object.GetObject<Android.OS.Bundle> (native_savedInstanceState, JniHandleOwnership.DoNotTransfer);
	    __this.OnCreate (savedInstanceState);
	  }
	
	  // Metadata.xml XPath method reference: path=""/api/package[@name='android.app']/class[@name='Activity']/method[@name='onCreate' and count(parameter)=1 and parameter[1][@type='android.os.Bundle']]""
	  [Register (""onCreate"", ""(Landroid/os/Bundle;)V"", ""GetOnCreate_Landroid_os_Bundle_Handler"")]
	  protected virtual unsafe void OnCreate (Android.OS.Bundle? savedInstanceState) => â€¦
	}

`Activity.n_OnCreate_Landroid_os_Bundle_()` is the JNI marshal method,
responsible for marshaling parameters from JNI values into C# types,
forwarding the method invocation to `Activity.OnCreate()`, and (if
necessary) marshaling the return value back to JNI.

`Activity.GetOnCreate_Landroid_os_Bundle_Handler()` is part of the
type registration infrastructure, providing a `Delegate` instance to
`RegisterNativeMembers .RegisterNativeMembers()`, which is eventually
passed to `JNIEnv::RegisterNatives()`.

While this works, it's not incredibly performant: unless using one of
the optimized delegate types (32cff438 et. al),
System.Reflection.Emit is used to create a wrapper around the marshal
method, which is something we've wanted to avoid doing for years.

Thus, the idea: since we're *already* bundling a native toolchain and
using LLVM-IR to produce `libxamarin-app.so` (b21cbf94, 5271f3e1),
what if we emitted [Java Native Method Names][2] and *skipped* all
the done as part of `Runtime.register()` and
`JNIEnv.RegisterJniNatives()`?

Given:

	class MyActivity : Activity {
	    protected override void OnCreate(Bundle? state) => â€¦
	}

During the build, `libxamarin-app.so` would contain the function:

	JNIEXPORT void JNICALL
	Java_crcâ€¦_MyActivity_n_1onCreate (JNIEnv *env, jobject self, jobject state);

During App runtime, the `Runtime.register()` invocation present in
[Java Callable Wrappers][3] would either be omitted or would be a
no-op, and Android/JNI would instead resolve `MyActivity.n_onCreate()`
as `Java_crcâ€¦_MyActivity_n_1onCreate()`.

Many of the specifics are still being investigated, and this feature
will be spread across various areas.

We call this effort ""LLVM Marshal Methods"".

First, prepare the way.  Update `Xamarin.Android.Build.Tasks.dll`
and `src/monodroid` to introduce support for generating JNI marshal
methods into `libxamarin-app.so`.  Most of the added code is
*disabled and hidden* behind `#if ENABLE_MARSHAL_METHODS`.

~~ TODO ~~

Other pieces, in no particular order:

  * Update [`Java.Interop.Tools.JavaCallableWrappers`][4] so that
    static constructors aren't needed when LLVM Marshal Methods
    are used.

  * Update [`generator`][5] so that *two* sets of marshal methods are
    emitted: the current set e.g.
    `Activity.n_OnCreate_Landroid_os_Bundle_()`, and an ""overload""
    set which has [`UnmanagedCallersOnlyAttribute`][6].
    LLVM Marshal Methods will be able to directly call these
    ""unmanaged marshal methods"" without the overhead of
    `mono_runtime_invoke()`; see also f48b97cb.

  * Finish the LLVM code generator so that LLVM Marshal Methods are
    emitted into `libxamarin-app.so`.

  * Update the linker so that much of the earlier marshal method
    infrastructure is removed in Release apps.  When
    LLVM Marshal Methods are used, there is no need for
    `Activity.cb_onCreate_Landroid_os_Bundle_`,
    `Actvitiy.GetOnCreate_Landroid_os_Bundle_Handler()`, or the
    `Activity.n_OnCreate_Landroid_os_Bundle_()` without
    `[UnmanagedCallersOnly]`.

Meanwhile, we cannot remove the existing infrastructure, as the
current System.Reflection.Emit-oriented code is needed for faster app
builds, a desirable feature of Debug configuration builds.

LLVM Marshal Methods will be a Release configuration-only feature.

[0]: https://docs.oracle.com/javase/8/docs/technotes/guides/jni/spec/design.html#native_method_arguments
[1]: https://docs.oracle.com/javase/8/docs/technotes/guides/jni/spec/functions.html#RegisterNatives
[2]: https://docs.oracle.com/javase/8/docs/technotes/guides/jni/spec/design.html#resolving_native_method_names
[3]: https://github.com/xamarin/xamarin-android/wiki/Blueprint#java-type-registration
[4]: https://github.com/xamarin/java.interop/tree/main/src/Java.Interop.Tools.JavaCallableWrappers
[5]: https://github.com/xamarin/xamarin-android/wiki/Blueprint#generator
[6]: https://docs.microsoft.com/en-us/dotnet/api/system.runtime.interopservices.unmanagedcallersonlyattribute?view=net-6.0
```"
"Technically, `int.MaxValue` is more than enough: `sysctl(3)` interface was never designed to pass large amounts of data. In the worst case I know (list of all processes and threads in system, or list of all routes on a BGP router) there will be only a few megabytes of data incoming.

The `Sys.Sysctl()` interface specifies `ref int len` already, and changing that would be a different issue anyway."
"> I am not aware of a reason for the register logic equivalent not to have the same issue. It is possible we have an bug there, the code sequence I described above, while legal, is incredibly rare, implying sparse test coverage at best.

See https://github.com/dotnet/coreclr/pull/21959#issuecomment-453713707 for some thoughts on this."
"We generally avoided doing all the .With calls to avoid extra garbage creation, if I recall.
"
"This pointer is potentially pointing into garbage at this point, since it's the same pointer that was passed in to mkdtemp but the array is no longer pinned and could have been moved.  You're not actually using the pointer other than to see if it's null, so it's safe, but I'd suggest not even storing it into a local in order to discourage any such use, e.g.
```suggestion
            if (Interop.Sys.MkdTemp(name) is null)
```"
"I'm not necessarily checking for expected values here, I'm trying to check that the service doesn't throw when you pass invalid things (or anything) to it 'over the wire'. I should probably actually try passing both (And I think I do - the first call to `onMessage` in the loop omits `arguments` which is equivalent to setting it to `undefined` afaik?)

Session is being consumed as a public interface, but `onMessage` is supposed to gracefully handle any errors in the passed string by responding with a message on the wire, rather than throwing. I'm checking for that here in a (mostly generic) way. It would be more complete of me to set up a whole suite of (valid and invalid) inputs and simply test the `onMessage` function, but any e2e tests on `tsserver.js` (if they exist) should already effectively do that and I didn't necessarily want to duplicate work. 

I should also probably add a test for passing garbage strings to onMessage, rather than just json with invalid/empty arguments.
"
"OK, I believe the initial results aren't the worst, the test 46239 still fails on arm but otherwise the Pri1 runs seem mostly clean. I'm working on retesting the change after cleaning up base class size manipulation per JanK's PR feedback and I'm going to look into the arm issue but I believe the change is mostly functional."
"> Is this pattern going to negatively effect the quality of callstacks?

Yes; because local function names become garbage and there is no contract with the C# compiler that the runtime can rely on to turn them back into anything sensible https://github.com/dotnet/roslyn/issues/22857. This came up when turning async callstacks into something nice https://github.com/dotnet/coreclr/pull/14652 /cc @github @github 

I can move them out into regular static methods, and they will show up sensibility if preferred?"
"I agree that it makes sense to lazily grow the collection, and List is probably the best way to do that.

The List should be a lot better in cases with a large RequestQueueLimit but not many queued requests in practice. Even in the worst case when the queue fills up, a List shouldn't be that much less efficient than an array. After all, List is [backed by a single array](https://github.com/dotnet/corefx/blob/04e05d7d0a02f43ce5e6df455052dbfc0862dff7/src/Common/src/CoreLib/System/Collections/Generic/List.cs#L115) anyway. The array just gets reallocated [with double the previous capacity](https://github.com/dotnet/corefx/blob/04e05d7d0a02f43ce5e6df455052dbfc0862dff7/src/Common/src/CoreLib/System/Collections/Generic/List.cs#L410) and copied as the list grows. So there should be at most log(N) reallocations/copies where N is the RequestQueueLimit, and it should be as fast as an array once the queue stops growing."
"> here's no other `awaits` in the top-level statement. We could add an analyzer to suggest `RunAsync` if an `await` is added as a top-level statement.

What happens if another await is added while the blocking `Run` is used? Is there any negative side effect?

> Worst case is one extra blocked thread for the lifetime of the application.

Even with RunAsync, is a thread somewhere is blocked while the app is running? I don't know the mechanics of async+Main and async+top-level."
"@github nevermind, this PR is incredibly well done given the complexity of the matter. I will push your changes to master and then push a change on top with some changes that I think are still needed and ping you on those changes and explain why I did them.

The next step I guess is to flip to `iconv-lite-umd` ðŸ‘ "
"Maybe we should, to avoid the (incredibly rare but absolutely _possible_) case from above? I think it'd be totally unexpected if you duplicated ""Foo"" and ended up in the settings for ""Bar"""
"Remember to submit doc updates: https://docs.microsoft.com/dotnet/api/system.security.cryptography.tobase64transform.cantransformmultipleblocks

(Seriously, I don't know why we doc things like ""always returns `false` as an implementation detail."" If it's an implementation detail and people shouldn't rely on it, we probably shouldn't doc it.)"
"Done.

> you'll need to build up the state if you want to ensure that you can log it at anytime while the process is up

If someone isn't `Dispose`'ing the ServiceProvider, and it gets garbage collected, I don't believe we want to log those providers if someone attaches after the provider has been collected. We really only care about ""active"" ServiceProviders."
"I think we could make this a suppression. The array is immediately cast to `object[]` - this cast will fail if the passed type wasn't a reference type. We can make reference type arrays at runtime because they don't need new code.

The worst case scenario is that instead of InvalidCastException we throw something else. The method should really throw InvalidArgument for a non-reference type anyway."
"@github 
>How confident are we that this won't piss people off? :) the 'keep selection the same' part seems fine to me. The 'move to next line' seems odd. Is this parity with another editor?

It's a fair question. The original design review outcome is here - https://github.com/dotnet/roslyn/issues/36351#issuecomment-513036449

We decided to take this specifically without an option.  I don't recall the exact discussion since it's been a while, but the points in favor of this are
1.  This extension provides toggle comment with similar behavior - https://marketplace.visualstudio.com/items?itemName=JustinClareburtMSFT.HotCommandsforVisualStudio .  This is also similar to the behavior of intellij, though VSCode does not do this.
2.  The command itself is fairly new (introduced midway through dev16) and the existing userbase is not incredibly large (we see ~4k unique daily users).  I think we're less likely to significantly impact muscle memory for a lot of people.

As such I would rather introduce this change and revisit with an option if we do get feedback later on."
"> Do we agree that the test is definitely wrong?

Not sure.  There are plenty of situations where one disposable type wrapping another disposable type doesn't do anything in its Dispose except call Dispose on the wrapped instance, and Dispose is idempotent, so it's acceptable use.  There are also cases where objects are ref counted, e.g. with SafeHandle, and the wrapper would have taken a ref on the wrapped thing such that disposing the wrapped thing wouldn't have actually closed the wrapped thing until after everything was disposed and released ref counts.  I don't know what category these fall into nor what guarantees we've made in docs... but it seems like this works on Windows?

> if it's possible for libgdiplus to detect that it's being used incorrectly

My confusion is that the APIs being used here are managed.  Misuse of a managed API shouldn't cause a seg fault / crash.  If it's actually misuse, worst case it should throw an exception.  e.g. if I misuse FileStream, it shouldn't crash the process (even though FileStream P/Invokes as an implementation detail), worst case a use-after-dispose would produce an ObjectDisposedException.  So I'm trying to understand what makes this special; it's possible there some aspect of this I'm just missing that makes it different :)"
"> Seems fine, but be aware that the bot is going to clobber the lockfile overnight.

At worst, we'll just drift back to less-than-minimal in time, which is fine. We can just do this again when it starts to look hairy. The nightly task shouldn't undo anything in this PR."
"ï»¿The 32-bit operations leave the top 32-bits alone so if they are non-zero then when an int parameter is directly used as uintptr the garbage may be passed to the method. The conversion ensures that the register only contains the i4 value. It only happens very rarely that a combination of the condition when thereâ€™s stuff in the top half and the parameter to be passed is already in the register that will be used as the parameter register  (i.e. if it had been saved to memory and then reloaded then thereâ€™s no problem).



"
"Yes, sorry for the delay.  Attempt to compile https://github.com/dotnet/runtimelab/blob/dd8c43a825c41ccfc21a8ceefa79e232adbe02a1/src/libraries/System.Private.CoreLib/src/System/Span.cs#L117 after LLVM lowering and dump the tree.  So in the rsp for the project add 
```
--codegenopt:NgenDump=*
```
And make sure the code will exercise this ctor.  I'm using the NativeAOT generics project for this.  https://github.com/dotnet/runtimelab/blob/feature/NativeAOT-LLVM/src/tests/nativeaot/SmokeTests/Generics/Generics.csproj

Then after LLVM lowering it will dump the tree and fail with the dump stopping here at the GT_OBJ:

```
Trees after LLVM Shadow Stack Setup

-----------------------------------------------------------------------------------------------------------------------------------------
BBnum BBid ref try hnd preds           weight    lp [IL range]     [jump]      [EH region]         [flags]
-----------------------------------------------------------------------------------------------------------------------------------------
BB01 [0000]  1                             1       [???..???)                                     keep i internal LIR
BB02 [0001]  1       BB01                  1       [000..022)        (return)                     i hascall gcsafe LIR
-----------------------------------------------------------------------------------------------------------------------------------------

------------ BB01 [???..???), preds={} succs={BB02}
N001 (  0,  0) [000000] ------------                 NOP       void

------------ BB02 [000..022) (return), preds={BB01} succs={}
               [000031] ------------                 IL_OFFSET void   IL offset: 0x0
N001 (  1,  1) [000001] ------------                 NO_OP     void
               [000032] ------------                 IL_OFFSET void   IL offset: 0x1
N002 (  3,  2) [000002] ------------         t2 =    LCL_VAR   int    V02 arg2
N003 (  1,  1) [000003] ------------         t3 =    CNS_INT   int    0
                                                  /--*  t2     int
                                                  +--*  t3     int
N004 (  8,  4) [000004] ------------         t4 = *  GE        int
               [000039] ------------        t39 =    LCL_VAR   int    V05 tmp2
               [000040] ------------        t40 =    CNS_INT   int    12
                                                  /--*  t39    int
                                                  +--*  t40    int
               [000041] ------------        t41 = *  ADD       int
                                                  /--*  t41    int
               [000042] ------------        t42 = *  PUTARG_TYPE int
                                                  /--*  t4     int
               [000043] ------------        t43 = *  PUTARG_TYPE bool
                                                  /--*  t42    int    arg-1
                                                  +--*  t43    bool   arg0
N005 ( 22,  7) [000007] --CXG-------              *  CALL      void   System.Diagnostics.Debug.Assert
               [000033] ------------                 IL_OFFSET void   IL offset: 0xd
N001 (  1,  1) [000009] ------------                 NO_OP     void
               [000034] ------------                 IL_OFFSET void   IL offset: 0xe
               [000044] ------------        t44 =    CNS_INT   int    4
               [000045] ------------        t45 =    LCL_VAR   int    V05 tmp2
                                                  /--*  t45    int
                                                  +--*  t44    int
               [000046] ------------        t46 = *  ADD       int
                                                  /--*  t46    int
N001 (  3,  2) [000011] ------------        t11 = *  IND       byref
                                                  /--*  t11    byref
N003 (  7,  7) [000015] DA----------              *  STORE_LCL_FLD byref  V04 tmp1         [+0] Fseq[_value]
               [000035] ------------                 IL_OFFSET void   IL offset: 0x15
               [000047] ------------        t47 =    CNS_INT   int    0
               [000048] ------------        t48 =    LCL_VAR   int    V05 tmp2
                                                  /--*  t48    int
                                                  +--*  t47    int
               [000049] ------------        t49 = *  ADD       int
                                                  /--*  t49    int
N001 (  3,  2) [000010] ------------        t10 = *  IND       byref  Zero Fseq[_pointer]
               [000050] ------------        t50 =    CNS_INT   int    8
               [000051] ------------        t51 =    LCL_VAR   int    V05 tmp2
                                                  /--*  t51    int
                                                  +--*  t50    int
               [000052] ------------        t52 = *  ADD       int
                                                  /--*  t52    int
N003 (  3,  2) [000016] -------N----        t16 = *  OBJ       struct<System.ByReference`1[System.Char], 4>
```
Because `Data` is not set on the GT_OBJ, it may not be `nullptr`,  `OperIsBlkOp` returns true and it then tries to indirect `Data()` at `AsBlk()->Data()->gtSkipReloadOrCopy()` which fails (as the `Data()` is garbage).  GT_OBJ has no `gtOp2`.  This change ensure that no attempt is made to `Data()` for ops that don't have a `gtOp2`.
"
"> What is the mov qword ptr [rsp+28H], rax line doing? Isn't it storing just garbage?

That's just my inability of manually writing diffs ðŸ˜„ The `xor      rax, rax` line stays

![image](https://user-images.githubusercontent.com/1142958/75151836-836f9a80-56ff-11ea-8528-4b02a651e0a9.png)
"
"Based on feedback I pivoted to storing the is-interned flag inside of MonoThreadsSync and inflating all interned strings. This means that if a string doesn't have a MonoThreadsSync we can early-out and return 0 (it can't be interned) and if it's inflated we just check the bool. This avoids having to take any locks to do the intern check and eliminates the hashmap.

From local measurements, this is a bit faster for my test cases, and it should be much faster in the worst cases (big intern table, big non-interned string, etc).

```
|                                            Method |       Mean |    Error |
|-------------------------------------------------- |-----------:|---------:|
|      InvokeJS_NoResult_NonLiteralString_SameValue | 2,024.7 ns | 30.76 ns |
| InvokeJS_NoResult_NonLiteralString_DifferentValue | 1,973.9 ns | 11.67 ns |
|                                 InvokeJS_NoResult |   913.0 ns |  6.65 ns |
|                            InvokeJS_NumericResult | 1,358.3 ns | 21.34 ns |
|                             InvokeJS_StringResult | 8,258.4 ns | 83.33 ns |
```"
"> it adds around +1 Mb: 9.8Mb -> 10.8Mb (+10%)

It means that the actual code size regressed by about 15% (the R2R binary has more that just the code). It feels like a lot.

Have you looked at asm diffs and examined several different methods with the worst size regressions?"
"@github You never thought it would get this complicated when you volunteered... :smile: Seriously though, well done and thanks!"
We were getting around concatenating these before in a seriously ugly way. I can explain if needed. This is much better and actually sets the automation name to the full thing (whereas before we were just tricking Narrator into reading the right thing instead of setting the automation name correctly).
"> I find it highly questionable that people would write code that needs culture sensitive results in en-us. And even if they did, i'm ok with that scenario regressing given the gain here.

I do not disagree with this. However, I am still generally concerned with any code that singles out en-US. It's a bad precedent that puts us in to a ""works on my machine"" state when interpreting feedback.  I checked the VS app domain manager code that sets CurrentUICulture based on configured VS language and, AFAICT, it leaves CurrentCulture alone. This means that this optimization (as currently written) will miss a lot of English VS users outside the US. We could tweak it to detect if the VS language is English, but then we'd still miss a not uncommon case of non-English VS with ASCII-only patterns and data.


> At worst the code performs the same as today for non-en-us. But for the common case of en-us and english code, we get a boost.

en-US is ""common"", but nowhere near ""99.9%"" mentioned elsewhere. I will forward you some data by email. 

I'd like to discuss other ways to achieve this optimization for a greater number of users. Here's my quick brainstorm on that:

* You mentioned elsewhere that there is an index that is built for the search. Is it possible to determine if the data is all ASCII during indexing and then use Ordinal only when that is the case and the pattern is all ASCII?

* Could we make ordinal vs culture-sensitive search an option that can be toggled by the user irrespective of their locale or VS language? Make Ordinal the default?

* Could we optimistically use ordinal and only fall back to culture-sensitive when no matches are found?



"
"> what (if any) scenarios worked before this PR, that do not work after

About the only things that change at all are scenarios involving switches between branches for which all needed runtimes are already in .dotnet/. In those cases, it's better to use `./build.* -projects {path}` instead of or `./build.* -noBuildManaged -noBuildNative` before `dotnet build`. But, the worst case would be using a different Microsoft.NETCore.App and that can only occur if the two branches began at different 'master' commits with dependency updates between."
The compiler no longer ships to the LTS SDK by default: only to `netcoreapp3.1` and forward. Any changes we `netcoreapp2.1` would be a servicing event but it's incredibly unlikely the compiler will meet the servicing bar. Instead we'd fix in current SDK and tell customers to use new SDK to build for `netcoreapp2.1` (this is not special to Roslyn but instead just how our SDK layer works)
"This pattern is done because the containing method is typically marked with `AggressiveInlining`.

The codegen for the accelerated case is generally 1-2 instructions and so it is something we want to always inline and the codegen for all the paths + the fallback typically makes the method to big for that to happen naturally. The codegen for the fallback case, however, is often of unknown size or is used in scenarios where inlining might not be beneficial and so we want the JIT to make the inlining decision on its own.

So, this pattern ensures that the accelerated case is always inlined and that the fallback case is ultimately inlined at the JITs disgression. Due to the containing method being `AggressiveInlining`, the ""worst"" case is a direct call to `SoftwareFallback`."
"The main thing I'm worried about is ""micro-benchmark"" vs ""real-world"".

Its pretty easy to define a microbenchmark that shows AVX-512 or AVX2 is faster. The concern is then how that impacts everything that runs after that is done.

For example, given the numbers above AVX-512 is 2-5x faster (depending on if compared to AVX2 or SSE). However, that is ~10ns vs ~50ns. If the downclocking can last up to 700 microseconds, that is 14000x longer than the SSE scenario: https://www.google.com/search?q=700+microseconds+%2F+50+nanoseconds and the question is whether the gains outweigh potentially running the entire processor at 50-75% speed for that much longer.
* This is naturally assuming a worst or near worst case scenario, it may not actually be that bad; I just think it is something that we should carefully measure and with real world workloads, rather than just micro-benchmarks"
"Worst-case, isn't the required length appx s.Length * 2 (i.e. a separator between every char)?  If so, this can be done simply with:
```C#
char[] arr = ArrayPool<char>.Shared.Rent(s.Length * 2 - 1); // worst-case length for a separator between every char
int pos = 0;
...
arr[pos++] = ...; // every time a char is added
...
string result = new string(arr, 0, pos);
ArrayPool<char>.Shared.Return(arr);
return result;
```"
"when IsComplete is false, it means we have no idea whether we got all references or not. so, we should assume worst which is we didn't get all references.
"
in the worst case it does 8 probes (decreasing 1Gb by 128Mb each iteration and increasing startAddress by 8mb) and bails out
i am a stupid person. typescript is garbage. Bill should be fired.
"Overall points:

1. the general design seems reasonable to me.
2. i woudl like if we could simplify the code somewhat.  
3. it's often unclear what the relationship is between several seemingly similar optinos are.  keeping track of what every boolean combo of each means is difficult.
4. i really dislike that we are randomly adding/removing data to random session property bags.  it feels like that should not be necessary at all.  In the worst case, the session should have a single backpointer (either on its property bag, or through a CWT) to a single roslyn object that represents all complex state about it.  Then we can keep track of everything in our roslyn object, and never need to do things like remove session properties and whatnot."
"Before I close this and seriously consider woodworking as a future career development direction, @dotnet/ncl there's a mildly interesting failure in the WASM legs - because we enable `-Werror` for CMake's configure checks (on the line that I'm deleting in this pull request), our check for `HAVE_SYS_POLL_H` always fails on MUSL-based targets - not because there's no poll.h, but because MUSL suggests us to `#include <poll.h>` instead, like POSIX says and issues a warning (causing us to go the `!HAVE_SYS_POLL_H` paths instead, because of treating the suggestion as error). Not sure if this has any material impact on the product on MUSL-based targets.

(I do wonder how many other CMake configure checks only fail because there is a warning.)"
"The answer won't ever be incredibly cut and dry btw. You can configure analyzers so, you can actually change the level of any error, in addition to suppressing the warning. "
"This is missing the following line from the previous implementation:
`s_resolutionList = new List<KeyValuePair<Layout, int>>();`

Without that line, `s_resolutionList` will hold references to every element that ever gets a layout pass, so they can never be garbage-collected. 

"
"I'm gonna say let's pull this for simplicity of this PR, and revisit in a follow-up.

(also, I'm thinking it should be next/previous, but seriously, lets have that discussion on another thread ðŸ˜‰)"
"@github So it appears you hit a fun case we haven't hit before.

Please change it to be long.MaxValue and i'll have to do some additional work after this PR to fix up the generator to do the right thing.

I know the review's been a bit brutal, but we have to get everything right the first time since we take API compatibility very seriously. 
"
"These two blocks do the same thing.. either this one can be simplified to not loop, or they could just be combined, or at worst, skip the `@` check for raw strings (though not sure that is worth bothering)"
"that's not the only goal.  a strong goal is to prevent wastage even when we need to generate new arrays.  in my testing we're continually creating temp arrays and purging them in all these cases due to the 128 item limit on normal pooling.  

i could triviall get around that by *always* pooling, but that was greatly worrying for me.  Imagine a case where for some reason we need to have a scratch array of 100k items at one point, but from taht point on, all future scratch arrays are like 100 elements.  We'd hold onto those 100k elements forever.  This approach allows that large allocation to decay out and eventually be recovered.

However, if we need a scratch array of 100k elements, and we're constantly producing around that many items on each incremental pass, then we def want to preserve that scratch buffer so that we don't constantly throw it away, and regen it, producing massive amounts of garbage and GC churn (note: these ar enot hypothetical concerns.  my other PRs are about directly addressing that being one of hte highest perf hits here) :)"
"â“ why do we need to dispose of the CTS here? Cancel being called should be sufficient, and it can be cleaned up with garbage collection as needed while safely passing a cancelled token around. "
"In some cases we're switching from `string` to `ROS` and back to `string`. Can we accept `string?`.
```

        internal static void ThrowNotFound(string? path)
        {
            // Windows distinguishes between whether the directory or the file isn't found,
            // and throws a different exception in these cases.  We attempt to approximate that
            // here; there is a race condition here, where something could change between
            // when the error occurs and our checks, but it's the best we can do, and the
            // worst case in such a race condition (which could occur if the file system is
            // being manipulated concurrently with these checks) is that we throw a
            // FileNotFoundException instead of DirectoryNotFoundException.
            bool directoryError = path is not null && !Directory.Exists(Path.GetDirectoryName(Path.TrimEndingDirectorySeparator(path)));
            throw Interop.GetExceptionForIoErrno(new Interop.ErrorInfo(Interop.Error.ENOENT), path, directoryError);
        }
```"
"Pfffffffffft, `getPropertiesOfType` isn't recursive or anything, it's just getting members (and not the types of those members). _Most of the time_, it _is_ just a `symbol.exports` or `symbol.members` lookup; it just also handles all the times when that's _not_ the case (which is when it's an expression), and imo generally they're cases worth handling. It's not worse than calling `checkExpressionCached` on every `export = <expr>` position (since that's all it desugars to in the 'worst' case, essentially)."
"> EDIT: There is a small regression on Utf8JsonReader.Get* methods caused by the addition of the helper *Core methods. This can be potentially mitigated by removing the helpers and just duplicate the code.

If this is still the case, let's duplicate the code for the worst regressions i.e. >= 4%."
"Looks like you're right - worst case, browsers are required to supply the empty string: https://w3c.github.io/FileAPI/#file-attrs."
"HttpRequestMessage.Properties/Options allocates once accessed, not the worst thing as it's client-side, but worth noting. The only other way I can think of is to add a header during negotiate, but that's super ugly and bleeds into the network calls."
"Done.
Added the following comment:
```
Upper bound that limits the byte buffer size to prevent an encoding that has a very poor worst-case scenario.
```

Now that makes me question if we should limit the char buffer on the Read side for the same reason."
"> Oh I'm all for making this the cheap and approximate check. I just wasn't sure if FAR still does call IsSameAssemblyOrHasFriendAccessTo after we have the Compilation but before we actually walk trees and do other expensive stuff. I'd hate to send FAR down a wild goose chase if it was missing the check.

No, it doesn't do that check later.  We do always check that the reference is a true reference.

> I also have no sense in the ""real world"" what sorts of strangeness is out there.

I think having the uncommon cases pay this price instead of having everyone pay this price is desirable.  

Again, for this to trigger, you would need ot have such a project arrangement where you had a project that said it had IVT to another project with the same name, but didn't intend for it to see internals.  And in that case, all that would happen is that we will end up checking that project, where the majority of the time will be getting the compilation for it.  And that's the cost we're already paying *today*, just that we pay it no matter what.  

i.e. today we go to all these potential matches, and we then make a compilation (the most expensive part of find refs) just to ask ""should we really check you?"".  This PR says: yes, we should check you, and pushes the getting of the compilation until teh point that we really actually would need to bind and determine if a symbol matches.  

So, in the case where the downstream project *does* reference the name that we're looking for, we pay the same price.  But in the common case that the downstream project doesn't, we are much much cheaper.  

i.e. if i'm searching for a public symbol from MS.CodeAnalysis, then wasting all this computation to determine if we have IVT is irrelevant and wasteful.  *And*, if i'm searching for an internal symbol, then precomputing the compilation for all downstream projects prior to even determining if there's a potential match in them is also wasteful.  

Currently, getting the compilation is the most expensive part of FAR.  Anything that pushes that off until absolutely necessary is seriously beneficial."
This should be stronger than an Assert.  Since we're guarding against Windows giving us garbage we should guard against this being invalid as well.  We should take min of numBytes and buffer.Length below.
"are we seriously adding another converter
for the life of me
come on"
"That won't override values set on the command-line. `msbuild /p:_LlvmNeededWindows32=Garbage` *always* takes precedence, *and* is **immutable**."
"> 
> 
> A choice snippet from `ConptyOutputTests::WriteAFewSimpleLines()`:
> 
> ```
>     // Here, we're going to emit 3 spaces. The region that got invalidated was a
>     // rectangle from 0,0 to 3,3, so the vt renderer will try to render the
>     // region in between BBB and CCC as well, because it got included in the
>     // rectangle Or() operation.
>     // This behavior should not be seen as binding - if a future optimization
>     // breaks this test, it wouldn't be the worst.
> ```
> 
> Yep, that's what I broke/fixed.

Updating the code as follows, fixes this one and the `ConptyRoundtripTests` copy:
```
    expectedOutput.push_back(""AAA"");
    expectedOutput.push_back(""\r\n"");
    expectedOutput.push_back(""BBB"");
    // Jump down to the fourth line because emitting spaces didn't do anything
    // and we will skip to emitting the CCC segment.
    expectedOutput.push_back(""\x1b[4;1H"");
    expectedOutput.push_back(""CCC"");

    // Cursor goes back on.
    expectedOutput.push_back(""\x1b[?25h"");
```

@github, @github, the only thing I didn't expect was the cursor to go back on. It wasn't there before in this test. Is it expected for the cursor to go back on when drawing is done? Under what circumstances should it not do that? "
"> 
> 
> Yeah, our squash workflow can be a pain for stacked branches. A merge _should_ work, though.
> 
> Worst case, a rebase?

It will work.. no worries - I was just surprised with the amount of conflicts ðŸ˜„ "
"> We will avoid using ""andn"" when one of the operands is (likely to be) in memory: ""and""'s RMW form provides for a shorter instruction sequence in that case.

We have the following:
```
and  r/m32, r32             ; 1-byte opcode
and  r32,   r/m32           ; 1-byte opcode
andn r32a,  r32b, r/m32     ; 4-byte opcode
mov  r/m32, r32             ; 1-byte opcode
mov  r32,   r/m32           ; 1-byte opcode
not  r/m32                  ; 1-byte opcode
```

Where `n-byte opcode` is the ""base encoding cost"" of the instruction. Encoding the register or memory information is the same additional cost on top of that. And also considering the decoding, dispatching, and pipelining costs (`not, and` is two instructions and are dependent on eachother, etc).

In the ""worst case"", we have all three coming from memory (destination and both operands) and so the for `not, and` you have:
```
mov reg, [mem]
not reg
and reg, [mem]
mov [mem], reg
```
vs
```
mov reg, [mem]
andn reg, reg, [mem]
mov [mem], reg
```
so 3-bytes larger, but taking 1 less cycle

Basically every encodable scenario is the same except for ""only the destination is memory"" where you have:
```
not reg
and [mem], reg, reg
```
vs
```
andn reg, reg, reg
mov [mem], reg
```

and where you may have faster throughput due to `and` also covering the store and being ""much smaller"""
"Do we permit this?

```csharp
[Explicit]
ref struct Foo
{
    [FieldOffset(0)] object o;
    [FieldOffset(0)] ref int x;
}
```

I think this has the same problem as:

```csharp
[Explicit]
ref struct Foo
{
    [FieldOffset(0)] object o;
    [FieldOffset(0)] IntPtr x;
}
```

I think both have the same kinds of problems (setting the value through the other field to a non-object-pointer value is probably going to be bad when GC happens). If it was up to me, I wouldn't allow that.

We don't care much if byref points to a garbage, but objref pointing to garbage is not good."
"It looks like we will need to revert this change until TS 4.4 while we investigate the significant performance regression this causes during emit. In the worst case, we may need to land this behind a flag to opt-in to the change, though ideally I can find a mitigation for the performance regression so that we can land this without a flag."
"> We could add the $(TargetFramework) as part of the intermediate path and that would fix the XSLT task,

Probably also needs the target OS (when split) so it doesn't fail when building net5.0-Linux and net5.0-OSX at the same time.  Though it looks like that should already be taken into account for the IntermediateOutputPath variable... so I'm surprised that there are two builds running concurrently for the same thing.

e.g. I see this when building the Cng project:

Property reassignment: $(IntermediateOutputPath)=""C:\git\bartonjs\runtime\artifacts\obj\System.Security.Cryptography.Cng\net5.0-Windows_NT-Debug\"" (previous value: ""C:\git\bartonjs\runtime\artifacts\obj\System.Security.Cryptography.Cng\Debug\"") at C:\git\bartonjs\runtime\eng\BeforeTargetFrameworkInference.targets (12,5)

oooh.... I wonder if the problem goes away if you move that PropertyGroup to inside the Task.  I think this property set evaluates too early and is getting the pre-substitution IntermediateOutputPath (a consequence of factoring it out).

> but that moves the concurrence problem to the Copy task.

Thankfully the Copy task is conditional on the intermediate file having a diff from the target file, so that race wouldn't happen in any build that wasn't changing an input... and since Copy retries (and the two sources are identicial) it just means a local build would (worst case) get one of the ""File in use, retrying"" warnings if that race hit."
"I don't know where you are seeing ""one line changed"" in this commit https://github.com/dotnet/roslyn/pull/36067/commits/d55ad54453d58fb2a978323d1844db4239fc3209 but something on your Github diff is seriously broken ðŸ˜‰ "
"Both of your suggested ways. [sharplab](https://sharplab.io/#v2:EYLgxg9gTgpgtADwGwBYA0AXEBDAzgWwB8ABAZgAJiAmcgFRlwwFgAoAb1fK8oEYlzgECABtyAYQAWMMAGsAklACWAVQB2iyABMYAJWyqA5jB4AKMBOxRyExQYkBlAK5QoacucvlhEAO5OXbhCOGAJCooq4/lAQBtgYMAAK2IquoSLkEQCKjjBQAJ4AlJzcHCzc5RkAZuQmEVExcYnJVgC87hZQAHRyuAAyvvWx8Sbefs5QBUVlFVylMzOOiqohwZWkNG0mi8sFHl1iEKoAbrkYtBDKGGtUJjZ2UW6jUQUA3KzF89zEAOw1W1frchwcgABgQPBBkJBBXIAB5NmCeAAxFEAESBoPBUOh5EIhA+nwq/2uGLBVGxMPhNTJKKR6OBZIpuPx00JRKyOXy5AAZNyCWyZiZiYCGQgkUyqSYwbS6aSxUy8fyBeVhTRRRCJQisbT6ZiNVDJpM3qyKgBfJVcH7kSrYYS4GDG8rmk3EPhpUSSaTyJRqDQQbR6QwwG57ay2BzjNyhp6R8hBEKCdJ1cYNeJJFJuRPhXDZXKFAlzebJlyppopchtPbdPoDFNDGAjWsuV4Wqo1YvRevpiatwts7YrAEbGoD3YdToHY6nc6Xa63cMPLxNibG1vlK1C1YivWa6ngnVy/U4xUm5WquXkg1wrVUA+iy+QmEn5Uqjl5nl808vribody8VXpK0p3piAGPsya4Cue6rYjiQHamih6wYaLZflwzqfFaNp2g6BIYV8bpZuIUiyAoKjqFouj6EYpBmB0Yb3LG0bLoEwTuhkkR1o03aZmEHG5vkUzlH2FSKNUdGWNW/RjCW9aNjJExCfMImfB2pbdhW5AYFAOSOgKA7kFuw5bEsGBjpJk4nFAZwXEO86MQES4KahkGWr8v4kjBgFasiiFeeBz7fiOf73ruUoILefmYg+x4skF7Y5pyeQfq5hIedu0phcBUWZVegXxT+RlIVlCGyv5j6GnpMzOq2alds0mnYfaVUVFhtrNXhrCmkAA=).

`if (isSurrogatePair = char.IsLowSurrogate(lowSurr))` looks the worst out of the three."
"We ignore what the user has specified as the ProgID and use the type name to set `SOFTWARE\Classes\CLSID\<clsid>\ProgID`, but add a key under `SOFTWARE\Classes\` using the specified ProgID? Seems odd to me - is this seriously what RegAsm does? "
"It is typically profitable to pool objects that are big (e.g. byte[] buffers) and/or that are allocated very frequently. Pooling these MemoryHandle arrays across the whole app is neither.

Global pooling is replacing one cost with a different cost. The objects in the pool make the garbage collector work harder. It is not always a win, especially for small objects like what these MemoryHandle[] arrays are typically going to be. Unless you have done a performance measurements that show that the global pooling is improving actual performance (e.g. requests per second), I would not introduce it just because you can.
"
"Some callers expect `GetMaxCharCount(byteCount)` to be the maximum number of characters that might be converted from any call to `Encoding.GetDecoder().GetChars(buffer_of_byteCount_length, some_output_buffer)`. `StreamReader` makes this assumption, for instance.

`DecoderNLS` can hold partial state between calls to `GetChars` if a non-ASCII byte is seen. There are two possible outcomes here:

1. The internal state is never completed and represents the maximum invalid subsequence of a UTF-8 buffer. The Encoding instance will replace the entire captured state with a single `'\uFFFD'` character before processing the rest of the input buffer.

2. The internal state is 3 bytes of a 4-byte sequence, and the first byte of the incoming buffer would complete the sequence. This means the output would contain _2_ characters: the high & low surrogates.

In both scenarios, the worst-case expansion is that the internally captured state results in +1 additional character needed in the output."
"What is the `mov      qword ptr [rsp+28H], rax` line doing? Isn't it storing just garbage?"
"Based on the review comments it seems I haven't done a good job of explaining what I'm trying to accomplish here. 

A compilation event is a muli-process problem for C# / VB. The MSBuild process (of which there are multiple) begins the compilation event by constructing a message that it sends to the VBCSCompiler process (of which there is one). The compiler receives the request, kicks off a `Thread` and then processes the compilation from there. Upon completion it constructs a response and sends it back to MSBuild. 

The log file contains messages from all these different MSBuild processes and all of the threads within VBCSCompiler interleaved. The only identifying marker for message in the log is the process and thread ID. That means reading a log file requires you to figure out which PID maps to which process (MSBuild or VBCSCompiler) and which TID within specific PID (the VBCSCompiler) maps to a given compilation event. That doesn't help a lot thouh though with the core problem which is typically trying to find the compilation that failed then look at all of the log entries for that compilation. Basically filter the log down to all of the events for a compilation and see where it went wrong. 

This PR attempts to address that by doing the following:

- Replace the PID in the log file with a concrete name: either `MSBuild <PID>`, `csc / vbc <PID>` or `VBCSCompiler`. Now at a glance can tell which process wrote the line
- Use a request id (simple `Guid.NewGuid()`) to track a compilation event across the processes. It is created by MSBuild, attached to the protocol and then every process puts that `Guid` into messages when they're writing a log event specific to a request. That makes it incredibly easy to filter out the log file to the events for a given process 

Below is a sample of how the log file looks after this change. Notice that `c8d7ad91-490f-420c-9a96-f3881a8804ca` is the request for a given compilation. I can easily just search for that in the log file, read through the messages now and easily see ever hop along the way to understand how this event was processed. 

```
ID=MSBuild 9120 TID=5: Begin writing request for c8d7ad91-490f-420c-9a96-f3881a8804ca
ID=MSBuild 9120 TID=5: End writing request for c8d7ad91-490f-420c-9a96-f3881a8804ca
ID=MSBuild 9120 TID=5: Begin reading response for c8d7ad91-490f-420c-9a96-f3881a8804ca
ID=VCBCSCompiler TID=7: Received request c8d7ad91-490f-420c-9a96-f3881a8804ca of type Microsoft.CodeAnalysis.CommandLine.BuildRequest
ID=VCBCSCompiler TID=41: 
Run Compilation for c8d7ad91-490f-420c-9a96-f3881a8804ca
  Language = C#
  CurrentDirectory = 'D:\workspace\_work\1\s\src\Compilers\Core\Portable
  LIB = ''
ID=VCBCSCompiler TID=41: Begin Analyzer Consistency Check
ID=VCBCSCompiler TID=41: End Analyzer Consistency Check
ID=VCBCSCompiler TID=41: Begin c8d7ad91-490f-420c-9a96-f3881a8804ca C# compiler run
ID=VCBCSCompiler TID=41: End c8d7ad91-490f-420c-9a96-f3881a8804ca C# compiler run
Return code: 0
Output:
Microsoft (R) Visual C# Compiler version 3.10.0-ci.21212.1 (<developer build>)
Copyright (C) Microsoft Corporation. All rights reserved.


ID=VCBCSCompiler TID=7: Writing Completed response for c8d7ad91-490f-420c-9a96-f3881a8804ca
ID=MSBuild 9120 TID=14: End reading response for c8d7ad91-490f-420c-9a96-f3881a8804ca
ID=MSBuild 9120 TID=8: CompilerServer: server - server processed compilation - c8d7ad91-490f-420c-9a96-f3881a8804ca
ID=VCBCSCompiler TID=1: Client request completed
ID=MSBuild 5956 TID=3: End reading response for 25074eec-9cf6-4fd9-b5f5-c9dbfd5aea78
ID=MSBuild 8408 TID=24: End reading response for 59d13f52-ec71-43df-b120-3a061af351c8
ID=MSBuild 5956 TID=3: Error Error: 'EndOfStreamException' 'Reached end of stream before end of read.' occurred during 'Reading response for 25074eec-9cf6-4fd9-b5f5-c9dbfd5aea78'
Stack trace:
   at Microsoft.CodeAnalysis.CommandLine.BuildProtocolConstants.<ReadAllAsync>d__4.MoveNext() in /_/src/Compilers/Core/CommandLine/BuildProtocol.cs:line 623
--- End of stack trace from previous location where exception was thrown ---
   at System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess(Task task)
   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)
   at System.Runtime.CompilerServices.ConfiguredTaskAwaitable.ConfiguredTaskAwaiter.GetResult()
   at Microsoft.CodeAnalysis.CommandLine.BuildResponse.<ReadAsync>d__5.MoveNext() in /_/src/Compilers/Core/CommandLine/BuildProtocol.cs:line 340
--- End of stack trace from previous location where exception was thrown ---
   at System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess(Task task)
   at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)
   at System.Runtime.CompilerServices.ConfiguredTaskAwaitable`1.ConfiguredTaskAwaiter.GetResult()
   at Microsoft.CodeAnalysis.CommandLine.BuildServerConnection.<TryCompileAsync>d__6.MoveNext() in /_/src/Compilers/Shared/BuildServerConnection.cs:line 289
```"
"I don't think it is a particular test @github. The general sequence is like this:
SslStrem is disposed and that calls Dispose on `SafeDeleteSslContext()`

https://github.com/dotnet/runtime/blob/acdb8696b02f18f2dc34286fb4ba3bd26ad14db4/src/libraries/System.Net.Security/src/System/Net/Security/SecureChannel.cs#L173-L184

That sets the pointer to null:
https://github.com/dotnet/runtime/blob/a9c5eadd951dcba73167f72cc624eb790573663a/src/libraries/Common/src/System/Net/Security/Unix/SafeDeleteSslContext.cs#L58-L70

There is pending encrypt/decrypt operation on another thread. In the past I think the p/invoke would fail and I think we would eventually wrap that in ObjectDisposed exception (or pass the raw error about closed safe handle. The safe handle was sufficient to guard agains passing garbage/null to OS call. "
"@github PTAL
cc @dotnet/jit-contrib 

SPMI sees ~300 cases across all the collections. Regressions are small.
```
33 total methods with Code Size differences (32 improved, 1 regressed), 1 unchanged.
15 total methods with Code Size differences (14 improved, 1 regressed), 3 unchanged.
89 total methods with Code Size differences (78 improved, 11 regressed), 7 unchanged.
94 total methods with Code Size differences (85 improved, 9 regressed), 8 unchanged.
107 total methods with Code Size differences (98 improved, 9 regressed), 9 unchanged.
149 total methods with Code Size differences (148 improved, 1 regressed), 1 unchanged.

Total bytes of delta: -1217 (-6.41% of base)
Total bytes of delta: -192 (-1.41% of base)
Total bytes of delta: -1221 (-1.17% of base)
Total bytes of delta: -1279 (-1.32% of base)
Total bytes of delta: -1647 (-1.63% of base)
Total bytes of delta: -7183 (-10.01% of base)
```
Sample diff. In the Before picture, `IG06` has 3 preds; two for which RAX is null, and one where it's not null. The compare in `IG06` in VN space is a constant compared to a PHI. The opt scans through the phi and discovers one of the phi input VNs was redundantly compared in a dominating block, and so we try jump threading, and that succeeds...
```asm
;;; Assembly listing for method System.IO.Pipelines.Pipe:GetReadAsyncStatus():int:this

;;; BEFORE

G_M35282_IG05:        ; gcVars=0000000000000000 {}, gcrefRegs=00000002 {rcx}, byrefRegs=00000000 {}, gcvars, byref, isz
       ; gcrRegs +[rcx]
       add      rcx, 216
       ; gcrRegs -[rcx]
       ; byrRegs +[rcx]
       mov      rax, gword ptr [rcx]
       ; gcrRegs +[rax]
       test     rax, rax
       je       SHORT G_M35282_IG06
       mov      rdx, 0xD1FFAB1E
       cmp      qword ptr [rax], rdx
       je       SHORT G_M35282_IG06
       xor      rax, rax
						;; bbWeight=0    PerfScore 0.00
G_M35282_IG06:        ; gcrefRegs=00000001 {rax}, byrefRegs=00000000 {}, byref, isz
       ; byrRegs -[rcx]
       test     rax, rax
       je       SHORT G_M35282_IG08
       mov      eax, 2
       ; gcrRegs -[rax]
						;; bbWeight=0    PerfScore 0.00
G_M35282_IG07:        ; , epilog, nogc, extend
       ret      
						;; bbWeight=0    PerfScore 0.00
G_M35282_IG08:        ; gcVars=0000000000000000 {}, gcrefRegs=00000000 {}, byrefRegs=00000000 {}, gcvars, byref
       mov      eax, 1
						;; bbWeight=0    PerfScore 0.00
G_M35282_IG09:        ; , epilog, nogc, extend
       ret      
						;; bbWeight=0    PerfScore 0.00

;;; AFTER

G_M35282_IG05:        ; gcVars=0000000000000000 {}, gcrefRegs=00000002 {rcx}, byrefRegs=00000000 {}, gcvars, byref, isz
       ; gcrRegs +[rcx]
       add      rcx, 216
       ; gcrRegs -[rcx]
       ; byrRegs +[rcx]
       mov      rax, gword ptr [rcx]
       ; gcrRegs +[rax]
       test     rax, rax
       je       SHORT G_M35282_IG07
       mov      eax, 2
       ; gcrRegs -[rax]
						;; bbWeight=0    PerfScore 0.00
G_M35282_IG06:        ; , epilog, nogc, extend
       ret      
						;; bbWeight=0    PerfScore 0.00
G_M35282_IG07:        ; gcVars=0000000000000000 {}, gcrefRegs=00000000 {}, byrefRegs=00000000 {}, gcvars, byref
       ; byrRegs -[rcx]
       mov      eax, 1
						;; bbWeight=0    PerfScore 0.00
G_M35282_IG08:        ; , epilog, nogc, extend
       ret      
						;; bbWeight=0    PerfScore 0.00
```
Jit dump for the above
```
... [hasInterestingVN] in BB06 relop first operand VN is PhiDef for V05
N003 (  5,  4) [000054] N------N----              *  EQ        int    $250
N001 (  3,  2) [000052] ------------              +--*  LCL_VAR   ref    V05 tmp4         u:2 (last use) $281
N002 (  1,  1) [000053] ------------              \--*  CNS_INT   ref    null $VN.Null
... phi input [0] has VN $280
... substitute VN is $24b
... phi search succeeded after checking 1 cases

Dominator BB03 of BB06 has relop with an interesting liberal VN:
N003 (  5,  4) [000042] J------N----              *  EQ        int    <l:$24b, c:$24a>
N001 (  3,  2) [000041] ------------              +--*  LCL_VAR   ref    V05 tmp4         u:1 <l:$280, c:$2c0>
N002 (  1,  1) [000040] ------------              \--*  CNS_INT   ref    null $VN.Null
Implies current relop is partially redundant compare
N003 (  5,  4) [000054] N------N----              *  EQ        int    $250
N001 (  3,  2) [000052] ------------              +--*  LCL_VAR   ref    V05 tmp4         u:2 (last use) $281
N002 (  1,  1) [000053] ------------              \--*  CNS_INT   ref    null $VN.Null
Both successors of IDom BB03 reach BB06 -- attempting jump threading
BB03 is a true pred
BB04 is a false pred
BB05 is a false pred
BB05 is the fall-through pred
Optimizing via jump threading
Jump flow from pred BB03 -> BB06 implies predicate true; we can safely redirect flow to be BB03 -> BB08
Setting edge weights for BB03 -> BB08 to [0 .. 0]
Jump flow from pred BB04 -> BB06 implies predicate
```
Still need to look at TP here as in the worst case (large number of PHI inputs, deep dominance tree) we could be doing a lot of searching."
"One test is failing on Linux x64:

```gdb
Running Test: ThreadLocalStatics.TLSTesting.ThreadLocalStatics_Test

Thread 2 ""DynamicGenerics"" received signal SIG34, Real-time event 34.
[Switching to Thread 0x7f0b60b3a700 (LWP 186231)]
futex_wait_cancelable (private=<optimized out>, expected=0, futex_word=0x5628c407bd4c) at ../sysdeps/nptl/futex-internal.h:183
183     ../sysdeps/nptl/futex-internal.h: No such file or directory.

(gdb) thread apply all bt

Thread 3 (Thread 0x7f0b5a23f700 (LWP 186232)):
#0  __libc_read (nbytes=1, buf=0x7f0b5a23ed7f, fd=5) at ../sysdeps/unix/sysv/linux/read.c:26
#1  __libc_read (fd=5, buf=0x7f0b5a23ed7f, nbytes=1) at ../sysdeps/unix/sysv/linux/read.c:24
#2  0x00005628c2c0dddf in ?? ()
#3  0x00007f0b80f03609 in start_thread (arg=<optimized out>) at pthread_create.c:477
#4  0x00007f0b80e28163 in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95

Thread 2 (Thread 0x7f0b60b3a700 (LWP 186231)):
#0  futex_wait_cancelable (private=<optimized out>, expected=0, futex_word=0x5628c407bd4c) at ../sysdeps/nptl/futex-internal.h:183
#1  __pthread_cond_wait_common (abstime=0x0, clockid=0, mutex=0x5628c407bd50, cond=0x5628c407bd20) at pthread_cond_wait.c:508
#2  __pthread_cond_wait (cond=0x5628c407bd20, mutex=0x5628c407bd50) at pthread_cond_wait.c:638
#3  0x00005628c2be9813 in GCEvent::Impl::Wait(unsigned int, bool) ()
#4  0x00005628c2be92a3 in GCEvent::Wait(unsigned int, bool) ()
#5  0x00005628c2b6ef87 in WKS::GCHeap::WaitUntilGCComplete(bool) ()
#6  0x00005628c2b5bcf7 in RedhawkGCInterface::WaitForGCCompletion() ()
#7  0x00005628c2b679c6 in Thread::WaitForGC(PInvokeTransitionFrame*) ()
#8  0x00005628c2b69710 in RhpWaitForGC2 ()
#9  0x00005628c2dcccfa in S_P_CoreLib_System_Runtime_InternalCalls__RhpSignalFinalizationComplete ()
#10 0x00005628c2dcb094 in S_P_CoreLib_System_Runtime___Finalizer__ProcessFinalizers ()
#11 0x00005628c2b58fce in FinalizerStart(void*) ()
#12 0x00007f0b80f03609 in start_thread (arg=<optimized out>) at pthread_create.c:477
#13 0x00007f0b80e28163 in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95

Thread 1 (Thread 0x7f0b80ce9880 (LWP 186227)):
#0  0x00005628c2ba64b7 in WKS::gc_heap::mark_object_simple1(unsigned char*, unsigned char*) ()
#1  0x00005628c2ba80f2 in WKS::gc_heap::mark_object_simple(unsigned char**) ()
#2  0x00005628c2bacc2c in WKS::GCHeap::Promote(Object**, ScanContext*, unsigned int) ()
#3  0x00005628c2be249f in PromoteObject(Object**, unsigned long*, unsigned long, unsigned long) ()
#4  0x00005628c2be0338 in ScanConsecutiveHandlesWithoutUserData(Object**, Object**, ScanCallbackInfo*, unsigned long*) ()
#5  0x00005628c2be054f in BlockScanBlocksWithoutUserData(TableSegment*, unsigned int, unsigned int, ScanCallbackInfo*) ()
#6  0x00005628c2be1764 in SegmentScanByTypeChain(TableSegment*, unsigned int, void (*)(TableSegment*, unsigned int, unsigned int, ScanCallbackInfo*), ScanCallbackInfo*) ()
#7  0x00005628c2be195f in TableScanHandles(HandleTable*, unsigned int const*, unsigned int, TableSegment* (*)(HandleTable*, TableSegment*, CrstHolderWithState*), void (*)(TableSegment*, unsigned int, unsigned int, ScanCallbackInfo*), ScanCallbackInfo*, CrstHolderWithState*) ()
#8  0x00005628c2bdbe47 in HndScanHandlesForGC(HandleTable*, void (*)(Object**, unsigned long*, unsigned long, unsigned long), unsigned long, unsigned long, unsigned int const*, unsigned int, unsigned int, unsigned int, unsigned int) ()
#9  0x00005628c2be3523 in Ref_TraceNormalRoots(unsigned int, unsigned int, ScanContext*, void (*)(Object**, ScanContext*, unsigned int)) ()
#10 0x00005628c2bdaea6 in GCScan::GcScanHandles(void (*)(Object**, ScanContext*, unsigned int), int, int, ScanContext*) ()
#11 0x00005628c2b98452 in WKS::gc_heap::mark_phase(int, int) ()
#12 0x00005628c2b93e06 in WKS::gc_heap::gc1() ()
#13 0x00005628c2ba3ebd in WKS::gc_heap::garbage_collect(int) ()
#14 0x00005628c2b88a31 in WKS::GCHeap::GarbageCollectGeneration(unsigned int, gc_reason) ()
#15 0x00005628c2bd1e55 in WKS::GCHeap::GarbageCollectTry(int, int, int) ()
#16 0x00005628c2bd1cb2 in WKS::GCHeap::GarbageCollect(int, bool, int) ()
#17 0x00005628c2b594d4 in RhpCollect ()
#18 0x00005628c2dcca39 in S_P_CoreLib_System_Runtime_InternalCalls__RhpCollect ()
#19 0x00005628c2dcc9ec in S_P_CoreLib_System_Runtime_InternalCalls__RhCollect ()
#20 0x00005628c2c5bf94 in S_P_CoreLib_System_GC__Collect_0 ()
#21 0x00005628c2c375e0 in DynamicGenerics_ThreadLocalStatics_TLSTesting__ThreadLocalStatics_Test ()
#22 0x00005628c2c4b843 in DynamicGenerics_EntryPointMain___c___Main_b__0_62 ()
#23 0x00005628c2c4713d in DynamicGenerics_CoreFXTestLibrary_Internal_Runner__RunTestMethod ()
#24 0x00005628c2c46c3f in DynamicGenerics_CoreFXTestLibrary_Internal_Runner__RunTest ()
#25 0x00005628c2c469b4 in DynamicGenerics_CoreFXTestLibrary_Internal_Runner__RunTests ()
#26 0x00005628c2c260fd in DynamicGenerics_EntryPointMain__Main ()
#27 0x00005628c305c107 in DynamicGenerics__Module___MainMethodWrapper ()
#28 0x00005628c305c1a3 in __managed__Main ()
#29 0x00005628c2b56fdf in main ()
```"
"Hi @github,

I'm not comfortable with some of those changes to the merge routine.

Some of them are just removing unnecessary code given the assumption that the second parameter is always the currently folded ranges. I wrote the code with an eye toward it being more general and eventually replacing the merge in syntaxRangeProvider with this merge. Because their purpose is the same and the merge in syntaxRangeProvider doesn't catch all bad cases. But removing the currently unnecessary bits is harmless for now, they just may be needed again later.

I'm uncomfortable with removing the test ""&& nextA.endLineNumber === nextB.endLineNumber"" and instead using rangeA whenever the start line number is the same. Two issues:
    1. Suppose we have a top of fold line like ""if (a) { if (b) {"". I.e. there are two fold open markers on one line. I know that's bad code but it is far from the worst things I've seen :)
    Fold the region.
    Now remove the ""if (a) {"" part of the line. With the old code it would unfold. With the new code it will stay folded, including whatever code was folded past the end of the ""b"" section.
    2. It is a problem if ever using the merge for more general purposes as above. (The second parameter is supposed to be definitive given any differences.)

I'm also uncomfortable with removing the two pass logic and the ""} else if (nextB.isCollapsed && !nextB.isManualSelection && passNumber === 1) {"" section. I think that the new code will now incorrectly keep a folded range in some cases. E.g. start with:
```
    if (something) {
        a = b;
    }
```
Fold the ""if"" line.
Delete the ""i"" in ""if"". The folded region will be converted to a manual fold with the new code. I think the previous approach did better by unfolding it.
"
"> Would it then be beneficial, long term, to get this as an approved change to the language spec?

What updates would we make though? Today expressions of type `nint` and `nuint` are legal according to the C# spec because they have implicit conversions to `long` and `ulong` respectively. Hence they fall into the following rule:

> each expression must be of type int, uint, long, ulong, or must be implicitly convertible to one or more of these types

This bug is just about how array indexes using these types are emitted as IL, essentially we could be a lot more efficient here. Hence that is what the PR should be focusing on changing. I'm definitely supportive of that change given that it's at worst execution neutral and reduces the size of emitted IL (which is a current goal of the runtime team). But don't see a need at the moment to bring language changes with it.

I can see some value over the long term in adjusting both the language space and ECMA 335 to allow `nint` and `nuint` as array indexes. Essentially make the language spec line up with the ideal way that the runtime wants to see the IL. That's a convo that would need to start on csharplang though. 

"
"For 95% of the use cases, things have worked fine without one (or the garbage path we did last release). I'm just concerned given the large nature of the BCL means that somehow the addition of one will break something. 

I might be paranoid though. I guess I can nuke it. 
"
"I'll do a follow up with file parsing, because I don't think we have a good helper here. For now worst case we can't go to the file path on click"
"But there are too many parameters, `previousToken` `previousKind` `currentKind`, in a similar check later: `currentToken` `currentKind` `previousKind`, if cut the kind parameter for corresponding token, then another call to `.Kind()` would be introduced,  in worst would introduce 2+2 calls in total. There are code paths like `currentKind == Comma && HasFormattableBracketParent(currentToken)`, I don't think adding another check to ensure the nearby token not being OmittedArraySizeExpressionToken needs a separate method.

What do you think?
"
"> could you add Null and Undefined to TypeFlags.IncludesMask to avoid the second check?

Nope, because we're looking for members-of-members that are undefined-or-null. We'd need 3 (or two) brand new flags (null and undefined are already used to track weather the intersection itself contains them). One for ""every member is a union"", one for ""every member contains `undefined`"", and one for ""every member contains `null`"". We only have two unused type flags left (all currently assigned type flags have a meaning in intersection construction), sooo.... Yeah.

This already bails early in every common scenario, the worst case where it's not needed is soemthing like a 52 element union where the first 51 contain `undefined`, but the 52nd contains `null`, which is irreducible and, notably, will trigger the ""excessively large union"" check in the `else` clause once the filtering fails, and cease to do _any_ more work. So the ""worst case"" is either the case where it needs to occur and all the work needs to be done, or where it's very close to that but not, and will likely be converted to `any`, anyway."
"> Well we also need to consider size of build info before we start storing all these details .. so storing them isnâ€™t practical esp when there are some huge projects with lots of package jsons

By this do you mean a single Project (one project in a solution of projects) can have up to 1 one package.json for each source directory under it's root? So the worst case scenario would be one package.json per source file?

Or is only one recognized package.json allowed per project?

I had thought it was 1 recognized per project (always adjacent to or above the project tsconfig), but I am aware I could be wrong.

IF it is only one package.json per project, then using the hash of the stringified collection of relevant properties would result in minimal impact on tsbuildinfo size."
"> I think using LayoutKind.Explicit is going to be incredibly error-prone.

Yeah, that's what I thought too. What we could do is to introduce SPC only `LayoutKind.Runtime` and teach linker to rewrite fields types wich this value. If we go with a separate struct the linker could actually even convert `LayoutKind.Runtime` to `LayoutKind.Explicit` and add used offsets.

Or teach linker about MonoRuntimeLayoutAware instead because I'm not sure what happens when C# compiler encounters StructLayout with an unknown value."
"Yes, we probably need to do something on the allocation path as well. Trying to attempt to steal regions from other heaps may end up being a bit messy though, perhaps we can do some cleanup at the end of a GC?

> for large pages where the reserve space is the most precious, we cannot decommit but we can still coalesce
Right, I think in this case we need to call memclr on the space to be recycled, or is there another way? Manipulating the used/allocated fields on the region is dangerous because of possible coalescing by the region allocator.

> it seems like all the checks in should_delete_region are pretty hard to hit unless the VA space is small so I'm wondering what scenario you are running that made it hit the VA limit that needs to be handled in region_allocator (vs for example getting a large region from say another heap since we haven't implemented that). can you elaborate a bit on the scenario you are running? is it allocating a big object and you've set the region range to be smaller than the default? what do the free region pools look like when you got OOM?

I was running the GC stress test (ReliabilityFramework.dll) on a checked build using the 32 core / 64 processor box. I haven't changed the region range - we have 65535 units, and we run out after about an hour. The small region free lists on the heaps look ok, but we have several heaps where the large region free lists have hundreds of entries.

> calling decommit_heap_segment_pages instead of virtual_decommit will update heap_segment_committed/heap_segment_used appropriately and you don't need to call memclr (the allocator will naturally do memclr if needed which will also not be done during pause time)

I started out using decommit_heap_segment (not decommit_heap_segment_pages), but it didn't decommit the first page, so I would still have to call memclr on that. It looks like decommit_heap_segment_pages would have the same issue. Not decommitting the initial pages is a problem because of later coalescing - you may end up with garbage in the middle of a region."
"NIH bro. But more seriously, the IIS targets had a bunch of things that look like they are very IIS specific (copying launch settings \ injecting deps file etc). All we need here is to publish a project with different settings. I did re-use the `PublishedApplicationPublisher` which majorly speeds up things."
"Everything it has read access to is already public, and all the branches we care about have branch protection. I expect the worst it could do is spam our issues/PRs."
"I guess. But that's true of nearly all of the features, no? We don't verify that the feature has been configured weirdly. In theory, most features could be implemented to return null values, but it would be incredibly cumbersome to use these APIs if they were all marked as nullable. In the ordinary case, it's either non-null or throws, no?"
"> CancelAfter(-1) stands for infinite timeout

Yes, but that timeout is the only thing that would cause cancellation to be requested.  If there's an infinite timeout, there's no need for the CancellationTokenSource at all, nevermind calling CancelAfter(-1).

> can't we rely on the garbage collector to recycle the resource?

Yes, but when you call CancelAfter (with a non-infinite timeout), you're causing the CancellationTokenSource to create a Timer under the covers.  If you don't dispose of that CTS, the Timer won't know it's no longer needed, and will continue taking up resources until it eventually fires."
"@github it's the addition of this loop and the call to `compilation.GetAssemblyOrModuleSymbol` that breaks things.  I really can't tell why.  I've spent hours deep in the reference manager, but it's all incredibly cryptic to me.  "
"How often is this codepath hit?  How much memory could we possibly leak?

Are we hitting this codepath *every time* we *invoke* a P/Invoke to a `__Internal` method, in which case we could be hitting this code path *thousands*-*millions* of times, which could be a significant memory leak?

Or do we hit this code path *every time* we *resolve* a P/Invoke to a `__Internal` method, in which case we could *at most* leak dozens of instances, *maybe*?  (Though each instance is probably on a separate code page, so that could still add upâ€¦)

What's our worst-case scenario here?"
"> Converting 60.nnn into 59.nnn sounds like the worst possible option (others being 59.999 or 59.9999999). From the original code structure it looked like an oversight not an intentional decision?

You never know what is the intentional of the constructor caller. At least we are sure this is not coming from `UtcNow` as we handle that there. We change the second to 59 because we had no other options and it is easy to explain. Changing the milliseconds can be confusing especially if this is not desired by the caller."
"> 
> 
> Hey @github. I'm really sorry that we've frustrated you like this. You put together some amazing PRs, and so consistently, and you deserve better than that. I could try and give excuses for taking so long (SUI sprint, baby not sleeping, attempted overthrow of democracy), but at the end of the day, this is on **me**. I dropped the ball by not prioritizing reivews fast enough, and I'm sorry for that.
> 
> Trying to clear out the PR backlog has been a personal quest of mine for a while. Since coming back from the holidays, I've been more distracted and just haven't given it the time it needs. I'll do better. I wanted to get the team to give me a second qualitative opinion on this particular PR during our team sync this week, but that got cancelled, (and now that I think on it, next week's needs to be moved too, @github). I'll summon the rest of the team this morning to see if they can give some feedback (when they wake up). Broader though, I'll try and be better about tapping other members of the team in on community PRs, and faster, so in the future you're not blocked on just mine and Dustin's time.
> 
> As far as PR scope goes, just keep doing what you're doing honestly. Big ones like this, like the startup actions, like some of the cmdpal work, they're all really impressive and end up moving the needle in big jumps. The small fixes are still great too! The selection dismissing for example - that's a great example of something quick that just needed someone's eyes to find the right place to fix it.
> 
> In my (very specific) opinion, the best thing you can do to get more timely reviews is _find someway to make my baby sleep_ ðŸ˜†. Seriously though, there's a _lot_ of work to in the search dialog, and this PR is a great example of making that experience a lot better. Now it's on **us** to make sure that your experience as a contributor is better.
> 
> Thanks for sharing. We'll do better ðŸ˜Š

@github - thanks for the kind words! And you absolutely should not be sorry - my feeling are subjective and I never shared them till now! :blush: 

Moreover, everything is a matter of expectations, this is my first open source project  and I am still calibrating :blush:

In any case, there's no urgency to cleanup the backlog right now - I was addressing the future work, and not just for me but probably for other contributors as well (which might feel similar). 

The Terminal community is great with tons of ideas and reports, but I do see only several robust external code contributors (and j4james who is not a contributor but a code monster). I believe that the project deserves a bigger code contributing community: after all this is the product we use every day, the standards are great and there are enough low hanging fruits to start contributing (though I still don't know how to approach some of your ""Easy Starters""). At least for me the major challenge is around interaction.

I know that you have SMEs, thus I am not sure if it is applicable, but making more devs engaged in PRs can help. In addition, even if there is no PR capacity, setting an assignee or giving some ack / keep-alive can improve the clarity... and I am already starting to  suggest solutions, though I promised not to :smile:

Finally, regarding the baby who refuses to sleep - my suggestion is to have a second baby. Usually this leads to two babies who do not sleep, but.. you stop caring about that anymore :smile:"
"> I am not aware of a reason for the register logic equivalent not to have the same issue. It is possible we have an bug there, the code sequence I described above, while legal, is incredibly rare, implying sparse test coverage at best.

Okay. That makes sense. I would still run this by you before making the change. Consider the original case you pointed out with those same assumptions
//         vfmadd213pd ymm0, ymm1, ymmword ptr[V02 rsp+20H] # last ymm0 write
//         vmovupd  ymmword ptr[V01 rbp-50H], ymm0  #  instructionA
//         vmovupd  ymm0, ymmword ptr[V01 rbp-50H]  # instructionB

What I believe I can use
idGCref() for an instruction tells me if that instruction creates a GC Live location/register. Now, we are dealing with 2 cases 1. Register write(creating a live register) 2. Stack write(creating a  live variable)(need some other checks like emitGCrFrameOffsMax etc). 

My understanding of what you're suggesting
1. Initially ymm0 is not GC live
2. vmovupd  ymmword ptr[V01 rbp-50H], ymm0   // this stack write turns var V01 live
3. vmovupd  ymm0, ymmword ptr[V01 rbp-50H] // this register write turns ymm0 GC live

Considering the above - conditions where I'll have to skip the optimizations

very conservative solution:
If both movs create GC live, do not apply optimization
if(needsGC(instructionA->idGCref())) && if(needsGC(instructionB->idGCref()))// maybe augment it with additional check for stack write

Probably the better way to go
check previous instruction which creates ymm0 and if ymm0 is live. In this example something like
vfmadd213pd ymm0, ymm1, ymmword ptr[V02 rsp+20H]

if ymm0 is not live, then execute logic above(conservative solution)

Does this make sense or am I misunderstanding anything here?
"
"Actually, that creates additional garbage from the dots we *don't* use, which is all dots except the last one in a JSDoc-syntax generic. These dots are relatively very rare, so I will save the start and end instead and manually specify the error span."
"I just decided to have declaration emit on to capture the true ""garbage in, garbage out"" aspect of error suppression. Again, this was already doable with multiple individual `//@ts-ignore`s so...."
"You can't, but I don't want to proceed up the tree any higher than the closest element; the user could totally put a completion at the closing tag.

Feasibly, I could remove the check here, and at worst it'd walk up the tree further than it needs, and do the same thing, though."
"I agree that these would be helpful for a performat node type check (compared to is) but wouldn't that fall out of DU work? - if we redefine tree types in terms of closed hierachies we can take advantage of compiler optimization on type checks (lowered to interger comparison)

Another distant alternative would be using custom positional patterns, so we could do `node is EqualsExpressionSyntax ..`

Worst thing about this is that it doesn't help with `switch` which to me is a deal breaker."
"> Is it as cheap to allocate a pinned array as it is an unpinned one? 

There two ways that allocating pinned array is more expensive:
- The path length of the allocation is longer (it is not a helper written in hand-optimized assembly)
- The pinned heap is treated as Gen2. It means that you need to do the more costly Gen2 GC to collect the garbage in it."
"@github 
> The initial change we had in mind for this API would be simply exposing the static readonly fields

Oh! That de-complicates this a lot. Guess I was reading a bit too much into it, thinking that this canâ€™t be seriously all thatâ€™s being asked for. Iâ€™m fine either way though. If adding parsing logic would have some possible drawbacks, then Iâ€™m happy to remove that again from the PR.

Should I keep the UriParser instances though, for a possible future expansion and to be able to reference the scheme names that way, or should the fields just be hardcoded completely?

@github 
> The build errors are due to needing to add the new members to the System.Runtime.cs reference source.

Thanks for that, I didnâ€™t get to check that last night and just wanted to get something out first. Did you write the changes manually or is there some tool that generates that automatically from the public interface? I remember there being something similar in the AspNetCore repos that automated this.

> Anything specific that we can help with?

So far, I just had problems all over the place. After adding some more bits that I was missing, I was following the [library quick start workflow](https://github.com/dotnet/runtime/blob/c7242f00176cac72e79629ff515d0c517f685e7e/docs/workflow/building/libraries/README.md) and at least was able to build the runtime and the library from the console. I wasnâ€™t able to launch the System.Private.Uri solution in Visual Studio though since that apparently didnâ€™t pick up the self-compiled runtime and as such couldnâ€™t open the projects (I donâ€™t need a preview VS for this, do I? The [requirements](https://github.com/dotnet/runtime/blob/c7242f00176cac72e79629ff515d0c517f685e7e/docs/workflow/requirements/windows-requirements.md) just specify 16.6, so current stable should work).

Anyway, Iâ€™m not sure if I missed something there but I didnâ€™t have the time today to retry it properly. I will have more time for this tomorrow though, so I will play around more and see if I get everything running properly, so that I can finish the PR then. Iâ€™ll let you know if I hit any other walls. I hope that this small PR will help me get the tooling running well enough so that I can see if I can find some more things to do in the future :blush:

(Btw. Iâ€™m not too happy that I need to compile C++ just to work on the .NET part of the repo; it would be cool if I could just use a pre-build SDK/runtime and just needed to compile those bits that I am working on)"
"> checking if the check and extends types are identical

Full identity checking is almost as expensive as normal relationship checking in the worst case (because of signature unification and the checks resulting from that, and these bad-perf examples almost always start with signature unification...), so that wouldn't save much, I don't think (as swapping to identity checks hasn't helped me improve perf on similar things in the past). Minimally, I think it'd just be moving the problem to a different class of types, since `identity` relations still have a class of types that relate slowly (namely, differing instantiations of a still-generic type).

> or if the extends type is a union that contains the check type

Which, again, would require identity checks on the union constituents, which, to do correctly, has the same issue as above (and raises big questions of why something like `T extends T | null` would be eagerly evaluated but not `T extends U | null` when `U extends T` - minimally it's _another_ refactoring hazard for conditional types). â˜¹ï¸  There's no easy fix like this, I don't think - it's all just kicking the can around on where the slowness lies or what our ability to check code is, and while this moves the can for certain examples, it breaks others - I know for a fact that as-is this'll break any `react` project relying on assignment to a higher-kinded `Props` types with a `ReturnType` inside it.

The fundamental issue at play here is that we're failing to identify an expensive, deep, comparison as such and bail appropriately early in the comparison with a `Maybe` (or an error). Fundamentally, doing these checks isn't what makes us slow - checking the the types we construct to do these checks is what makes us slow; so this feels like a bit of a cudgel that's trying to make the issue less apparent, but doesn't get at the root cause. Take [the repo you linked earlier](https://github.com/jonohill/tsc-bug-44851/blob/main/src/main.ts). Yes - it's slow, and it became slow when we started performing these comparisons; however, if we construct another example where we construct the same types we make in these checks, that's _also_ slow. The reason it's slow is because the _massive_ intersection-of-unions we end up making makes us make types with huge alternatives for their member types, and comparing those takes quite a long time. I could easily manufacture those same types without needing conditional types at all. This doesn't really get at the true root cause of the problem as-is. _In fact_, if you go into ajv and replace every conditional type in `UncheckedJSONSchemaType` (the expensive type in question) with a union of the branches of those conditionals (which is what they'd probably do if we didn't have conditionals), we actually issue a pile of `Excessive stack depth comparing types 'UncheckedPartialSchema<?>' and 'UncheckedPartialSchema<?>'` errors after taking more than half the time we take to check with the conditionals in place - and I'm sure if we didn't (more performantly) decide to bail out, we'd take just as long. So the issue isn't these comparisons we're performing per sey, but rather that we're not as good at giving up quickly when conditionals are in play."
"This is the general pattern in [the docs](https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/implementing-disposeasync), though this exact case of sync-calling-async isn't present."
"Yeah I don't think it would be a performance difference, at best it would make the code more concise and at worst it might make it easier to miss a code path where we forget to initialize it. I'll leave it for now since it's consistent with how we do it for request headers."
"I commented on the gist as well, but this greatly depends on the machine you are running against, the input data, and a number of other factors.

On my box (I can run on several others as well):
```
BenchmarkDotNet=v0.12.0, OS=Windows 10.0.18363
AMD Ryzen 9 3900X, 1 CPU, 24 logical and 12 physical cores
.NET Core SDK=5.0.100-alpha.1.20064.2
  [Host]     : .NET Core 5.0.0 (CoreCLR 5.0.20.6307, CoreFX 5.0.20.6307), X64 RyuJIT  [AttachedDebugger]
  DefaultJob : .NET Core 5.0.0 (CoreCLR 5.0.20.6307, CoreFX 5.0.20.6307), X64 RyuJIT


|           Method |                Input |     Mean |    Error |   StdDev |
|----------------- |--------------------- |---------:|---------:|---------:|
|    StringVersion | 18446744073709551615 | 44.64 ns | 0.235 ns | 0.220 ns |
| ConditionVersion | 18446744073709551615 | 58.82 ns | 1.199 ns | 1.516 ns |
|       ROSVersion | 18446744073709551615 | 43.57 ns | 0.291 ns | 0.227 ns |
```

The main issue with the ""Condition"" version is that it involves two branches and its predictability depends greatly on the input. Alternating hex and numeric digits is basically the worst case scenario.

It could likely be improved by changing to a single branch, such as the following:
```
        value &= 0xF;
        value += '0';
        
        if (value > '9')
        {
            value += ('a' - '0');
        }
        
        return (char)value;
```
In this scenario, the branch predictor has less to worry about and we still aren't doing any extra work.

An even better approach, especially since the inputs will likely always be fairly unpredictable would be to use `cmov` and avoid branching altogether. This is what GCC, Clang, and MSVC do; as an example (but is namely only beneficial for inputs that can't be accurately predicted). However, I don't believe the JIT has support for this today."
Incredibly nit-picky: There are extra spaces at the end of this line.
"I feel like there are a lot of places in this PR where we call `load_dso_from_override_dirs()` then `load_dso_from_app_lib_dirs()` if `load_dso_from_override_dirs()` fails.

Perhaps we should have a helper which does that?  `load_dso()`, perhaps?  (Which admittedly *already* exists, but I'm seriously getting confused trying to keep all these new functions straight.)"
"Should you assert the magic number you're using for the non BF case? Looks like -1. And throw otherwise. (You could pick something more exotic than -1 possibly.)

That way you could give a better error than later when you try to read garbage."
"â” How is this first invoked? Can we make sure that one call to this occurs immediately after a document is opened and/or `roles` changes? It's really frustrating how completion for unimported types/members never works the first time it's used in a document, and the worst thing possible here would be extending that behavior to all completion operations."
"This isn't ideal... worst case it could be O(N^2).  Instead, you can do something like `signalRegistrations[i] = null`, and then after the loop, do `signalRegistrations.RemoveAll(item => item is null);`, or something like that."
why not just create OutliningSpans for every region when we are building them instead of creating the extra object. less garbage to collect later.
"The ""problem"" is that material-ui has some types that are large unions ~700 types, and at least some have ~100 optional types. `discriminateTypeByDiscriminableItems` has complexity `O(number of types in union x number of discriminators)` and they way I naively implemented this extension required checking all of them. This new version only adds member names that discriminate against different sets of types in the union. This trick doesn't change the worst case complexity of this change, but it should prevent common / degenerate cases like material-ui.

It's not clear to me that the performance could be improved any more, so if it's still bad somehow, or is deemed too complex for the handful of edge cases it helps, I'm cool with abandoning in.

As an implementation side note, I needed a way to hash subsets of the union type, and I did this by forming a string with space delimited indices. This works, but I'm not sure if there's a better way to achieve this same result."
"Need to look at this in detail, but should we treat collection interfaces as being FromBody? Itâ€™s not incredibly common to try and get optional services in your action, much more common to want to read an `IList<T>`"
"i'm virtually certain that this can't happen. Indeed, all we're doing is checking for System_Nullable_T.  If you look at the compiler, for example, they assume all over the place that this means you certainly can safely get that type arg back.

The main reason i'm complaining here is that this means we now have this weird null/tri-state pattern for this method.  I'd rather us not have to have that.  Worst case, even if we're paranoid and we see this, we just default to assuming it's mutable (or immutable) or something.  It's basically fine to have this be a boolean and not a tristate."
that's probably the worst decision we ever agreed on
"> I prefer to not surface this as a NullReference exception

That's fine, but we're seriously busted at this point, right? Should we consider throwing an unreachable or something?

---
In reply to: [616239428](https://github.com/dotnet/roslyn/pull/52685#discussion_r616239428) [](ancestors = 616239428)"
"Maybe I'm misreading, but the new test case appears to cover the scenario where the `#r` is missing.  I was asking about the scenario where the `#r` is present but the argument is bad (e.g. `#r ""garbage.dll""`).
"
"This is leaking the converted strings. You may want to instead convert the managed string to byte[] that gets garbage collected, compare it with the incoming string as Span; and add a note to switch it over to https://github.com/dotnet/csharplang/blob/main/proposals/utf8-string-literals.md once it becomes available in the compiler."
"I do not think that this improvement is worth this hack. These hacks tend to live their own life, cause pain, and it is hard to get rid of them.

Wasm in .NET 5 is going to be incredibly slow for anything compute intensive. This hack is not going to change what kind of wasm-targeting applications are feasible in .NET 5."
it's seriously the most mundane things we end up missing..
"> after we drag the ""debug arrow"" back to sumcheck() after Console.ReadLine() we might end up with a garbage in ecx

Do we know if the impact will be visible to the user? In your example, I don't think that would be the case."
"You are right. Other than being on the latest version (which should not matter if free variables and method are the same), this does not have a value. It only matters in the sense that we are probably promoting one extra object to gen2 for the garbage collector by retaining a reference to it. My idea was following the principle that we should not retain a reference to an object that was otherwise not referenced anymore. However, there is something to say for not doing the administration at all and saving the cycles. Want me to change it?"
"The `uint`-suggestion is separate from `TextInfo.ToLowerInvariant(char)`'s inlining.

> Worst case it adds 1/4 nanosecond latency, so whatever. :)

Yeah :wink:"
"The vast majority of assemblies are single module. Multi-module assemblies are incredibly rare and not supported at all on .NET Core. I didn't want to complicate the output for that case. "
"The `CoreCLRRuntimeEnvironment` class comes into play when we are executing our tests on .NET Core vs. NET Framework. The ILVerify toolchain though should be runnable on any IL. Or at worst should be runnable when compiling for .NET Core. What the test compiles for and what runtime the test executes on are different concepts.

That made me a bit surprised to see this type here. My intuition was that ILVerification would be a general verification pass we could run independent of which runtime our tests were executing on. 

Am I missing something here?  #Resolved"
"Alternatively, could consider garbage-in-garbage-out for the serial number. If someone has a malformed serial number... chances are they are going to want to use it anyway in the extension. ""Yes my CA has a garbage subject key and I want my AKI to identify my garbage""."
"Can you add a comment that you need this test because garbage declarations may be introduced between overloads and cause issues according to our expectations
"
"Just for reference : The performance benchmarks code is [here](https://github.com/dotnet/performance/blob/8aed638c9ee65c034fe0cca4ea2bdc3a68d2a6b5/src/benchmarks/micro/libraries/System.Text.Json/Utf8JsonWriter/Perf.Strings.cs).

Questions/Observations:
1. I see lot more benchmarks run on SSE2 than on AdvSimd. I can't see their full names. Have we missed reporting any benchmarks for AdvSimd above?

2. It looks like SSE2 is better or mostly flat (exception of `1.06` , etc which I consider noise) if we compare on vs. off. So essentially SSE2 improved the performance of all the methods that are touched in your 3 PRs or at least didn't regress any of those 3 methods.

3. There are set of benchmarks that appears slower than the SSE2 counter part. E.g. For `GetPointerToFirstInvalidByte` , there are 5 benchmarks slower, the worst being 1.29X i.e. **30%** which is huge. 

4. It would be good to list all the benchmarks as shown for SSE2 (along with full names so we would know which scenario is slower) for AdvSimd in your ""All 3 PRs merged together"" section above. From what you have shared, there are couple of benchmarks for which we see **15%~24%** regression.

The first line of action is to see if you can repro the regression on your local machine (just take the relevant code from the benchmark and experiment if you see the slowness). The next thing to do is compare the JIT code before and after your changes as @github  pointed out. It might not be that something that you introduced, but possibly that those methods already run decently with existing vectorization and the operations that we are doing to find mask are turning out expensive than their counter part in default implementation."
"> when event occurs on fileNameWithExtension the relative file name in the event is fileNameWithExtension~ and it is treated as if its fileNameWithExtension, So This will update the watchers correctly (that is recreate watchers which it didnt do previously and hence lost events after that.

I see; I was just trying to think of an edge case where this method wouldn't be good, but I suppose at worst, it means we look at the non-`~` an extra time or something. 

> Eg on linux vm the tilda thing never came up as part of events and it was working even before that extra commit

Just so I understand, do you mean that `~` files just never showed up in any file watcher events at all? Or that bugs related to them didn't appear?"
"My last try (promised!) - This is a doc I wrote for my implementation (at least I would have tried everything I could to gain attention on this)...

## Json polymorphism support

One of the big issue to solve when de/serializing objects is to manage polymorphism: when an abstraction must be serialized, the type of the serialized object **must** also be written so that deserialization know what to instantiate.

### The classical approach
The classical approach here is to inject a `$type` (or `_$type$_` or any strange enough property that contains a string with the name of the type) into the object properties. 
Since in JSON, collections (arrays, lists, sets) are expressed with `[arrays]` (and this cannot hold any property), another syntactical
construct is required for collections, typically something like `{ ""$type"" : ""A(int)"", ""$values"" : [1,2,3] }`.

Note that, in this approach, serialized objects with or without a type are structurally identical (only the existence of the `$type` property makes the difference) whereas serialized collections ARE structurally different (they appear inside a ""wrapper"" object).

> When is a `$type` NOT required? When the object's type is *statically known*, when the de/serialized type is *unambiguous*. As soon as 
> an ambiguity exists, **polymorphism** is at stake.

The `object` type being the ""worst case of polymorphism"", but any interface or base class with specializations are *ambiguous*), the runtime type of the object MUST be serialized and MUST be used to deserialize the object.

We consider ""Structural Difference"" between unambiguous and polymorphic objects to be a good thing since this obliges the reader to take care of the *object's type* if there is one.
Without any difference in the shape of the data, a polymorphic object can be falsely read as an object of an *unambiguous* (wrong) type. 

### Structural Polymorphism

We are using a different approach that offers the following advantages:
- No ""magic name"" like `$type` or `$values`.
- Uniform type handling for objects and collections: there is always a ""Structural Difference"" between a *polymorphic* and an *unambiguous* shape.

Thanks to this, the code is simple and can be split in 3 layers:
- Nullable layer: the `null` occurrence is handled.
- Polymorphic layer: a 2-cells array is expected, the first cell containing the type name, the second one the unambiguous object's representation.
- Unambiguous layer (the exact type is known): the serialize/deserialize code handles purely the object's data.

Last (but not least) advantage of this approach: **when deserializing, the type is known BEFORE the object's data**. This enable deserializer to easily work in ""pure streaming"" (no lookup required, no intermediate instantiation): the Utf8Reader can be used directly.

Below is a simple example with:
```csharp
class Person
{
   public string Name { get; set; }
}

class Student : Person
{
    public int Age { get; set; }
}

class Teacher : Person
{
    public bool IsChief { get; set; }
}
```

The C# `Student[]` is an array of Student. There is no specialization, hence no ambiguity: `[{""Name"":""A"", ""Age"":12},{""Name"":""B"", ""Age"":13}]`.

The C# `Person[]` can contain 3 different types of objects. Each item needs to be ""typed"":
`[[""Student"",{""Name"":""A"", ""Age"":12}],[""Person"",{""Name"":""E""}],[""Teacher"",{""Name"":""T"",""IsChief:false}]]`.

When the same array of Persons is known only as an object[] (or any other non-unique ""base"" type like the non-generic `ICollection`), then the array itself requires its type:
`[""A(Person)"",[[""Student"",{""Name"":""A"", ""Age"":12}],[""Person"",{""Name"":""E""}],[""Teacher"",{""Name"":""T"",""IsChief:false}]]]`


"
"Well, that sucks. We'll see if there's a way to have your voice represented in that issue (worst case scenario we'll create a new one)."
"Ignoring a possible failure of imagination, not really. 

A user might hold a reference to a Page for the entire life time of an app. For example, now with `Shell` since you can register pages as singleton a lot of people are going to register those pages as singletons which means every single control on that page will live for the entire life of the application. We can't really assume what the user wants to happen with that page they are retaining a reference to. For example, at the platform level someone might retain a view with a map control on it because they don't want that map control to dispose and have to recreate it. 

I think what we'll need to look at as we gather use cases are places where we can make good enough guesses and then let people influence those guesses.  For example, with Shell maybe we should always disconnect the handler if they've registered something as transient? I think one area here that would delight users is if they could scope to a page/navpage/window and then once that container is disposed that will naturally dispose of the children. We can't really do that currently with the limitations of MS.Ext.DI but just spit balling here :-) 

>  ""this things is definitely garbage and I definitely want to destroy everything"",

TLDR; not really but we should figure out a way to let users tap into life cycle points better. 
"
"Yay itâ€™s green! ðŸ’š Will seriously need to clean this up and finish the TODOs, though."
"@github I looked into adding a test, but it is incredibly cumbersome as the routes get picked up from the assembly, and I don't think its worth creating an assembly for a test like this."
"Since this is a servicing fix and this case is rare, we can keep the current behaviour from this PR.  However, keep the code from this diff somewhere, should we need it for the next servicing release (it looks pretty clean and makes the worst case scenario more manageable)."
"Exactly. Short:

Code Point U+0000..U+FFFF: 1 UTF-16 code unit --> 1..3 UTF-8 code units.  
Code Points >U+FFFF: 2 UTF-16 code units --> 4 UTF-8 code units.  

Thus, the worst ratio of UTF-16 code units to UTF-8 code units is 1 to 3."
"That `CollectionView.ContentInsetAdjustmentBehavior` does have a purpose; without it, horizontal grid layouts in CollectionViews break pretty badly. So your fix isn't quite right.

I'll take a look at this on Monday - it looks like we might have a conflict between the CollectionView's handling of safe area and the overall handling in Forms, for which `UIScrollViewContentInsetAdjustmentBehavior.Never` is just a band-aid. The automatic behavior (what you get when you remove that line of code) *is* what we want for handling keyboard shifting. So the fix probably lies elsewhere. 

But in the worst case (where we can't reconcile the two), we can add a keyboard listener to the CollectionView. The listener can tell us when the keyboard is popping up and allow us to adjust the insets accordingly. "
"I seriously appreciate that. Thank you.

PNGs for 256x256 and BMPs for everything else is totally fine with me."
I seriously love how nice this feels.
I seriously think the text should not be localizable. I don't want to ship with it localizable and then have to back that out. :smile:
"@github @github @github @github finally the PR should be ready and I already reviewed it myself. I summarized the changes in the top post with a list of the commits. I already merged some related changes into master and arcade in the last weeks. The remaining changes in this PR weren't easy to encapsulate further.

I highly recommend to review per commit. Will share out perf numbers with best and worst case build times later after CI has completed."
"I would like to understand the worst-case impact here, e.g. measuring `new Regex(...).IsMatch(...)` with a pattern that exhibits the problematic case."
"This is currently incredibly arbitrary - we're waiting on LDM to decide the precedence. I just kind of threw it somewhere without thinking about it (it's really really easy to change - just shuffle this enum).

Thanks for the feedback though :)

(I think I said ""uh, let's copy Rust, I guess"", and then I promptly mis-remembered what precedence Rust's range has ðŸ‘ ðŸŽ‰)"
"Any way we could make something like `TerminalSettings::SetParent(const TerminalSettingsStruct& parent)` that does this `if (UnfocusedSettings()) { update the parent for the unfocused appearance}`..


Uhg, no not easily. I suppose this is ther only place where a control's settings are actually updated, so that's not actually the worst thing"
"I feel worst about this. It was either ""use the new commandline detection in `GetProfileForArgs`"" or ""copy the feature flag detection code over to TerminalPage as well"" ðŸ˜¦  (since it needs to work properly for all types of invocation.)"
"A choice snippet from `ConptyOutputTests::WriteAFewSimpleLines()`:
```
    // Here, we're going to emit 3 spaces. The region that got invalidated was a
    // rectangle from 0,0 to 3,3, so the vt renderer will try to render the
    // region in between BBB and CCC as well, because it got included in the
    // rectangle Or() operation.
    // This behavior should not be seen as binding - if a future optimization
    // breaks this test, it wouldn't be the worst.
```

Yep, that's what I broke/fixed."
"Yeah, our squash workflow can be a pain for stacked branches. A merge _should_ work, though.

Worst case, a rebase?"
"Actually, without the ""right hand side"" qualification, it's a garbage comment (""allowReservedWords allows reserved words!""). I deleted it."
"This is what I mentioned in my most recent comment above. As of now (with passing tests) the messages are shown twice and the second message is actually garbage. If I test only for the getter the output looks as desired (that's what I presume), but the tests fail. 
So what do we do?"
"Every entry in that table gets `realpath`'d (if it exists) though, so there won't be duplicate entries, at least. And since we wipe away and replace the whole table (elementwise, via mutateMap), we don't even run the risk of accidentally leaving behind old watches if there's a case mismatch (we'd just remove the old case, then add the new one, which is potentially inefficient, but not the worst). And again, at least on windows, node realpath returns input case, so it shouldn't change anything (unless it remaps a symlink, in which case the new part will be FS case, I think). (If you always wanted the FS case you actually have to use the ""native"" realpath variant, iirc, which we don't use because it caused issues if I'm remembering correctly)."
"See above comment.  The existing model for parsing inappropriately pushed outer flags to nested children.  This broke the invariant that nodes are only affected by changes below them, and affected the ability for us to be able to incrementally reuse nodes safely.

Note: while this adds a small amount of memory to compilation, the incremental gain produces vastly less garbage for the LS scenario.  this is quite important as garbage leads to GCs which lead to pauses, which pushes the latency for typing above our high water mark.
"
I think I prefer calculating the drain timeout this way vs deriving the worst-case from `MaxResponseBufferSize` like we do for HTTP/1.x and HTTP/2. Can we do the same thing in `Http1Connection` and `Http2Connection` for consistency?
Good question - this is also an area I'm not super familiar with!  I guess it looks like I should be using the Dispose() pattern?  https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/implementing-dispose
This is going to create lots of garbage as this lambda cannot be cached. #Closed
"How big do we expect these lists to be?  And more importantly, what's the worst case?  N calls to AddIfNotExist results in O(N^2) work here."
"I know that a lot of the changes here are to telemetry calls, which presumably will impact a database somewhere.
I'd argue that changing to the correct spelling and at worst adding a shim/collator for the backend is the right thing to do.

That said, I'm happy to drop these things if requested."
"That's good and in line with https://github.com/microsoft/vscode/wiki/Extension-API-guidelines#copy-data. An extension is free to give you a ""bigger"" object than needed or, worst, one with cycling dependencies. Therefore we copy."
"We at one point changed all our living ASSERTs into FAILFASTs, and it was the worst thing we had done to conhostâ€™s reliability to date. Theyâ€™re the reason Terminal spontaneously exits to this day! They werenâ€™t harming anyone as asserts, and it turns out we just kept chugging along even with inconsistent state.

Maybe â€œon error resume nextâ€ had a place in some world ... ðŸ˜ "
"Can you add some variants on this test where we have the possible resync points? Ie, some garbage before the `As`, `=`, or `Implements` keywords."
"Seriously, can JSON be any worse...?"
"We immediately pop up the threaded wait dialog.  So reentrancy seems very unlikely.  Specifically:

The command is invoked.  That's going to be on the UI thread.  Then we yield and make the TWD.  So hte only way to get reentrancy would be if somehow there was a queued message after the invoke.   
1. that seems incredibly unlikely/difficult to get into.
2. i can't even think of what sort of message we might get that would be problematic.

Note: the experience without this change is exceedingly poor.  
"
"Is the `List<T>` version generating more garbage from the non-List paths? Looking at those numbers, while using IROL list a clear win in terms of memory, it doesn't actually make a different CPU-time-wise, as the IROL and Baseline are within the Error margin. I think I'd prefer to have both a List and IROL overload. The method complexity is not very high, and 200 MB of memory savings on a large `NormalizeWhitespace`, while nice, is basically peanuts for our scenarios."
"> it sounds like you think with this we can unify some code and either make it perform better or at least not lose any perf.

Not to put words in @github mouth, but I expect what he's hoping to do is improve the `Array.Sort(..., IComparer<T>)` code paths.  Today we have one sort implementation that covers both providing an `IComparer<T>` and a `Comparison<T>` (a delegate); the former is implemented by creating a delegate to its Compare method, which means `Array.Sort(..., IComparer<T>)` is allocating.  On top of that, we just recently added a span-based sort, and it's currently defined with a generic `TComparer : IComparer<T>` comparer, with the idea that you could provide a struct-based comparer and it wouldn't allocate... unfortunately, it's the worst of all worlds right now, in that we box the `TComparer` into an `IComparer<T>`, and then create a delegate from its `Compare` method.  If all of these code quality issues got sorted out, you could imagine we just had a single code path for `TComparer` that was written generically to use its `Compare` sans boxing, and then if you used the `Comparison<T>` overload, we'd create a struct-based `TComparer` that just wrapped that `Comparison<T>` to invoke it.  If we everything worked out really well, it could potentially even be unified with the separate, duplicate code path we currently have that targets `T : IComparable<T>`, using a `TComparer` comparer struct that delegated to the `T : IComparable<T>` implementation.

> So perhaps a benchmark along these lines would be instructive?

Sounds like a good thing to add to dotnet/performance."
"> This problem (screenshot) reproduces on .NET Framework x64 (32bit is fine) 

Yes, I missed the part where the `val` was showing garbage.

> I ran a test 0f69114 where I disabled reuseArg completely - as expected it passed all tests and the spmi diffs were quite big - Total bytes of delta: 28970 (0.09 % of base) for crossgen libs

Yes, I see lot of regression. I think there is something going on with asmdiffs. I just see the diffs from `coreclr_tests` and `libraries_tests` and they are huge. I see lot of Missing value errors. Do you mind rebasing your PR so we can run it against the latest Spmi collection?"
"> Instead of validating the contents of the cache when we download bits from the network, can we validate them when we load them from the cache?

That seems strictly worse to me. We're then doing the hash check on every load instead of only when the data was first acquired. We still have to do it on the first load when it comes from the network too (see below).

> It is very unlikely that you end up in a situation where the manifest doesn't reflect the contents of the server or that you can't trust the manifest (it's your app, you are serving it).

It will certainly happen sometimes. It's always possible during deployments that a user fetches `blazor.boot.json` from the old version of the site and some assemblies from the new version.

> Worst case scenario, your app is in the middle of a deployment and you got the wrong contents, the likely outcome is that your app will crash and the user will have to reload (this problem is orthogonal to what we are trying here)

Doesn't seem like an orthogonal problem to me. The scenario this code deals with is acquiring the resources and verifying they are the ones we want. I don't see any benefit in letting the app start up with the wrong resources, performing whatever undefined behavior this leads to for some period, and then hoping we can shut it down later and hope we get to run more logic to clean up the polluted cache.

> If you want to do this validation, then do it over the cached data the first time you load from cache and ""repair"" purge the entry from the dll once you detect that scenario.

I definitely think we should validate the data the first time it comes from the network, for the reasons mentioned above. And then having done that and cached it under a key derived from that hash, there's no purpose in revalidating it again later."
"AFAIK I believe we can't. I know there are functions to read from the heap, but I don't think there are functions to write to the heap. Are there?

I don't think this is incredibly critical since unmarshalled interop should be pretty fast, I'm happy to change this post RC1 if we think it's important."
"(more seriously, didn't want to break something while fixing something, especially given that this will be an insertion blocker)"
"You just leave it out of the IDL. winrt::make calls the boring normal C++ constructor for the class in the implementation namespace, so there isnâ€™t anything else to be done. The IDL only specifies how it can be constructed from the outside.

so really itâ€™s just `winrt::make<implementation::ConptyConnection>(literally whatever garbage you want, any C++ types or expressions)`"
"Do we really care about/need `haveMonoAndroid` and `hasExportReference`?  ""Worst"" case, we'll try to add these assemblies to the `HashSet<string>`, which I would *hope* wouldn't actually take much time.  Does this really save anything?"
"OMG! All checks have passed?! I almost can't even believe it anymore. ðŸ˜„
This PR was _seriously_ hard to create (due to all the little details surrounding it).
TBH I hope we can merge at least parts of it. ðŸ˜…"
"I did not check other platforms, but win-x64 seems to have the most optimized code originally, so I assumed if any regression, it must be worst on win-x64.

Is any other platform interesting in particular?"
"@github, @github Why do we need a comma at the end. What meaning does it carry, except for garbage?"
"asmdiffs results: https://dev.azure.com/dnceng/public/_build/results?buildId=1507589&view=ms.vss-build-web.run-extensions-tab

The worst difference are around 228 bytes in windows/linux arm64. I also inspected diffs from other platforms, and almost all of them are coming because we stop promoting the `DateTimeOffset` struct. So the code changes are related to `_offsetMinutes` and `_offsetMinutes` as they are passed in memory now.

https://github.com/dotnet/runtime/blob/06bb200673f0a4ac37ea49a4aaf6c16be4441a10/src/libraries/System.Private.CoreLib/src/System/DateTimeOffset.cs#L32-L65"
"The new linker attributes are only in `net5.0` and higher. It would be incredibly ugly to sprint `#if` everywhere we use them.

So I took the approach we used in Xamarin.Android, which is to include an `internal` copy of these attributes for target frameworks that don't have them:

https://github.com/xamarin/xamarin-android/tree/main/src-ThirdParty/System.Diagnostics.CodeAnalysis"
I don't think I follow. How could there be garbage here? If the encoded bytes had an extra invalid byte at the end?
"@github zombies ðŸ§Ÿ are not to be trifled with.

> I get the hanging tests on master branch too on my Mac.

More seriously, what is your Mac doing exactly when the timeouts occurâ”

And, because we're not seeing similar problems in the rolling builds of our 'master' branch, I wonder about scorched Earth approaches like clearing your NuGet caches and rebooting."
wat...  seriously.  wtf :)
"Would it make sense for this method to write zero to the array if it is not a constant? That way, when/if we support creating with a partial constant, we can avoid initializing it to garbage."
"Fine if it works for you, but normally I'm super pessimistic about timeouts as I've seen stuff be incredibly slow occasionally. Eg., 30 secs"
"> How often is this codepath hit? How much memory could we possibly leak?
> 
> Are we hitting this codepath _every time_ we _invoke_ a P/Invoke to a `__Internal` method, in which case we could be hitting this code path _thousands_-_millions_ of times, which could be a significant memory leak?
> 
> Or do we hit this code path _every time_ we _resolve_ a P/Invoke to a `__Internal` method, in which case we could _at most_ leak dozens of instances, _maybe_? (Though each instance is probably on a separate code page, so that could still add upâ€¦)
> 
> What's our worst-case scenario here?

From what I see, the `__Internal` DSO lookup is done once per assembly, so we'd leak N times where N equals the number of assemblies. "
"tbh, this is just incredibly concerning.  semantic models (and the work therin) is not cheap, and this is a sync codepath.  do we need semantics?  can't we look for throws purely syntactically?"
"Yes. The `buckets[SLOT_NEXT]` will be pointing to the next larger table, if resize happened while you are doing a lookup. It will be rare. It is possible, even though extremely rare to see more than one new version. 
Every new one is 2x larger than previous, so there can't be too many even in the ultimate worst case."
"I thought a Project subclass is probably a lot heavier of an object than I need, but I agree the ScriptInfo changes (and Project/LanguageServiceHost API changes that work with ScriptInfo) is the worst part of this PR, so maybe that would simplify things a lot."
"If this were public or internal maybe, but since it's only used privately from one place I'm not concerned about it. Worst case we'll get a type case exception that should show up in tests."
"I don't think we need a special limit here, in most cases we are going to be matching a single header and if a header matches multiple values I think that's a semantic we should be happy with?

If you are concerned about the work the server has to do, the worst request that a client can send is only going to have a constant factor on the amount of work the server needs to do, so I think we are covered here based on the limits that the server already has in place for header size, so I think this optimizes for the uncommon case and that we shouldn't need it.

Does that make sense?"
"> basically working based on roughly line based.

Line based is usually quite inappropriate for a refactoring.  A line can contain a huge amount of different nodes and concepts.  For example, if i go into ""va$$r x"" and try to bring up refactoring to convert 'var' to 'int', then i do not want to see 'convert for to foreach'.  It's just clutter at best, and at worst, it will be prioritized poorly and it will impede the user from making the change.

> I am +1 on making it same as regular code fix

Note: this is how our regular code fixes operate.  And ones that do not are intended to be updated to follow these patterns :)"
"It's in the fine print of the EULA tagging me in a review: I'm entitled to change my opinion for any reason, or for no reason whatsoever.

More seriously: if you want to expose this I guess we'll use it in that one spot. If you don't want to expose it...we'll be fine. #Resolved"
Are you on the Xamarin Chat slack? Or have some other way to reach you? Or worst case can you reach out to gerald.versluis@microsoft.com? Let's see if we can get it to work and from there I'll create a little writeup to make sure this doesn't happen to other people
"I had a hard time choosing a good number. 256 was a tiny bit faster for shorter lines, 1024 was optimally fast for longer lines (no improvements beyond 1024) and 512 was basically the worst of both worlds.
The performance improvements with a buffer size of 1024 are more noticable if accessibility events are enabled. In that case conhost is up to 20% faster after this change."
"@github sorry for the delay, didn't notice your comment.
So from my understanding we were supposed to save BBF flags here https://github.com/dotnet/runtime/blob/main/src/coreclr/jit/fginline.cpp#L1434

but we lose connection between CALL and RET_EXPR somehow, I feel like it happens because RET_EXPR is re-used. So initially we inserted RET_EXPR in `Test` for `Calle1` but after `Calle1` was inlined we kept it to then re-use for `Callee2`
```csharp
    [MethodImpl(MethodImplOptions.NoInlining)]
    private static void Test(int[] x)
    {
        try
        {
             Callee1(x);
        }
        catch
        {
        }
    }

    [MethodImpl(MethodImplOptions.AggressiveInlining)]
    private static int Callee1(int[] x) => Callee2(x, 0);

    [MethodImpl(MethodImplOptions.AggressiveInlining)]
    private static int Callee2(int[] x, int index)
    {
        if (x == null)
            Callee3();

        return x.Length;
    }
```
while my fix works I guess I better investigate why we didn't update `iciCall->gtInlineCandidateInfo->retExpr` (currently it's some garbage value)"
"> gafter Wouldn't it be more convenient to introduce this as a breaking change?
> I feel that multiple entry points is an error more than just a warning, and this behavior will be consistent with multiple synchronous entry points.
> Why should the behavior for multiple entry points containing an async main be different than multiple entry points that are all sync?
> 
> I also would like to know more info on the warning wave thing, and how its process goes.

This isn't really an option. We take backcompat seriously, and introducing a new warning that cannot be opted out of is absolutely a backcompat break.

You'll need to add the warning to the level 5 switch here: http://sourceroslyn.io/#Microsoft.CodeAnalysis.CSharp/Errors/ErrorFacts.cs,210, and make sure to follow the `WRN` prefix instead of an `ERR` prefix."
"Can you also remove the finalizers? Since these classes don't use native resources, they shouldn't have finalizers.

See https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/implementing-dispose#implement-the-dispose-pattern

>  Important
> It is possible for a base class to only reference managed objects, and implement the dispose pattern. In these cases, a finalizer is unnecessary. A finalizer is only required if you directly reference unmanaged resources."
"It was a good thing this was reverted, because the PR is actually unsound, and it was caught from the local serve build. The problem is a few fold, and so I'm writing here to seek guidance. As a general note, I'm unsure how this should function in `exactOptionalPropertyTypes` land.

The original PR sought to add any discriminable property of any optional property of any element of the union type as a discriminator checking for assignability to `undefined`. This concept had some performance hits that I was able to address, but there was actually an underlying bug in the test. If an ""unrelated"" type had optional members, those would still get added to the discriminator list. However in `discriminateTypeByDiscriminableItems` if a type doesn't have that property, it's immediately marked as not discriminable. This vaguely makes sense, but doesn't actually align with what this was trying to do, which is inherently check for assignability to undefined as a way to check for missing member as part of a discriminated union. I tried defaulting to undefined in `discriminateTypeByDiscriminableItems` but that created a few other problems.

Either way, this premise of checking for assignability to undefined of optional members is actually flawed. In principle what we actually want to is check for all potential properties in the union type that are missing from the original object for their assignability to missing. Ignoring `exactOptionalPropertyTypes` this should be possible to do by checking assignability to `undefined`, but as currently written, this would need a new path `discriminateTypeByDiscriminableItems` and seems potentially error prone, and potentially very expensive.

As I write this, another idea comes to mind, which seems less ""correct"" but may still be able to achieve the ideal result: In principle this is still meant to help figure out ""discriminated unions"", so instead of caching all properties, or all optional properties, we could specifically look for optional discriminated unions, e.g. the property is optional on one type and non-optional on any other members. By itself this will still fail on the union of two disjoint optional discriminated unions (which seems unlikely). But instead of checking all discriminators we check them independently for the set of types they discriminate against. Worst case this could be really bad, but I think in most common usages it might not even require an extra check.

I'm curious as to general thoughts on these different approaches. I realize this isn't very important, so maybe I'm overthinking this.


"
"@github, what do you think about adding {M}IBC to glossary. To my understanding:

```diff
--- a/docs/project/glossary.md
+++ b/docs/project/glossary.md
@@ -24,12 +24,14 @@ terminology.
 | DAC | Data Access Component. An abstraction layer over the internal structures in the runtime. |
 | EE | [Execution Engine](https://docs.microsoft.com/dotnet/standard/managed-execution-process#running_code). |
 | GC | [Garbage Collector](https://github.com/dotnet/runtime/blob/main/docs/design/coreclr/botr/garbage-collection.md). |
+| IBC | Instrumented Block Counts - used as extension (`*.ibc`) for old PGO files. |
 | IPC | Inter-Process Communication. |
 | IL | Intermediate Language. Equivalent to CIL, also equivalent to [MSIL](https://docs.microsoft.com/dotnet/standard/managed-execution-process#compiling-to-msil). |
 | JIT | [Just-in-Time](https://github.com/dotnet/runtime/blob/main/docs/design/coreclr/jit/ryujit-overview.md) compiler. RyuJIT is the code name for the next generation Just-in-Time(aka ""JIT"") for the .NET runtime. |
 | LCG | Lightweight Code Generation. An early name for [dynamic methods](https://github.com/dotnet/runtime/blob/main/src/coreclr/System.Private.CoreLib/src/System/Reflection/Emit/DynamicMethod.cs). |
 | MD | MetaData. |
 | MDA | Managed Debugging Assistant - see [details](https://docs.microsoft.com/dotnet/framework/debug-trace-profile/diagnosing-errors-with-managed-debugging-assistants) (Note: Not in .NET Core, equivalent diagnostic functionality is made available on a case-by-case basis, e.g. [#9418](https://github.com/dotnet/runtime/issues/9418)) |
+| MIBC | Modern Instrumented Block Counts - used as extension (`*.mibc`) for modern PGO files. |
 | MSIL | [Microsoft Intermediate Language](https://docs.microsoft.com/dotnet/standard/managed-execution-process#compiling-to-msil).Common Intermediate Language. Equivalent to IL, also equivalent to CIL. |
 | NGen | Native Image Generator. |
 | NYI | Not Yet Implemented. |
```"
This is visual studio autonamespace complete thingy that leaves garbage all over the place. I'll remove.
"The binder doesn't know it binds types from the signature.

As I see it we have three options:

1. When creating the TypeParameterSymbols in SourceOrdianaryMethodSymbol, recursively check the ParameterSyntaxes to see if any TypeParameters are used as NullableTypes.

2. When binding a Type, pass in enough information for the binder to know it is binding types from the signature.

3. When binding a parameter, deep copy the type of the parameter, replacing all lazy nullable types with Nullable Types.

I think 3. is the worst option, as it is both most expensive, and most error prone. I am not sure about 1 or 2, but will go with 1 for now unless you tell me otherwise."
"> a latch would only ever go from ""supported"" to ""not supported"". so the worst that happens in a few requests throw, but eventually you get into the not-supported state, and stop throwing. i don't think we'd need a latch that guaranteed only one throw.

Exactly. This is what I meant by ""...at most a small number..."""
"again, generic macro with an INCREDIBLY SPECIFIC type in it."
"JIT has never implemented the ECMA memory model (e.g., regarding volatile reads), bug reports (mine included) have been ignored for years, developers have been told that JIT actually implemented something different, so it is hard to take this argument seriously."
"Hi! We have recently observed the presumably well-intentioned act of the author of this PR (@inclusive-coding-bot), who has been spamming pull requests to many (~50) github repos, claiming to ``switch to gender neutral pronouns''. In reality, the bot performs a dictionary replace of gendered nouns and pronouns, and the outcome is questionable at best, and literally harmful at worst. See:
- The bot did a terrible job at replacing these words - [EbookFoundation/free-programming-books#6801](https://github.com/EbookFoundation/free-programming-books/pull/6801/commits/5257301642c173e2dc4f034f8c0460ce7ea99de6)
- The code fails to compile at [rust-lang/rust#95508](https://github.com/rust-lang/rust/pull/95508)
- The bot ignores all context whatsoever at [moby/moby#43441](https://github.com/moby/moby/pull/43441)
- After initial PR being closed, the bot keeps sending PRs, effectly spamming the repos. For example, these 5 PRs were sent to the same repo within 2 days [#1](https://github.com/996icu/996.ICU/pull/26073) [#2](https://github.com/996icu/996.ICU/pull/26074) [#3](https://github.com/996icu/996.ICU/pull/26075) [#4](https://github.com/996icu/996.ICU/pull/26076) [#5](https://github.com/996icu/996.ICU/pull/26077)

To save more time for the open-source community, we recommend the maintainers of VSCode to close this PR and ban this bot from further spamming."
"@github: should we just *remove* this constructor?  I don't see it used anywhere, and if it *had* been used, we'd have had a runtime crash because `wrappers` would have been garbage."
"Good, I was just going on the simple fact that tabs are one of the 3 worst things on Earth.
"
